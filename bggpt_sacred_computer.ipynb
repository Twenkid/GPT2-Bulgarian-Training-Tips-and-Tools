{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ce5bf7b8f4740a08edd1df00306dc7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adca3df4c0bb47fca206cb95a5626d8d",
              "IPY_MODEL_39c88b2a0e0f4b20be0aa03412fccabb",
              "IPY_MODEL_c09dae61614c460b819d001aaf1e0988"
            ],
            "layout": "IPY_MODEL_9e8ab69002cc4aefa81e92f2f5345f3b"
          }
        },
        "adca3df4c0bb47fca206cb95a5626d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cda178088ad4eb79cf2124dbea0995c",
            "placeholder": "​",
            "style": "IPY_MODEL_829778a66f3f47f4b148ac69b0923aec",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "39c88b2a0e0f4b20be0aa03412fccabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6b2973f9cc7406bb2b5cdf4c662bf26",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2938c37f8fa4541a789e770a3700499",
            "value": 2
          }
        },
        "c09dae61614c460b819d001aaf1e0988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55b3b350b62144d1977120dbd8eeb8b0",
            "placeholder": "​",
            "style": "IPY_MODEL_8663905a79754ac6b0bea954a399d2fc",
            "value": " 2/2 [00:01&lt;00:00,  1.80it/s]"
          }
        },
        "9e8ab69002cc4aefa81e92f2f5345f3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cda178088ad4eb79cf2124dbea0995c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829778a66f3f47f4b148ac69b0923aec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6b2973f9cc7406bb2b5cdf4c662bf26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2938c37f8fa4541a789e770a3700499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55b3b350b62144d1977120dbd8eeb8b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8663905a79754ac6b0bea954a399d2fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools/blob/main/bggpt_sacred_computer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k_lKxlssvJTU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g8B-4Fo6vIEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **BgGPT в Colab - безплатно на Tesla T4 или TPU**\n",
        "#### Изпробвано от Тодор Арнаудов - Тош от **СВЕЩЕНИЯТ СМЕТАЧ** - Институт за Мислещи машини, творчество и развитие на човека - основан през 2000 г.\n",
        "* http://github.com/twenkid\n",
        "* http://artificial-mind.blogspot.com\n",
        "* http://research.twenkid.com\n",
        "* http://eim.twenkid.com\n",
        "\n",
        "**Видеоръководства:** от канала \"Twenkid Studio - Artificial Mind (todprog): https://www.youtube.com/channel/UCgyhnsM9ed292HUAObXUsvw\n",
        "\n",
        "**Версии**:\n",
        "* 19.2.2024: първа, единични заявки;\n",
        "* 20.2: Цикъл с извиквания, промяна на дължината на породения текст, [INST]...[/INST], промяна на max_... и др.\n",
        "* 22.2: вкл/изкл [INST], ... TextWrapper ... и др.\n",
        "\n",
        "Тази тетрадка е качена в: https://github.com/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools\n",
        "...\n",
        "\n",
        "Тош, тогава още тийнейджър, е автор на \"пророческата\" интердисциплинарна **\"Теория на разума и Вселената (2001-2004)\"** https://github.com/Twenkid/Theory-of-Universe-and-Mind за общ изкуствен интелект и др., която е в основата на **Първия в света интердисциплинарен курс по Универсален изкуствен разум (Artificial General Intelligence)**, който създава и преподава в ПУ \"Паисий Хилендарски\" през 2010 г. https://artificial-mind.blogspot.com/2010/04/universal-artificial-intelligence.html\n",
        "Тош е автор на **Стратегията за развитие на България чрез свръхинтердисциплинарен институт за Изкуствен интелект и създаване на универсални мислещи машини от 2003 г.**, която INSAIT преоткрива с 20 години закъснение и над 200 милиона лв по-голям бюджет: **\"Как бих инвестирал един милион с най-голяма полза за развитието на страната\"**:\n",
        "* https://www.oocities.org/todprog/ese/proekt.htm\n",
        "* https://artificial-mind.blogspot.com/2020/07/interdisciplinary-research-institute.html\n",
        "... на безплатните синтезатори на реч\n",
        "* **\"Глас 2004\" и \"Тошко 2\"**: https://github.com/Twenkid/Toshko_2\n",
        "\n",
        "* ... на **безплатния интелигентнен речник-помощник на преводача \"Smarty\" (2007)** който беше най-\"умното\" подобно приложение в света, с наченки на разбиране на контекста: https://github.com/Twenkid/Smarty\n",
        "\n",
        "* ...на всестранния проект **\"Вси, или Специалист по всичко\"** за инфраструктура за Общ изкуствен интелект, 2022:\n",
        "https://github.com/Twenkid/Vsy-Jack-Of-All-Trades-AGI-Bulgarian-Internet-Archive-And-Search-Engine\n",
        "\n",
        "* Обучава голям езиков модел на българскив Колаб **GPT2-BG Medium, обучен в Колаб през 2021 г.**: - ръководство за обучение, за пораждане на по-\"творчески\" текстове и самият модел:\n",
        "https://github.com/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools\n",
        "\n",
        "и др.\n",
        "\n",
        "Интервю от 2009 г.: **Тодор Арнаудов: Ще създам мислеща машина, която ще се самоусложнява.\n",
        "Фантазьори и авантюристи правят великите открития. Работата на скептиците е да отричат, а после да не вярват на собствените си очи**\n",
        "\n",
        "https://artificial-mind.blogspot.com/2009/11/dreamers-and-adventurists-do-big.html\n",
        "\n",
        "**СВЕЩЕНИЯТ СМЕТАЧ** търси всякакви партньори, съдружници, спонсори, съмишленици. Виж например \"issue\"-то в проекта \"Вси\":\n",
        "https://github.com/Twenkid/Vsy-Jack-Of-All-Trades-AGI-Bulgarian-Internet-Archive-And-Search-Engine/issues/10\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "19.2.2024\n",
        "\n",
        "BgGPT:\n",
        "\n",
        "https://bggpt.ai\n",
        "\n",
        "https://huggingface.co/INSAIT-Institute/BgGPT-7B-Instruct-v0.1/blob/main/README.md?code=true\n",
        "\n",
        "https://bggpt.ai/blog/2024-02-18-launching-the-first-free-and-open-bulgarian-llm/\n",
        "\n",
        "https://huggingface.co/INSAIT-Institute/BgGPT-7B-Instruct-v0.1/tree/main\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PD8yuacUV4bI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwTabNcJXaxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "31vvLOvzZcwh",
        "outputId": "3345f093-4334-40ef-8836-b000c5723b57"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f7e957707e4b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huggingface-cli download INSAIT-Institute/BgGPT-7B-Instruct-v0.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ],
      "source": [
        "# !huggingface-cli download INSAIT-Institute/BgGPT-7B-Instruct-v0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging ninja\n",
        "!pip install flash-attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdfHe2_2Zu1t",
        "outputId": "f98ec1bc-b327-4e76-958e-1eeebd01bcbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.5.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(AutoModelForCausalLM.from_pretrained)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWSlbIaEbgKE",
        "outputId": "951546ac-75c5-408d-886d-a23059eee5a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method from_pretrained in module transformers.models.auto.auto_factory:\n",
            "\n",
            "from_pretrained(*model_args, **kwargs) method of builtins.type instance\n",
            "    Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.\n",
            "    \n",
            "    The model class to instantiate is selected based on the `model_type` property of the config object (either\n",
            "    passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
            "    falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
            "    \n",
            "        - **bart** -- [`BartForCausalLM`] (BART model)\n",
            "        - **bert** -- [`BertLMHeadModel`] (BERT model)\n",
            "        - **bert-generation** -- [`BertGenerationDecoder`] (Bert Generation model)\n",
            "        - **big_bird** -- [`BigBirdForCausalLM`] (BigBird model)\n",
            "        - **bigbird_pegasus** -- [`BigBirdPegasusForCausalLM`] (BigBird-Pegasus model)\n",
            "        - **biogpt** -- [`BioGptForCausalLM`] (BioGpt model)\n",
            "        - **blenderbot** -- [`BlenderbotForCausalLM`] (Blenderbot model)\n",
            "        - **blenderbot-small** -- [`BlenderbotSmallForCausalLM`] (BlenderbotSmall model)\n",
            "        - **bloom** -- [`BloomForCausalLM`] (BLOOM model)\n",
            "        - **camembert** -- [`CamembertForCausalLM`] (CamemBERT model)\n",
            "        - **code_llama** -- [`LlamaForCausalLM`] (CodeLlama model)\n",
            "        - **codegen** -- [`CodeGenForCausalLM`] (CodeGen model)\n",
            "        - **cpmant** -- [`CpmAntForCausalLM`] (CPM-Ant model)\n",
            "        - **ctrl** -- [`CTRLLMHeadModel`] (CTRL model)\n",
            "        - **data2vec-text** -- [`Data2VecTextForCausalLM`] (Data2VecText model)\n",
            "        - **electra** -- [`ElectraForCausalLM`] (ELECTRA model)\n",
            "        - **ernie** -- [`ErnieForCausalLM`] (ERNIE model)\n",
            "        - **falcon** -- [`FalconForCausalLM`] (Falcon model)\n",
            "        - **fuyu** -- [`FuyuForCausalLM`] (Fuyu model)\n",
            "        - **git** -- [`GitForCausalLM`] (GIT model)\n",
            "        - **gpt-sw3** -- [`GPT2LMHeadModel`] (GPT-Sw3 model)\n",
            "        - **gpt2** -- [`GPT2LMHeadModel`] (OpenAI GPT-2 model)\n",
            "        - **gpt_bigcode** -- [`GPTBigCodeForCausalLM`] (GPTBigCode model)\n",
            "        - **gpt_neo** -- [`GPTNeoForCausalLM`] (GPT Neo model)\n",
            "        - **gpt_neox** -- [`GPTNeoXForCausalLM`] (GPT NeoX model)\n",
            "        - **gpt_neox_japanese** -- [`GPTNeoXJapaneseForCausalLM`] (GPT NeoX Japanese model)\n",
            "        - **gptj** -- [`GPTJForCausalLM`] (GPT-J model)\n",
            "        - **llama** -- [`LlamaForCausalLM`] (LLaMA model)\n",
            "        - **marian** -- [`MarianForCausalLM`] (Marian model)\n",
            "        - **mbart** -- [`MBartForCausalLM`] (mBART model)\n",
            "        - **mega** -- [`MegaForCausalLM`] (MEGA model)\n",
            "        - **megatron-bert** -- [`MegatronBertForCausalLM`] (Megatron-BERT model)\n",
            "        - **mistral** -- [`MistralForCausalLM`] (Mistral model)\n",
            "        - **mpt** -- [`MptForCausalLM`] (MPT model)\n",
            "        - **musicgen** -- [`MusicgenForCausalLM`] (MusicGen model)\n",
            "        - **mvp** -- [`MvpForCausalLM`] (MVP model)\n",
            "        - **open-llama** -- [`OpenLlamaForCausalLM`] (OpenLlama model)\n",
            "        - **openai-gpt** -- [`OpenAIGPTLMHeadModel`] (OpenAI GPT model)\n",
            "        - **opt** -- [`OPTForCausalLM`] (OPT model)\n",
            "        - **pegasus** -- [`PegasusForCausalLM`] (Pegasus model)\n",
            "        - **persimmon** -- [`PersimmonForCausalLM`] (Persimmon model)\n",
            "        - **plbart** -- [`PLBartForCausalLM`] (PLBart model)\n",
            "        - **prophetnet** -- [`ProphetNetForCausalLM`] (ProphetNet model)\n",
            "        - **qdqbert** -- [`QDQBertLMHeadModel`] (QDQBert model)\n",
            "        - **reformer** -- [`ReformerModelWithLMHead`] (Reformer model)\n",
            "        - **rembert** -- [`RemBertForCausalLM`] (RemBERT model)\n",
            "        - **roberta** -- [`RobertaForCausalLM`] (RoBERTa model)\n",
            "        - **roberta-prelayernorm** -- [`RobertaPreLayerNormForCausalLM`] (RoBERTa-PreLayerNorm model)\n",
            "        - **roc_bert** -- [`RoCBertForCausalLM`] (RoCBert model)\n",
            "        - **roformer** -- [`RoFormerForCausalLM`] (RoFormer model)\n",
            "        - **rwkv** -- [`RwkvForCausalLM`] (RWKV model)\n",
            "        - **speech_to_text_2** -- [`Speech2Text2ForCausalLM`] (Speech2Text2 model)\n",
            "        - **transfo-xl** -- [`TransfoXLLMHeadModel`] (Transformer-XL model)\n",
            "        - **trocr** -- [`TrOCRForCausalLM`] (TrOCR model)\n",
            "        - **whisper** -- [`WhisperForCausalLM`] (Whisper model)\n",
            "        - **xglm** -- [`XGLMForCausalLM`] (XGLM model)\n",
            "        - **xlm** -- [`XLMWithLMHeadModel`] (XLM model)\n",
            "        - **xlm-prophetnet** -- [`XLMProphetNetForCausalLM`] (XLM-ProphetNet model)\n",
            "        - **xlm-roberta** -- [`XLMRobertaForCausalLM`] (XLM-RoBERTa model)\n",
            "        - **xlm-roberta-xl** -- [`XLMRobertaXLForCausalLM`] (XLM-RoBERTa-XL model)\n",
            "        - **xlnet** -- [`XLNetLMHeadModel`] (XLNet model)\n",
            "        - **xmod** -- [`XmodForCausalLM`] (X-MOD model)\n",
            "    \n",
            "    The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are\n",
            "    deactivated). To train the model, you should first set it back in training mode with `model.train()`\n",
            "    \n",
            "    Args:\n",
            "        pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
            "            Can be either:\n",
            "    \n",
            "                - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
            "                  Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
            "                  user or organization name, like `dbmdz/bert-base-german-cased`.\n",
            "                - A path to a *directory* containing model weights saved using\n",
            "                  [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
            "                - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
            "                  this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
            "                  `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
            "                  PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
            "        model_args (additional positional arguments, *optional*):\n",
            "            Will be passed along to the underlying model `__init__()` method.\n",
            "        config ([`PretrainedConfig`], *optional*):\n",
            "            Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
            "            be automatically loaded when:\n",
            "    \n",
            "                - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
            "                  model).\n",
            "                - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
            "                  save directory.\n",
            "                - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
            "                  configuration JSON file named *config.json* is found in the directory.\n",
            "        state_dict (*Dict[str, torch.Tensor]*, *optional*):\n",
            "            A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
            "    \n",
            "            This option can be used if you want to create a model from a pretrained configuration but load your own\n",
            "            weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
            "            [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
            "        cache_dir (`str` or `os.PathLike`, *optional*):\n",
            "            Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
            "            standard cache should not be used.\n",
            "        from_tf (`bool`, *optional*, defaults to `False`):\n",
            "            Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
            "            `pretrained_model_name_or_path` argument).\n",
            "        force_download (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
            "            cached versions if they exist.\n",
            "        resume_download (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
            "            file exists.\n",
            "        proxies (`Dict[str, str]`, *optional*):\n",
            "            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
            "            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
            "        output_loading_info(`bool`, *optional*, defaults to `False`):\n",
            "            Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
            "        local_files_only(`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to only look at local files (e.g., not try downloading the model).\n",
            "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
            "            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
            "            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
            "            identifier allowed by git.\n",
            "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
            "            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
            "            execute code present on the Hub on your local machine.\n",
            "        code_revision (`str`, *optional*, defaults to `\"main\"`):\n",
            "            The specific revision to use for the code on the Hub, if the code leaves in a different repository than\n",
            "            the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\n",
            "            system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\n",
            "            allowed by git.\n",
            "        kwargs (additional keyword arguments, *optional*):\n",
            "            Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
            "            `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
            "            automatically loaded:\n",
            "    \n",
            "                - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
            "                  underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
            "                  already been done)\n",
            "                - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
            "                  initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
            "                  corresponds to a configuration attribute will be used to override said attribute with the\n",
            "                  supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
            "                  will be passed to the underlying model's `__init__` function.\n",
            "    \n",
            "    Examples:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import AutoConfig, AutoModelForCausalLM\n",
            "    \n",
            "    >>> # Download model and configuration from huggingface.co and cache.\n",
            "    >>> model = AutoModelForCausalLM.from_pretrained(\"bert-base-cased\")\n",
            "    \n",
            "    >>> # Update configuration during loading\n",
            "    >>> model = AutoModelForCausalLM.from_pretrained(\"bert-base-cased\", output_attentions=True)\n",
            "    >>> model.config.output_attentions\n",
            "    True\n",
            "    \n",
            "    >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
            "    >>> config = AutoConfig.from_pretrained(\"./tf_model/bert_tf_model_config.json\")\n",
            "    >>> model = AutoModelForCausalLM.from_pretrained(\n",
            "    ...     \"./tf_model/bert_tf_checkpoint.ckpt.index\", from_tf=True, config=config\n",
            "    ... )\n",
            "    ```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cd /root/.cache/huggingface/hub/models--INSAIT-Institute--BgGPT-7B-Instruct-v0.1/snapshots/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6glng5bcAg1",
        "outputId": "b5450ae4-f49d-4c2e-a126-bb4905e9c9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.cache/huggingface/hub/models--INSAIT-Institute--BgGPT-7B-Instruct-v0.1/snapshots\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9dPFj8IMpDu",
        "outputId": "6bfb6c62-b4b3-496f-af8a-48da6c1d846c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\",\n",
        "    #pretrained_model_name_or_path=\"/root/.cache/huggingface/hub/models--INSAIT-Institute--BgGPT-7B-Instruct-v0.1/snapshots/9c96e8cefc1079ef566cc46fc9b60b52dc36f583/\",\n",
        "    #pretrained_model_name_or_path=\"/root/.cache/huggingface/hub/models--INSAIT-Institute--BgGPT-7B-Instruct-v0.1\",\n",
        "    #model=\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    #use_flash_attn_2=True,\n",
        ")\n",
        "#device_map=\"auto\",\n",
        "#low_cpu_mem_usage=True, #or crashes\n",
        "#use_flash_attn_2=True,\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "8ce5bf7b8f4740a08edd1df00306dc7d",
            "adca3df4c0bb47fca206cb95a5626d8d",
            "39c88b2a0e0f4b20be0aa03412fccabb",
            "c09dae61614c460b819d001aaf1e0988",
            "9e8ab69002cc4aefa81e92f2f5345f3b",
            "1cda178088ad4eb79cf2124dbea0995c",
            "829778a66f3f47f4b148ac69b0923aec",
            "d6b2973f9cc7406bb2b5cdf4c662bf26",
            "e2938c37f8fa4541a789e770a3700499",
            "55b3b350b62144d1977120dbd8eeb8b0",
            "8663905a79754ac6b0bea954a399d2fc"
          ]
        },
        "id": "zjz9UBdoalFp",
        "outputId": "acd06cbd-1dbb-437f-bc0b-3465acce0389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ce5bf7b8f4740a08edd1df00306dc7d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_8IfSlcGV2tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = AutoModelForCausalLM.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\") #mistralai/Mistral-7B-v0.1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\")\n",
        "device = \"cuda\"\n",
        "#prompt = \"Как се правят принджиничени шляпунцели?\"\n",
        "#prompt = \"Откъде мога да си купя шляпунцели със зеленчуци и шоколад? Обичам да ги мажа с приндиджлячки, но оборвляквам шакалакщряк? Нали? Обясни ми\"\n",
        "prompt = \"Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "prompt = \"[s][INST]\"+prompt+\"[/INST]\"\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "model.to(device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "xaelArdPfcam",
        "outputId": "651e3a1c-9965-440b-d923-5d60be5ba04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо? Какви спортни постижения имат и в какъв спорт? [/INST]1. Яна Копчева - лека атлетика\\n2. Ивет Лалова-Колио - лека атлетика\\n3. Мирела Демирева - Лека атлетика\\n4. Евгения Стоева - Лека атлетика\\n5. Аштън Игнат - Лека атлетика\\n6. Габриела Петрова - хвърляне на копие\\n7. Радослава Мавродиева - хвърляне на гюле\\n\\nСамо три (Яна Копчева, Ивет Лалова-Колио и Габриела Петрова) от седем са жени.\\n\\nНякои от най-бързите времена за бягане, постигнати от тези спортисти, включват:\\n\\n1. Яна Копчева - 400 метра (50, 01 секунди)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "0qTPd7sClOg-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nRh2wvlajGhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "fname = \"llm_log.txt\" #in the current directory\n",
        "#if the file exists, it will be appended at (instead of wt)\n",
        "f = open(fname, \"at\", encoding=\"utf-8\")\n",
        "#now = datetime.now()\n",
        "#date_time = now.strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "\n",
        "def get_time_string():\n",
        "  return datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "\n",
        "#f.write ...)\n",
        "prompt_arr = []\n",
        "answer_arr = []\n",
        "prompt_and_answer = []\n",
        "max_new_tokens = 250"
      ],
      "metadata": {
        "id": "80BSoWMUlgTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qkvl46FvudY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EXPERIMENTS\n",
        "===========\n",
        "It doesn't load these mistrals, check formats etc.\n",
        "see also, try:\n",
        "INSAIT-Institute/BgGPT-7B-Instruct-v0.1\n",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "#TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
        "#TheBloke/mistral-7b-instruct-v0.1.Q6_K.gguf\n",
        "#TheBloke/mistral-7b-instruct-v0.1.Q8_0.gguf\n",
        "\n",
        "#TheBloke...: 7.63, 8.44, 10.20 G\n",
        "\"\"\"\n",
        "## LOAD ANOTHER MODEL | ЗАРЕДИ ДРУГ МОДЕЛ\n",
        "# m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
        "#m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\" #mistralai/Mistral-7B-Instruct-v0.1\" #\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
        "\n",
        "ms =[\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.1\"] #\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q6_K.gguf\" #TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "n = 0\n",
        "m = ms[n]\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=m,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "device = \"cuda\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8qRWFP3rllY-",
        "outputId": "9f91281f-474d-4060-cb2d-e3c687ad440a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1c7b6a1e6047>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Read a list of prompts. One per line\n",
        "openfile ...\n",
        "300, ...\n",
        "'''\n"
      ],
      "metadata": {
        "id": "IhSz1Y4zkmF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20.2.2024\n",
        "# MISTRAL - up to 4000 ... context length, but it seems it won't fit in Tesla T4 - about 2000 or something\n",
        "from textwrap import TextWrapper\n",
        "m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "device = \"cuda\"\n",
        "model.to(device)\n",
        "b_new_chat = True #insert \"[s][INST]...\"\n",
        "max_new_tokens = 500\n",
        "w = TextWrapper(width=80) #break long lines, default: 70 chars  w.wrap(string)\n",
        "b_autoregression = False #True\n",
        "\n",
        "def valid_range(m):\n",
        "  if m > 2400: return False # m = 2400\n",
        "  if m < 1: return False\n",
        "  return True\n",
        "\n",
        "def cycle():\n",
        "    global max_new_tokens\n",
        "    global b_new_chat\n",
        "    global b_autoregression\n",
        "    prompt = input(\"Enter prompt... \") #\"Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "    if prompt == \"q\": return False\n",
        "    if prompt == \"n\": b_new_chat = True; return True\n",
        "    if prompt == \"a\": b_autoregression = True; return True #GPT2, GPT3-like: not very smart\n",
        "    if prompt == \"i\": b_autoregression = False; return True #Instruct\n",
        "    if prompt == \"m\":\n",
        "      max = int(input(\"max_tokens=?...\"))\n",
        "      if valid_range(max): max_new_tokens = max; return True\n",
        "      else: print(\"Invalid max_new_tokens\")\n",
        "    if not b_autoregression:\n",
        "      if b_new_chat: prompt = \"[s][INST]\"+prompt+\"[/INST]\"; b_new_chat = False\n",
        "      else: prompt=\"[INST]\"+prompt+\"[/INST]\"\n",
        "    print(prompt)\n",
        "    for i in w.wrap(prompt): print(i) #input window is too small\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens, do_sample=True)\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    prompt_arr.append(prompt)\n",
        "    answer_arr.append(answer)\n",
        "    prompt_and_answer.append((prompt, answer))\n",
        "    #print(w.wrap(answer))\n",
        "    for i in w.wrap(answer): print(i) #print wrapped lines\n",
        "    f.write(\"\\n\"+get_time_string()+\"\\n???: \"+prompt+\"\\n\\n===: \"+answer+\"\\n------------\\n\")\n",
        "    return True\n",
        "\n",
        "def interact():\n",
        "   i = True\n",
        "   while i == True:\n",
        "      i = cycle()\n",
        "def culturno():\n",
        "    messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин. \"}\n",
        "    ]\n",
        "    #encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, do_sample=True)\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    print(answer)\n",
        "#culturno() #proba\n",
        "interact()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ahInASr5hb-W",
        "outputId": "2d943087-7779-4dd1-e89e-c072c2d94fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter prompt... Ти си математик, алгебра, уравнения. Реши уравнението x = 2x - 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ти си математик, алгебра, уравнения. Реши уравнението x = 2x - 1\n",
            "Ти си математик, алгебра, уравнения. Реши уравнението x = 2x - 1\n",
            "<s> Ти си математик, алгебра, уравнения. Реши уравнението x = 2x - 1 и покажи\n",
            "работата си. Ограничете отговора си до математически изрази и символи. [/INST]x\n",
            "= 2x - 1  Трябва да намерим x, така че можем да поставим всички членове x от\n",
            "едната страна на уравнението:  x - 2x = -1  Сега можем да комбинираме подобни\n",
            "термини от лявата страна:  -1x = -1  За да намерим x, можем да разделим двете\n",
            "страни на -1:  -1x / -1 = -1 / -1  х = 1</s>\n",
            "Enter prompt... i\n",
            "Enter prompt... Иван има три круши, Драган има 12 банана. Пенка има 1 кило краставици. Кой има най-много плодове?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[s][INST]Иван има три круши, Драган има 12 банана. Пенка има 1 кило краставици. Кой има най-много плодове?[/INST]\n",
            "[s][INST]Иван има три круши, Драган има 12 банана. Пенка има 1 кило краставици.\n",
            "Кой има най-много плодове?[/INST]\n",
            "<s> [s][INST]Иван има три круши, Драган има 12 банана. Пенка има 1 кило\n",
            "краставици. Кой има най-много плодове?[/INST]Нека преобразуваме всички храни в\n",
            "една единица. За този пример, нека използваме \"плод\". 1 плод може да бъде:  1.\n",
            "Крум 2. Банан 3. 1 кг краставица  Иван има 3 круши, което може да се преобразува\n",
            "в 3 плода. Драган има 12 банана, което може да се сравни с 12 плодове. Пенка има\n",
            "1 килограм краставица, което е еквивалентно на 1 плод.  След такова сравнение\n",
            "имаме: - Иван има 3 плода. - Драган има 12 плода. - Пенка има 1 плод.  Така че\n",
            "този път човекът с най-много плодове е Драган.  Забележка на редактора:\n",
            "Плодовете са сравнение за тази конкретна задача. Всяка задача може да изисква\n",
            "различни методи за събиране на факти или сравнения. В сценариите от реалния свят\n",
            "различните видове храни не могат да се съпоставят по честен начин (т.е. банан за\n",
            "банан и кило краставици за кило краставици).</s>\n",
            "Enter prompt... А кой е с най-много зеленчуци?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]А кой е с най-много зеленчуци?[/INST]\n",
            "[INST]А кой е с най-много зеленчуци?[/INST]\n",
            "<s> [INST]А кой е с най-много зеленчуци?[/INST]Добре, ето една шега за вас!\n",
            "Защо градинарите винаги се смеят, когато започнат да ядат зеленчуци?  Защото\n",
            "правят \"гре\"!  Не забравяйте, че храната е забавна и трябва да се наслаждавате\n",
            "на това, което ядете, приятели!</s>\n",
            "Enter prompt... Ако Сталинка е леля на Страцимира, а тя е майка на Пенка, каква е Пенка на Сталинка?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Ако Сталинка е леля на Страцимира, а тя е майка на Пенка, каква е Пенка на Сталинка?[/INST]\n",
            "[INST]Ако Сталинка е леля на Страцимира, а тя е майка на Пенка, каква е Пенка на\n",
            "Сталинка?[/INST]\n",
            "<s> [INST]Ако Сталинка е леля на Страцимира, а тя е майка на Пенка, каква е\n",
            "Пенка на Сталинка?[/INST]За да разберете връзката между Пенка и Сталинка, трябва\n",
            "да разгледаме дадените твърдения стъпка по стъпка:  1. Сталинка е леля на\n",
            "Страцимира. 2. Страцимира е майка на Пенка.  Въз основа на предоставената\n",
            "информация можем да заключим, че Пенка е внучка на Сталинка. По този начин Пенка\n",
            "е пра-племенница на Сталинка.</s>\n",
            "Enter prompt... Изчисли израза: y = x*x, където x = 2; Изчисли x^3 където x = 3. Колко е SQRT(125.67)? Колко е третичен корен от 90?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Изчисли израза: y = x*x, където x = 2; Изчисли x^3 където x = 3. Колко е SQRT(125.67)? Колко е третичен корен от 90?[/INST]\n",
            "[INST]Изчисли израза: y = x*x, където x = 2; Изчисли x^3 където x = 3. Колко е\n",
            "SQRT(125.67)? Колко е третичен корен от 90?[/INST]\n",
            "<s> [INST]Изчисли израза: y = x*x, където x = 2; Изчисли x^3 където x = 3. Колко\n",
            "е SQRT(125.67)? Колко е третичен корен от 90?[/INST]1. y = x*x: Когато x = 2, y\n",
            "= 2*2 = 4 2. x^3: Когато x = 3, x = 3^3 = 3*9 = 27 3. SQRT(125,67): √125,67 ≈\n",
            "11,18 (Приблизително) 4. α-корен от 90: Кубичният корен (α) от 90 може да се\n",
            "изчисли, както следва: α(90) = (90^(1/3)) = (3^2*10^(1/3)) ≈ 9,01</s>\n",
            "Enter prompt... Създай папка в Линукс и се премести в нея.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Създай папка в Линукс и се премести в нея.[/INST]\n",
            "[INST]Създай папка в Линукс и се премести в нея.[/INST]\n",
            "<s> [INST]Създай папка в Линукс и се премести в нея.[/INST]Първо трябва да\n",
            "отворите терминал (Ctrl+Alt+T) и да създадете нова папка. Ето командата за\n",
            "създаване на папка:  ``` mkdir newfolder ```  Можете да промените името на\n",
            "„newfolder“ на каквото искате име на папка, например „mynewfolder“.  След като\n",
            "папката бъде създадена, можете да използвате командата „cd“, за да промените\n",
            "текущата директория към новосъздадената папка. Заменете `newfolder` с вашето\n",
            "избрано име на папка:  ``` cd newfolder ```  Сега трябва да сте в папката\n",
            "„mynewfolder“ (или която и да е друга папка, която сте създали).</s>\n",
            "Enter prompt... Как да извикам SITL и mavlink с команди в bash, какви портове да задам. За симулация на дрон с Газебо, iris.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Как да извикам SITL и mavlink с команди в bash, какви портове да задам. За симулация на дрон с Газебо, iris.  [/INST]\n",
            "[INST]Как да извикам SITL и mavlink с команди в bash, какви портове да задам. За\n",
            "симулация на дрон с Газебо, iris.  [/INST]\n",
            "<s> [INST]Как да извикам SITL и mavlink с команди в bash, какви портове да\n",
            "задам. За симулация на дрон с Газебо, iris.  [/INST]За да извикате SITL\n",
            "(Software In The Loop) и MAVLink, можете да използвате следните команди в Bash:\n",
            "1. Стартирайте Gazebo SITL: ```bash gazebo -s your_scene.scm ``` Заменете\n",
            "\"your_scene.scm\" с пътя към вашия сценарий Gazebo за дрон.  1. Активирайте\n",
            "MAVLink: ```bash mavlink_gateway -- Gazebo_host=localhost ``` Този ред ще\n",
            "активира MAVLink порта локално.  1. Създайте MAVLink UDP връзка: ```bash\n",
            "mavlink_send_socket -- Gazebo_host=localhost Gazebo_port=4444\n",
            "MAVLink_host=localhost MAVLink_port=4445 ``` Това създава UDP връзка между\n",
            "Gazebo и MAVLink, които работят на различни портове. Портовете 4444 и 4445\n",
            "трябва да са отворени.  Сега можете да стартирате вашия код на дрон, за да\n",
            "взаимодействате със симулирания дрон в Gazebo. Не забравяйте да настроите\n",
            "съответно симулацията на дрон и комуникационните портове в кода си.  Ако имате\n",
            "нужда от допълнителни насоки или ако срещате проблеми с конфигурирането, не се\n",
            "колебайте да попитате!</s>\n",
            "Enter prompt... Ти си разказвач, писател на приказки. Напиши предистория на \"Котката изпи камъка и литна под нанагорнището\" от 200 думи. После завръзка кулминация около 300 думи. Напиши случка с друго животно и човек, които се срещат, и намират нещо - избери си какво. Накрая около 150 думи заключение, което да бъде с щастлив край и да включва компютри.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Ти си разказвач, писател на приказки. Напиши предистория на \"Котката изпи камъка и литна под нанагорнището\" от 200 думи. После завръзка кулминация около 300 думи. Напиши случка с друго животно и човек, които се срещат, и намират нещо - избери си какво. Накрая около 150 думи заключение, което да бъде с щастлив край и да включва компютри.[/INST]\n",
            "[INST]Ти си разказвач, писател на приказки. Напиши предистория на \"Котката изпи\n",
            "камъка и литна под нанагорнището\" от 200 думи. После завръзка кулминация около\n",
            "300 думи. Напиши случка с друго животно и човек, които се срещат, и намират нещо\n",
            "- избери си какво. Накрая около 150 думи заключение, което да бъде с щастлив\n",
            "край и да включва компютри.[/INST]\n",
            "<s> [INST]Ти си разказвач, писател на приказки. Напиши предистория на \"Котката\n",
            "изпи камъка и литна под нанагорнището\" от 200 думи. После завръзка кулминация\n",
            "около 300 думи. Напиши случка с друго животно и човек, които се срещат, и\n",
            "намират нещо - избери си какво. Накрая около 150 думи заключение, което да бъде\n",
            "с щастлив край и да включва компютри.[/INST]Предистория:  В причудливия и\n",
            "магически град Катимор, хората живееха със странно благоговение към особен вид\n",
            "котки, за които се вярваше, че притежават силата да превърнат всеки камък в\n",
            "злато. Сред тези очарователни създания беше Астарот, котарак с огнени очи, който\n",
            "живееше в древен замък на върха на хълм. Приказка разказваше как Астарот веднъж\n",
            "изпил магическа течност от вълшебна сфера, открита в дълбините на замъка, и\n",
            "полетял нагоре по хълма, превръщайки подножието му в злато. Местните хора и\n",
            "техните потомци запазиха историята жива през вековете, вплитайки я в тъканта на\n",
            "ежедневието си.  Една топла лятна сутрин, докато слънцето хвърляше златни лъчи\n",
            "върху Катимор, група странници пристигнаха в града. Сред тях беше Адриан, млад и\n",
            "амбициозен изобретател, който търсеше вълшебни артефакти, които биха могли да\n",
            "революционизират света на технологиите. Адриан беше придружаван от верния си\n",
            "спътник, лисица на име Фоукс, която притежаваше необикновена способност да\n",
            "разбира и превежда древните езици.  Завръзка:  Докато Адриан и Фоукс изследваха\n",
            "града, те се натъкнаха на древния замък на върха на хълма. Заинтригувани от\n",
            "приказката за Астарот, те се изкачват на хълма и търсят самия Астарот. Докато се\n",
            "изкачваха, те се натъкнаха на различни жители на Катимор, всеки от които знаеше\n",
            "историята по малко по-различен начин. Достигайки върха, те откриха скрита врата\n",
            "в стената и влязоха в замъка.  Вътре Адриан и Фоукс намериха свещения кладенец,\n",
            "където се смяташе, че Астарот е изпил магическата течност. Интригата им се\n",
            "изостри, когато чуха слаб звук отгоре. Приближавайки се към звука, те откриха\n",
            "малка дупка в та\n",
            "Enter prompt... m\n",
            "max_tokens=?...2000\n",
            "Enter prompt... Ти си разказвач, писател на приказки. Напиши предистория на \"Котката изпи камъка и литна под нанагорнището\" от 200 думи. После завръзка кулминация около 300 думи. Напиши случка с друго животно и човек, които се срещат, и намират нещо - избери си какво. Накрая около 150 думи заключение, което да бъде с щастлив край и да включва компютри.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Ти си разказвач, писател на приказки. Напиши предистория на \"Котката изпи камъка и литна под нанагорнището\" от 200 думи. После завръзка кулминация около 300 думи. Напиши случка с друго животно и човек, които се срещат, и намират нещо - избери си какво. Накрая около 150 думи заключение, което да бъде с щастлив край и да включва компютри.[/INST]\n",
            "[INST]Ти си разказвач, писател на приказки. Напиши предистория на \"Котката изпи\n",
            "камъка и литна под нанагорнището\" от 200 думи. После завръзка кулминация около\n",
            "300 думи. Напиши случка с друго животно и човек, които се срещат, и намират нещо\n",
            "- избери си какво. Накрая около 150 думи заключение, което да бъде с щастлив\n",
            "край и да включва компютри.[/INST]\n",
            "<s> [INST]Ти си разказвач, писател на приказки. Напиши предистория на \"Котката\n",
            "изпи камъка и литна под нанагорнището\" от 200 думи. После завръзка кулминация\n",
            "около 300 думи. Напиши случка с друго животно и човек, които се срещат, и\n",
            "намират нещо - избери си какво. Накрая около 150 думи заключение, което да бъде\n",
            "с щастлив край и да включва компютри.[/INST]Част 1 - Предисторията:  Дълбоко в\n",
            "омагьосаната гора имаше малко селце, наречено Серенавис. Необикновеното село\n",
            "живееше под магията на вековен камък, чиято тайна беше известна само на местната\n",
            "вещица, Матилда. В края на всяко лято Матилда събираше съкровища от цялото\n",
            "кралство Венделайна и щеше да ги принесе в жертва на Камъка на Серенавис.\n",
            "Жителите на селото вярваха, че този ритуал защитава селото им от зли сили и\n",
            "злоба от векове.  Тази година беше различно. Матилда, на 101 години, почувства,\n",
            "че силите й отслабват. Изпълнение с угризения и страх, тя осъзна, че времето й\n",
            "изтича. Полагайки последните си сили, тя заговори със селото на събранието им.\n",
            "Тя им разказа за камъка, магията и нейната история. Когато дойде времето тя\n",
            "гордо да доведе наследника, жителите на селото бяха ужасени да научат, че тя не\n",
            "е намерила такъв.  Част 2 – Завръзката:  Без наследник селото щеше да бъде\n",
            "оставено беззащитно на произвола на съдбата. Матилда призова населението на\n",
            "селото да потърси нов пазител сред тях. Никой не пристъпи напред. Изведнъж едно\n",
            "момче на име Алвин излезе напред. Точно тогава мъдра, стара котка на име Елара\n",
            "се стрелна към него.  — Ти — промълви Матилда с треперещ глас.  „Моля?“ – попита\n",
            "колебливо Алвин.  — Елара ще бъде твоят водач — рече Матилда.  „Какво?“ – извика\n",
            "Алвин, гледайки Елара с широко отворени очи.  „Изпитанието вече започна“, каза\n",
            "Матилда.  Част 3 – Кулминацията:  Алвин и Елара бързо се опознаха. След няколко\n",
            "дни Алвин разкри на Елара необикновената си дарба да поправя машини. През\n",
            "следващите седмици те изследваха гората, търсейки предмети и древни легенди. И\n",
            "така, един съдбовен ден те откриха тайна стая, пълна с ръждясали машини, които\n",
            "никой не беше докосвал от векове.  Част 4 – Развръзката:  Въоръжен с нови знания\n",
            "и разбиране за историята на камъка, Алвин реши да се изправи срещу най-големите\n",
            "си страхове - и да превърне древните машини в нещо красиво. Алвин и Елара\n",
            "работиха неуморно, използвайки силите си, за да възстановят ръждясалите машини и\n",
            "да влеят нов живот в тях.  Не след дълго тяхното творение - чудо на елегантност\n",
            "и сила - работеше безупречно, излъчвайки топла светлина, която обгърна селото и\n",
            "разсея облаците над него. В този момент старият Камък на Серенавис започна да\n",
            "блести ярко, излъчвайки мощна енергия и признавайки новото си завещание.  Част 5\n",
            "- Заключението:  Жителите на селото гледаха със страхопочитание как мощността на\n",
            "камъка се предава на новото устройство, образувайки невидим щит около селото,\n",
            "защитавайки го от всяка лоша сила. И докато слънцето залязваше над Серенавис,\n",
            "селото беше преизпълнено с надежда, любов и единство. Беше намерено ново\n",
            "завещание, което да защитава селото, оставяйки всички в селото щастливи\n",
            "завинаги.  И така, приказката за котката, която погълна камъка и полетя на\n",
            "вододела на нанагорнището, намери своето продължение - история за единството,\n",
            "креативността и магията на живота. А иронията беше, че в този случай камъните са\n",
            "се превърнали в машини, носейки по-голяма магия отпреди.</s>\n",
            "Enter prompt... Направи граматичен разбор на \"Котката изпи камъка и литна под нанагорнището\".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Направи граматичен разбор на \"Котката изпи камъка и литна под нанагорнището\".[/INST]\n",
            "[INST]Направи граматичен разбор на \"Котката изпи камъка и литна под\n",
            "нанагорнището\".[/INST]\n",
            "<s> [INST]Направи граматичен разбор на \"Котката изпи камъка и литна под\n",
            "нанагорнището\".[/INST]Има две очевидни граматически грешки в изречението\n",
            "„Котката изпи камъка и литна под нанагорнището“:  1. Глаголът „изпи“ не се\n",
            "съгласува с обектива на изречението, „камъкът“, по отношение на число (ед. срещу\n",
            "мн.). Трябва да бъде \"изяде\".  2. Съществителното \"включено\" се използва\n",
            "неправилно като прилагателно тук, променяйки \"камък\", вместо да го описва.\n",
            "Трябва да бъде \"включено\".  Ето коригираното изречение за по-добро разбиране:\n",
            "\"Котката изяде камъка и избяга под нанагорнището.\"</s>\n",
            "Enter prompt... Изброй 30 употреби на \"котка\" с глагол, по схемата \"Котка ...\". \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]Изброй 30 употреби на \"котка\" с глагол, по схемата \"Котка ...\". [/INST]\n",
            "[INST]Изброй 30 употреби на \"котка\" с глагол, по схемата \"Котка ...\". [/INST]\n",
            "<s> [INST]Изброй 30 употреби на \"котка\" с глагол, по схемата \"Котка ...\".\n",
            "[/INST]1. Котка бозае кравата. 2. Котка дои кравата. 3. Котка тъче крава. 4.\n",
            "Котка чете крава. 5. Котка рисува крава. 6. Котка пише книга на крава. 7. Котка\n",
            "лети до луната. 8. Котка копае луната. 9. Котка чете приказка на крава. 10.\n",
            "Котка играе карти с крава. 11. Котка пее на крава. 12. Котка брои звездите. 13.\n",
            "Котка брои листа. 14. Котка носи играчка на кравата. 15. Котка подрежда пъзел.\n",
            "16. Котка украсява крава. 17. Котка учи кравата на номера. 18. Котка язди крава.\n",
            "19. Котка шие палто на крава. 20. Котка учи кравата да рисува. 21. Котка води\n",
            "кравата на поклонение. 22. Котка плува до Австралия. 23. Котка се смее на шега.\n",
            "24. Котка играе на криеница с крави. 25. Котка дава на кравата торта. 26. Котка\n",
            "води крава на училище. 27. Котка рисува цветя върху кравата. 28. Котка се шегува\n",
            "с крава. 29. Котка купува подарък за крава. 30. Котка решава математически\n",
            "проблеми с крава.</s>\n",
            "Enter prompt... Дай поне 30 употреби на \"котка\" с глагол, по схемата \"Котка + глагол\" от две думи.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]Дай поне 30 употреби на \"котка\" с глагол, по схемата \"Котка + глагол\" от две думи.[/INST]\n",
            "[INST]Дай поне 30 употреби на \"котка\" с глагол, по схемата \"Котка + глагол\" от\n",
            "две думи.[/INST]\n",
            "<s> [INST]Дай поне 30 употреби на \"котка\" с глагол, по схемата \"Котка + глагол\"\n",
            "от две думи.[/INST]1. Играйте с котка 2. Хранете котка 3. Почистете котката 4.\n",
            "Учете котка 5. Разхождайте котка 6. Почивайте с котка 7. Подстригване на котка\n",
            "8. Разговаряйте с котка 9. Галете котка 10. Настанете котка 11. Адаптирайте се\n",
            "към котка 12. Научете котка 13. Приспате котка 14. Излекувайте котка 15.\n",
            "Развеселете котка 16. Слушайте котка 17. Напътствайте котка 18. Разходка с котка\n",
            "19. Осиновете котка 20. Грижете се за котка 21. Обучете котка 22. Отглеждайте\n",
            "котка 23. Споделяйте моменти с котка 24. Прегърнете котка 25. Направете котка\n",
            "щастлива 26. Изградете доверие с котка 27. Осигурете комфорт на котка 28.\n",
            "Осиновете котка 29. Посетете ветеринарен лекар с котка 30. Защитете котка</s>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-45d204f1cc62>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#culturno() #proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-45d204f1cc62>\u001b[0m in \u001b[0;36minteract\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m    \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m    \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mculturno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     messages = [\n",
            "\u001b[0;32m<ipython-input-19-45d204f1cc62>\u001b[0m in \u001b[0;36mcycle\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mb_new_chat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mb_autoregression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter prompt... \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\"Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"q\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"n\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb_new_chat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\"\n",
        "wr = w.wrap(s)\n",
        "print(s); print(wr)\n",
        "for i in wr: print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzt82L5YmlM5",
        "outputId": "99ff29b8-f628-45bd-af95-4b5e79000be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\n",
            "['Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо,', 'Вазов и Лем. Напиши текст за песен, който започва със селската баня,', 'голям кеф, градската десница, софийската левица с голямата... каца на', 'планетата на маймуните във Вселената на Стартрек, при Хари Потър и', 'философския камък на Властелина на пръстените с лазерния меч на Люк.', 'Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.']\n",
            "Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо,\n",
            "Вазов и Лем. Напиши текст за песен, който започва със селската баня,\n",
            "голям кеф, градската десница, софийската левица с голямата... каца на\n",
            "планетата на маймуните във Вселената на Стартрек, при Хари Потър и\n",
            "философския камък на Властелина на пръстените с лазерния меч на Люк.\n",
            "Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин. \"\n",
        "]\n",
        "# {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "#? why the assistant as well"
      ],
      "metadata": {
        "id": "sdbwldNm0epT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8-_fnzparaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2cuHYah0e_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3i1hIrYwo8WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_and_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1VQ0--zizna",
        "outputId": "b5f9104f-5274-4875-eabd-b251eaa93c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Колко от тях са жени?', '<s> Колко от тях са жени?\\nОбщата средна възраст на всички жени е $30$ години. Каква е средната възраст, в години, на мъжете? Изразете отговора си като десетичен знак до най-близката десета. [/INST]Да приемем, че броят на жените е $w$ и броят на мъжете е $m$.\\nДадено ни е, че $w + m = 32$ и средната възраст на всичките $w + m$ души е $30$ години.\\nСредната възраст може да се изчисли, като се вземе сумата от всички възрасти и се раздели на броя на хората.\\nТака че имаме уравнението $(30)(w + m) = \\\\frac{30}{32}(w + 180)$ (тъй като средната възраст на мъжете е $180$ години).\\nОпростявайки това уравнение, получаваме $')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLOSE\n",
        "f.close()"
      ],
      "metadata": {
        "id": "eQ4Bve9ljiUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Програмирай на Бейсик рисуване на триизмерен цилиндър в перспектива.\"\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=500, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "EaEQ6a0JNiPG",
        "outputId": "4264f5ed-6e77-4194-cfdf-d98b4558ee20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> Програмирай на Бейсик рисуване на триизмерен цилиндър в перспектива. Програмата трябва да подкани потребителя за радиуса и височината на цилиндъра, след което да ги покаже от различни ъгли. Използвайте ASCII знаци за създаване на вашата картина. [/INST]За да създадем програма на BASIC, която рисува 3D цилиндър в перспектива, ще използваме текст и символи, за да представим страничната и горната част на цилиндъра. Ето примерна програма, която можете да използвате като отправна точка:\\n\\n```basic\\n10 DISP \"Welcome to the Cylinder Drawer!\"\\n20 DISP \"Enter Radius and Height:\"\\n30 INPUT \"Radius: \"; r\\n40 INPUT \"Height: \"; h\\n50 CLS\\n60 DRAW 150, r*2+5, 5, r*2+5, -5\\n70 DRAW r*2+5, 0, -h, r*2+5, h\\n80 DRAW r*2-5, 0, -h, r*2-5, h\\n90 DRAW 150, r*2-5, 5, 150, r*2+5\\n100 PRINT \"Press any key...\"\\n110 INKEY$\\n120 CLS\\n130 DRAW 150, r*2+5, 5, r*2+5, -30\\n140 DRAW r*2+5, -30, 5, r*2+5, 30\\n150 DRAW r*2-5, -30, 5, r*2-5, 30\\n160 DRAW 150, r*2-5, 5, 150, r*2+5\\n170 PRINT \"Press any key...\"\\n180 INKEY$\\n190 CLS\\n200 DRAW 150, r*2+5, 5, r*2+5, -60\\n210 DRAW r*2+5, -60, 5, r*2+5, '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vJKEzme5V1A4"
      }
    }
  ]
}