{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a8564e3b2b8a46c0b80739f1a1701dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_698005d6f544434ea1c9b1e554f9a023",
              "IPY_MODEL_bb75642839f140dc8a3ad493ecb307aa",
              "IPY_MODEL_a9f44f27b3a74fee8bc378b87617c223"
            ],
            "layout": "IPY_MODEL_c86fe12fe4ff4f2c9e990159dabdf551"
          }
        },
        "698005d6f544434ea1c9b1e554f9a023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a13bc1a72eb43e9bf8bada97c5d2f95",
            "placeholder": "​",
            "style": "IPY_MODEL_7481b739f23a43778440234b1cf91838",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bb75642839f140dc8a3ad493ecb307aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7635ee578ecf459d8483686370be2ae4",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efad7d91a6764dbd9ba8ef2af74651aa",
            "value": 2
          }
        },
        "a9f44f27b3a74fee8bc378b87617c223": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7d0c267903d43b6aeaacb4fb406fae3",
            "placeholder": "​",
            "style": "IPY_MODEL_c6110f5569044067839b0d4f85fe5c7f",
            "value": " 2/2 [00:00&lt;00:00,  2.90it/s]"
          }
        },
        "c86fe12fe4ff4f2c9e990159dabdf551": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a13bc1a72eb43e9bf8bada97c5d2f95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7481b739f23a43778440234b1cf91838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7635ee578ecf459d8483686370be2ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efad7d91a6764dbd9ba8ef2af74651aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7d0c267903d43b6aeaacb4fb406fae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6110f5569044067839b0d4f85fe5c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools/blob/main/bggpt_sacred_computer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k_lKxlssvJTU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GP7wu5oYovWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g8B-4Fo6vIEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **BgGPT в Colab - безплатно на Tesla T4 или TPU**\n",
        "#### Автор: Тодор Арнаудов - Тош от **СВЕЩЕНИЯТ СМЕТАЧ** - Институт за Мислещи машини, творчество и развитие на човека - основан през 2000 г.\n",
        "* http://github.com/twenkid\n",
        "* http://artificial-mind.blogspot.com\n",
        "* http://research.twenkid.com\n",
        "* http://eim.twenkid.com\n",
        "\n",
        "**Видеоръководства:** от канала \"Twenkid Studio - Artificial Mind (todprog): https://www.youtube.com/channel/UCgyhnsM9ed292HUAObXUsvw\n",
        "\n",
        "**Версии**:\n",
        "* 19.2.2024: първа, единични заявки;\n",
        "* 20.2: Цикъл с извиквания, промяна на дължината на породения текст, [INST]...[/INST], промяна на max_... и др.\n",
        "* 22.2: вкл/изкл [INST], ... TextWrapper ... и др.\n",
        "* 2x.2: batch requests, Options class etc.\n",
        "\n",
        "Тази тетрадка е качена в: https://github.com/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools\n",
        "...\n",
        "\n",
        "Тош, тогава още тийнейджър, е автор на \"пророческата\" интердисциплинарна **\"Теория на разума и Вселената (2001-2004)\"** https://github.com/Twenkid/Theory-of-Universe-and-Mind за общ изкуствен интелект и др., която е в основата на **Първия в света интердисциплинарен курс по Универсален изкуствен разум (Artificial General Intelligence)**, който създава и преподава в ПУ \"Паисий Хилендарски\" през 2010 г. https://artificial-mind.blogspot.com/2010/04/universal-artificial-intelligence.html\n",
        "Тош е автор на **Стратегията за развитие на България чрез свръхинтердисциплинарен институт за Изкуствен интелект и създаване на универсални мислещи машини от 2003 г.**, която INSAIT преоткрива с 20 години закъснение и над 200 милиона лв по-голям бюджет: **\"Как бих инвестирал един милион с най-голяма полза за развитието на страната\"**:\n",
        "* https://www.oocities.org/todprog/ese/proekt.htm\n",
        "* https://artificial-mind.blogspot.com/2020/07/interdisciplinary-research-institute.html\n",
        "... на безплатните синтезатори на реч\n",
        "* **\"Глас 2004\" и \"Тошко 2\"**: https://github.com/Twenkid/Toshko_2\n",
        "\n",
        "* ... на **безплатния интелигентнен речник-помощник на преводача \"Smarty\" (2007)** който беше най-\"умното\" подобно приложение в света, с наченки на разбиране на контекста: https://github.com/Twenkid/Smarty\n",
        "\n",
        "* ...на всестранния проект **\"Вси, или Специалист по всичко\"** за инфраструктура за Общ изкуствен интелект, 2022:\n",
        "https://github.com/Twenkid/Vsy-Jack-Of-All-Trades-AGI-Bulgarian-Internet-Archive-And-Search-Engine\n",
        "\n",
        "* Обучава голям езиков модел на българскив Колаб **GPT2-BG Medium, обучен в Колаб през 2021 г.**: - ръководство за обучение, за пораждане на по-\"творчески\" текстове и самият модел:\n",
        "https://github.com/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools\n",
        "\n",
        "и др.\n",
        "\n",
        "Интервю от 2009 г.: **Тодор Арнаудов: Ще създам мислеща машина, която ще се самоусложнява.\n",
        "Фантазьори и авантюристи правят великите открития. Работата на скептиците е да отричат, а после да не вярват на собствените си очи**\n",
        "\n",
        "https://artificial-mind.blogspot.com/2009/11/dreamers-and-adventurists-do-big.html\n",
        "\n",
        "**СВЕЩЕНИЯТ СМЕТАЧ** търси всякакви партньори, съдружници, спонсори, съмишленици. Виж например \"issue\"-то в проекта \"Вси\":\n",
        "https://github.com/Twenkid/Vsy-Jack-Of-All-Trades-AGI-Bulgarian-Internet-Archive-And-Search-Engine/issues/10\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "19.2.2024\n",
        "\n",
        "BgGPT:\n",
        "\n",
        "https://bggpt.ai\n",
        "\n",
        "https://huggingface.co/INSAIT-Institute/BgGPT-7B-Instruct-v0.1/blob/main/README.md?code=true\n",
        "\n",
        "https://bggpt.ai/blog/2024-02-18-launching-the-first-free-and-open-bulgarian-llm/\n",
        "\n",
        "https://huggingface.co/INSAIT-Institute/BgGPT-7B-Instruct-v0.1/tree/main\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PD8yuacUV4bI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwTabNcJXaxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "31vvLOvzZcwh",
        "outputId": "3345f093-4334-40ef-8836-b000c5723b57"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f7e957707e4b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huggingface-cli download INSAIT-Institute/BgGPT-7B-Instruct-v0.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ],
      "source": [
        "# !huggingface-cli download INSAIT-Institute/BgGPT-7B-Instruct-v0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Инсталиране на библиотеки ..."
      ],
      "metadata": {
        "id": "ZLTBxZS1KpQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging ninja\n",
        "!pip install accelerate\n",
        "#!pip install flash-attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdfHe2_2Zu1t",
        "outputId": "044e44d7-7f5e-48e9-f888-3b9459677f01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "dWSlbIaEbgKE",
        "outputId": "08fbbd83-71fb-4e31-c5f4-6236f753ea49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-04578d045c84>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# List of Available Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Списък с достъпни модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Заредете модела\n",
        "Може да бъде и друг: с път до адрес в huggingface, профил/име-на-модел. Виж в следващата клетка."
      ],
      "metadata": {
        "id": "VzXKbK5p-KRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upgrade transformers for flash_attn_2"
      ],
      "metadata": {
        "id": "2LW7lIWKJsgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "7CT7gIKIJqDu",
        "outputId": "ea8a489b-b38b-44f1-f321-3d46083569dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.37.2\n",
            "    Uninstalling transformers-4.37.2:\n",
            "      Successfully uninstalled transformers-4.37.2\n",
            "Successfully installed transformers-4.38.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only Ampere or newer! (RTX 3000 for PC etc. On Colab now it's just Tesla T4, 2018)\n",
        "#!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m5aGZ_aKu6Y",
        "outputId": "4003ac17-3241-4f40-90c8-302960d180ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.5.5.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu121)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.5.5-cp310-cp310-linux_x86_64.whl size=120352304 sha256=e70f2f0d4fae98c9ff9742404f3e067bbae56bd3212578ff81c30f73c5ae1a15\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/67/52/8b6d5fcffdd9e1ec868f554cdef8f03eedb4bf4dcac852fca2\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: einops, flash-attn\n",
            "Successfully installed einops-0.7.0 flash-attn-2.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    #use_flash_attention_2 = True  #Only for Geforce 3000+ etc.\n",
        ")\n",
        "#use_flash_attn_2=True,\n",
        "#If using GPU, uncomment low_cpu_mem\n",
        "#If using CPU: comment it (but probably out of memory... it couldn't fit the model)\n",
        "#device_map=\"auto\",\n",
        "#low_cpu_mem_usage=True, # or crashes\n",
        "#use_flash_attn_2=True,\n",
        "# mistralai/Mistral-7B-Instruct-v0.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "a8564e3b2b8a46c0b80739f1a1701dd2",
            "698005d6f544434ea1c9b1e554f9a023",
            "bb75642839f140dc8a3ad493ecb307aa",
            "a9f44f27b3a74fee8bc378b87617c223",
            "c86fe12fe4ff4f2c9e990159dabdf551",
            "5a13bc1a72eb43e9bf8bada97c5d2f95",
            "7481b739f23a43778440234b1cf91838",
            "7635ee578ecf459d8483686370be2ae4",
            "efad7d91a6764dbd9ba8ef2af74651aa",
            "d7d0c267903d43b6aeaacb4fb406fae3",
            "c6110f5569044067839b0d4f85fe5c7f"
          ]
        },
        "id": "zjz9UBdoalFp",
        "outputId": "d42d13e4-22ff-4c3e-8b31-5657574498b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8564e3b2b8a46c0b80739f1a1701dd2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / | grep bggpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Npu8I1xNKWD9",
        "outputId": "80bf82e5-404c-419f-c24b-f230b3d69ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘/proc/62/task/62/net’: Invalid argument\n",
            "find: ‘/proc/62/net’: Invalid argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of Available Models\n",
        "# Списък с достъпни модели\n",
        "# help(AutoModelForCausalLM.from_pretrained)"
      ],
      "metadata": {
        "id": "zRtq8eV1_OHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "e404dda8-207d-44f9-da7b-7352b5aaa61e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7dc02a2f820e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# List of Available Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Списък с достъпни модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_8IfSlcGV2tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EXPERIMENTS\n",
        "===========\n",
        "It doesn't load these mistrals, check formats etc.\n",
        "see also, try:\n",
        "INSAIT-Institute/BgGPT-7B-Instruct-v0.1\n",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "#TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
        "#TheBloke/mistral-7b-instruct-v0.1.Q6_K.gguf\n",
        "#TheBloke/mistral-7b-instruct-v0.1.Q8_0.gguf\n",
        "\n",
        "#TheBloke...: 7.63, 8.44, 10.20 G\n",
        "\"\"\"\n",
        "## LOAD ANOTHER MODEL | ЗАРЕДИ ДРУГ МОДЕЛ\n",
        "\"\"\"\n",
        "ms =[\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.1\"] #\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q6_K.gguf\" #TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "n = 0\n",
        "m = ms[n]\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=m,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "device = \"cuda\"\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8qRWFP3rllY-",
        "outputId": "9f91281f-474d-4060-cb2d-e3c687ad440a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1c7b6a1e6047>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Първо пораждане"
      ],
      "metadata": {
        "id": "d__abiUw-Axk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdJPzJHVV-RP",
        "outputId": "090acee4-81fd-4b40-fbe7-3d698bb3370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 26 06:23:31 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Wrap for long lines #1.3.2024\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "0o4kmQERfZwu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка на лога за история на запитванията"
      ],
      "metadata": {
        "id": "czoVaeOj-8vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Помощен код за сърхраняване на историята във файл\n",
        "# Utility code for saving to a log file\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def get_time_string():\n",
        "  return datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "\n",
        "#fname = \"llm_log.txt\" # in the current directory; first version\n",
        "fname = \"llm_log_\" + get_time_string() + \".txt\" #unique file\n",
        "\n",
        "f = open(fname, \"at\", encoding=\"utf-8\") # if the file exists, it will be appended at (instead of wt)\n",
        "\n",
        "#Then use f.write ...\n",
        "prompt_arr = []\n",
        "answer_arr = []\n",
        "prompt_and_answer = []\n",
        "max_new_tokens = 250"
      ],
      "metadata": {
        "id": "80BSoWMUlgTf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d5700682-588d-4fdb-9282-429f96e63cf4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First generation, upload the model to the GPU, streamer (1.3.2024)\n",
        "\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\") #mistralai/Mistral-7B-v0.1\")\n",
        "# Load the tokenizer\n",
        "from transformers import TextStreamer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\")\n",
        "device = \"cuda\"\n",
        "#prompt = \"Как се правят принджиничени шляпунцели?\"\n",
        "#prompt = \"Откъде мога да си купя шляпунцели със зеленчуци и шоколад? Обичам да ги мажа с приндиджлячки, но оборвляквам шакалакщряк? Нали? Обясни ми\"\n",
        "#prompt = \"Резюмирай в 30 думи: Изброй седемте най-нископлатени тежкоатлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "#Резюмирай в 30 думи:\n",
        "prompt = \"Изброй петте най-високоплатени математици, програмисти, информатици и шофьори от България. Колко от тях са жени? Кои смятат най-бързо?\"\n",
        "\n",
        "#prompt = \"Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "prompt = \"Изброй непопулярни, оранжеви, щръклести и математически гении, и неуспешни български музиканти\\\n",
        "в произволен стил: класическа, поп, народна, рок.\"\n",
        "#prompts = [\"Каква е сумата на числата от 1 до 100?\", \"Как се прави таратор?\", \"Кога и от кого е основан софийският университет?\", \"На какви изпити трябва да се явя за да кандидатствам журналистика в Софийския университет?\", \"От чуждите езици, може ли вместо изброените да кандидатствам с италиански?\", \"Какво мога да работя след като завърша журналистика?\", \"Кои са петимата най-известни български спортисти?\", \"Напиши есе за приложенията на изкуствения интелект.\"]\n",
        "#ps = []\n",
        "#for p in prompt:\n",
        "# ps.append(\"[s][INST]\"+p+\"[/INST]\")\n",
        "#prompt = \"[s][INST]\"+prompt+\"[/INST]\" # better Question Answering\n",
        "#model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "#model_inputs = tokenizer([ps], return_tensors=\"pt\").to(device)\n",
        "prompt = \"Каква е сумата на числата от 1 до 10?\" #?\"\n",
        "prompt = \"[s][INST]\"+prompt+\"[/INST]\" # better Question Answering\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "model.to(device) #if not moved in initialization\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "#generated_ids = model.generate(**model_inputs, streamer=streamer, max_new_tokens=1000,do_sample=True, temperature=2.0, top_k=20, repetition_penalty=1.15, top_p=0.9)\n",
        "generated_ids = model.generate(**model_inputs, streamer=streamer, max_new_tokens=50,do_sample=True, temperature=0.5, top_k=20, repetition_penalty=1.2, top_p=0.5)\n",
        "f.write(\"\\n\"+get_time_string()+\"\\n???\")\n",
        "f.write(prompt)\n",
        "f.write(tokenizer.batch_decode(generated_ids)[0])\n",
        "f.flush()\n",
        "tokenizer.batch_decode(generated_ids)[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "xaelArdPfcam",
        "outputId": "1a2d118f-dfa5-4b1a-8763-950bc2558f7f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [s][INST]Каква е сумата на числата от 1 до 10?[/INST]Сумата от първите n естествени числа може да се изчисли по формулата:\n",
            "\n",
            "Сума = (n * (n + 1)) / 2\n",
            "\n",
            "В този случай искаме да намерим сбора на чис\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> [s][INST]Каква е сумата на числата от 1 до 10?[/INST]Сумата от първите n естествени числа може да се изчисли по формулата:\\n\\nСума = (n * (n + 1)) / 2\\n\\nВ този случай искаме да намерим сбора на чис'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tokenizer.batch_decode(generated_ids)[0])\n",
        "# tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "8qbIJeL4av_n",
        "outputId": "57dce7e1-e6d6-4279-82e3-0f9df5d34a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> [s][INST]Изброй непопулярни, но талантливи български музиканти в произволен стил: класическа, поп, народна, рок.[/INST]Изданието за класации е свързано основно със слушане, разбито с помощта и предоставен линк, и гласуране чрез платформени услуги или уебсайтови опций за избор (на избори), които ще извървят стъпка за филтринг за най–добрите артистични резултати.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DiaaIJDBajBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9tihspfEnqCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "0qTPd7sClOg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Read a list of prompts. One per line\n",
        "openfile ...\n",
        "300, ...\n",
        "'''\n"
      ],
      "metadata": {
        "id": "IhSz1Y4zkmF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Language utils ... leverage with nltk, spacy etc.\n",
        "def add_noise(txt):\n",
        "  pass #insert words..."
      ],
      "metadata": {
        "id": "pst4UCdo_v_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sacred Computer's AGI strategy from 2003:\n",
        "# https://www.oocities.org/todprog/ese/proekt.htm\n",
        "# https://artificial-mind.blogspot.com/2020/07/interdisciplinary-research-institute.html\n",
        "# Try resuming\n",
        "strategy = 'Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектиране, мултимедия, текстообработка, преводачи, игри и др. приложни програми.          Целта на Института ще бъде програмно създаване на ММ, притежаваща универсални възможности за обмен на информация с други изчислителни машини, в  частност роботизирани модули. Роботите, създавани от робототехническия отдел, ще бъдат, освен начин за използване на ИР за физически дейности, още средство  за привличане на вниманието на обществеността и за реклама на Института.          След като бъде осъществена Мислеща машина, тя ще може да се използва във всякакви творчески сфери на човешката дейност и в работата на самия Институт.          Предполагам, че след Откритието и създаването на ММ, работеща на стандартни компютри, Институтът ще се \"опаричи\" и ще получи възможност да  обособи проектантски отдел за разработване на нови цялостни изчислителни  системи, пригодени специално за работата на Машината.          Като завършек бих цитирал няколко факта и имена на млади български \"кандидат-творци на разум\".          Преди няколко месеца Бистра Дилкина, завършваща тази година университета \"Саймън Фрейзър\", спечели мащабно  състезание по програмиране, от областта на ИР, в САЩ и с блестящия си ум привлече вниманието на научните среди.          Ахмед Мерчев, 19-годишен, е основател и ръководител на проекта за човекоподобен робот с умствени и физически възможности сходни с човешките - \"Кибертрон\". По-малко от година след обявяването на проекта, в \"Кибертрон\" постигнаха действителни резултати по робототехническото осъществяване на тялото и получиха признание от БАН.          Авторът на това есе, почти 19-годишен, е основател на дружество \"Разум\", което има за  цел \"разнищването\" на разума. Понастоящем то свързва двама изследователи, чиято стратегия е да разберат действието на мисълта чрез многостранно опознаване и овладяване на  науките и изкуствата. За Илиян Георгиев, студент в САЩ, добре  говори кореспонденцията му с Марвин Мински - един от \"бащите\" на науката за Изкуствения разум, от когото новите идеи не спират да бликат до днес.          Новите идеи не спират да извират от младите български  учени, за които съм убеден, че ако бъдат поставени в благоприятни условия за  работа, ще успеят да направят от поточето река, достатъчно пълноводна, така  че Мислеща машина да \"заплава\" по нея от българско \"пристанище\".'\n",
        "print(strategy)\n",
        "\n",
        "#Too long\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-svs39DjlGC",
        "outputId": "a71f7126-4e7a-47d1-8623-8fd289e8c3e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектиране, мултимедия, текстообработка, преводачи, игри и др. приложни програми.          Целта на Института ще бъде програмно създаване на ММ, притежаваща универсални възможности за обмен на информация с други изчислителни машини, в  частност роботизирани модули. Роботите, създавани от робототехническия отдел, ще бъдат, освен начин за използване на ИР за физически дейности, още средство  за привличане на вниманието на обществеността и за реклама на Института.          След като бъде осъществена Мислеща машина, тя ще може да се използва във всякакви творчески сфери на човешката дейност и в работата на самия Институт.          Предполагам, че след Откритието и създаването на ММ, работеща на стандартни компютри, Институтът ще се \"опаричи\" и ще получи възможност да  обособи проектантски отдел за разработване на нови цялостни изчислителни  системи, пригодени специално за работата на Машината.          Като завършек бих цитирал няколко факта и имена на млади български \"кандидат-творци на разум\".          Преди няколко месеца Бистра Дилкина, завършваща тази година университета \"Саймън Фрейзър\", спечели мащабно  състезание по програмиране, от областта на ИР, в САЩ и с блестящия си ум привлече вниманието на научните среди.          Ахмед Мерчев, 19-годишен, е основател и ръководител на проекта за човекоподобен робот с умствени и физически възможности сходни с човешките - \"Кибертрон\". По-малко от година след обявяването на проекта, в \"Кибертрон\" постигнаха действителни резултати по робототехническото осъществяване на тялото и получиха признание от БАН.          Авторът на това есе, почти 19-годишен, е основател на дружество \"Разум\", което има за  цел \"разнищването\" на разума. Понастоящем то свързва двама изследователи, чиято стратегия е да разберат действието на мисълта чрез многостранно опознаване и овладяване на  науките и изкуствата. За Илиян Георгиев, студент в САЩ, добре  говори кореспонденцията му с Марвин Мински - един от \"бащите\" на науката за Изкуствения разум, от когото новите идеи не спират да бликат до днес.          Новите идеи не спират да извират от младите български  учени, за които съм убеден, че ако бъдат поставени в благоприятни условия за  работа, ще успеят да направят от поточето река, достатъчно пълноводна, така  че Мислеща машина да \"заплава\" по нея от българско \"пристанище\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(strategy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBa23r2AlX5V",
        "outputId": "becfeeca-2be7-4db7-acf3-8c2d7f2e73ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3483"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Пакетно извикване на много въпроси, заявки и т.н."
      ],
      "metadata": {
        "id": "Fc7fzn10anz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 24.2.2024\n",
        "# Batch calling of prompts\n",
        "# qs ... (text,  max_len, {params} ...) --> create future structures, more info etc.\n",
        "# or (text, options, {params //for the model//} )  some options may overide the model params\n",
        "# or (text, options) ... opt... contains the params //separate text for dynamically changing the options?\n",
        "# params: top_k, temp, ... options: multiple generation (compare), sliding-window generation, scanning top_k, temperature, \"augmented\", scrambled etc.\n",
        "# A finding: it can answer long questions, even 500?+ characters, out of memory (it's close to the edge, 14.7 GB etc.)!!!\n",
        "\n",
        "#option = {'max: 200'}\n",
        "import copy #not used yet\n",
        "from textwrap import TextWrapper\n",
        "\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()\n",
        "w = TextWrapper(width=80)\n",
        "streamer = TextStreamer(tokenizer)\n",
        "\n",
        "ls = [\"Иван има 8 плода. Чочко притежава 3 банана. Пенка държи в джоба си шест ягоди. Кое дете има най-много на брой плодове? Кое има най-малко?\",\n",
        "\"Иван има 8 плода. Слънцето пече силно през пролетта. Чочко притежава 3 банана. Маймуните обичат да ядат банани. Пенка държи в джоба си шест ягоди. В градините растат и са червени. Кое дете има най-много на брой плодове? Кое има най-малко?\",\n",
        "\"Чучолина е майка на Магда. Стоянка е леля на Мимето. Цона е дъщеря на Стоянка.  Магда е приятелка на Стоянка. Какви са Магда и Чучолина?\"]\n",
        "\n",
        "class Options:\n",
        "    \"\"\"\n",
        "    __init__(self, general=None, options=None):\n",
        "      self.max_default = 200\n",
        "      if options!=None: self = copy.deepCopy(options) # future use if more complex structrues etc. are used\n",
        "      if general!=None:\n",
        "         self.max_new_tokens = general.max_new_tokens\n",
        "         self.temperature = general.tempperature\n",
        "         self.top_k = general.top_k\n",
        "         self.repetition_penalty = general.repetition_penalty\n",
        "         print(\"init options...\", self)\n",
        "\n",
        "    #general=None,\n",
        "    \"\"\"\n",
        "    def getDefaultPrompt(hint):\n",
        "        return \"Преведи на английски: Котката изпи камъка и литна под нанагорнището.\"\n",
        "\n",
        "\n",
        "    #enabled -- allows to disable some questions etc. without removing them from the list\n",
        "    #if copying: prompt can be null or whatever, it's overwritten by options\n",
        "    #def __init__(self, options=None, prompt=None, enabled = True, inst=True, max=0, t=0, top_k=0, top_p=0, repetition_penalty=0): # #0 means default\n",
        "    def __init__(self, prompt=None, enabled = True, inst=True, max=0, t=0, top_k=0, top_p=0, repetition_penalty=0): # #0 means default\n",
        "       self.max_default = 600 #1000\n",
        "       self.t_default = 0.9\n",
        "       self.top_k_default = 40\n",
        "       self.top_p_default = 0.9 #0.93\n",
        "       #self.max_new_tokens_defau\n",
        "       self.repetition_penalty_default = 1.15 #1.2\n",
        "       self.inst = inst #[INST][/INST]\n",
        "       self.answer = None\n",
        "       self.ids = None\n",
        "       self.enabled = True\n",
        "\n",
        "       #if options!=None: self = copy.deepcopy(options); print(self) # but still may override the parames future use if more complex structrues etc. are used\n",
        "       if prompt==None: self.prompt = self.getDefaultPrompt()\n",
        "       else: self.prompt = prompt\n",
        "       #elif self.prompt==None: self.prompt = prompt #when copy ctr\n",
        "       print(\"__init__.prompt: \"+self.prompt)\n",
        "\n",
        "       if (max==0): self.max_new_tokens = self.max_default  # could be dictionaries etc.\n",
        "       else: self.max_new_tokens = max\n",
        "       if (t==0): self.temperature = self.t_default\n",
        "       else: self.temperature = t\n",
        "       if (top_k==0): self.top_k = self.top_k_default\n",
        "       else: self.top_k = top_k\n",
        "       if (top_p==0): self.top_p = self.top_p_default\n",
        "       else: self.top_p = top_p\n",
        "       if (repetition_penalty==0): self.repetition_penalty = self.repetition_penalty_default # -1\n",
        "       else:  self.repetition_penalty = repetition_penalty\n",
        "       print(\"init Options...\", self)\n",
        "\n",
        "#call if not initialized\n",
        "def init_model(m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"):\n",
        "  #m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "  device = \"cuda\"\n",
        "  model.to(device)\n",
        "\n",
        "b_new = False\n",
        "b_inst = True #False # True\n",
        "b_store_ids = False #store token ids - for debugging etc. if true it complicates the simple printing of all params (too much numbers)\n",
        "\n",
        "def batch_questions(qs):\n",
        "  answers = []\n",
        "  results = []\n",
        "\n",
        "  for n,o in enumerate(qs):\n",
        "    #prompt, o = q #could contain control commands such as new [INST] session etc. ... title?... in the output\n",
        "    prompt = o.prompt\n",
        "    if prompt == \"m\": b_new_chat = True\n",
        "    elif prompt == \"t\": title = prompt; continue\n",
        "    if b_inst: prompt = \"[s][INST]\"+prompt+\"[/INST]\"\n",
        "    s = w.wrap(f\"PROMPT {n}:{prompt}\") #,n, prompt)\n",
        "    f.write(\"\\n\"+get_time_string()+\"\\n???\")\n",
        "    f.write(prompt)\n",
        "    f.flush()\n",
        "    print(s)\n",
        "\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    print(\"o.max_new_tokens = \", o.max_new_tokens)\n",
        "    #pp.pprint(o)\n",
        "    #pp.pprint(o.__dict__) #__repr__())\n",
        "    print(o.__dict__) #__repr__())\n",
        "\n",
        "    f.write(\"\\n\"+get_time_string()+\"\\n???\")\n",
        "    #streamer = TextStreamer(tokenizer) # call once initially\n",
        "    generated_ids = model.generate(**model_inputs, streamer=streamer, max_new_tokens=o.max_new_tokens, do_sample=True, top_k = o.top_k, top_p = o.top_p, temperature = float(o.temperature), repetition_penalty = o.repetition_penalty )\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    #print(answer)\n",
        "    print(w.wrap(answer))\n",
        "    #record also the tokens!\n",
        "    results.append( (o, generated_ids, answer)) #options contain all...\n",
        "    answers.append( (o, answer))\n",
        "    o.answer = answer #copy.copy(answer)    )\n",
        "    f.write(\"\\n\"+get_time_string()+\"\\n???: \"+prompt+\"\\n\\n===: \"+answer+\"\\n------------\\n\")\n",
        "    f.flush()\n",
        "    if b_store_ids: o.ids = generated_ids\n",
        "  return (results, answers)\n",
        "\n",
        "qs = []\n",
        "opt = Options()\n",
        "opt.prompt = \"Тръгнал лос с дълъг кос, през града към нос Калиакра!\"\n",
        "qs.append(opt)\n",
        "\n",
        "g = \"The Green ideas furiously sleep!\"\n",
        "#opt_2 = Options(\"Зелените идеи яростно спят!\", t=3.0, top_k=5, top_p=0.4)\n",
        "opt_2 = Options(\"The Green ideas furiously sleep! Who was the author and what does it mean?\", max=100, t=3.0, top_k=40, top_p=0.9)\n",
        "qs.append(opt_2)\n",
        "\n",
        "qs.clear()\n",
        "#for s in ls:\n",
        "#  qs.append(Options(s))\n",
        "\n",
        "qs.append(opt_2)\n",
        "qs.append(Options(g, t=0.0, max=50))\n",
        "#qs.append(Options(g, t=0.0))\n",
        "#qs.append(Options(g, t=2.0))\n",
        "\"\"\"\n",
        "qs.append(Options(g, t=3.0))\n",
        "qs.append(Options(g, top_k=30, t=0.9, top_p=0.8))\n",
        "qs.append(Options(g, top_k=20, t=0.9, top_p=0.8))\n",
        "qs.append(Options(g, top_k=10, t=0.9, top_p=0.8))\n",
        "qs.append(Options(g, top_k=5, t=0.9, top_p=0.8))\n",
        "\"\"\"\n",
        "qs.clear()\n",
        "#opt_2.prompt = strategy\n",
        "\n",
        "b_q_file = True # False #True if processing a text file\n",
        "s_file = \"q5.txt\"\n",
        "####################\n",
        "def test_from_file(path=\"q1.txt\",n=15):\n",
        "  qf = path #\"q1.txt\"\n",
        "  f1 = open(qf,\"rt\", encoding='utf-8', errors='ignore' )\n",
        "  lines = f1.readlines()\n",
        "  f1.close\n",
        "  print(lines)\n",
        "  tops = [40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,40,30,20,10,5,3,2,1,10,20,30,40,30,20,10,5,10,15,18,20,25,30,30,40,40,40,20,1,4,5,9,10,6,15,20,15,30,23,40,12] #top_k\n",
        "  #temps = [0.8, 0.8, 0.8,0.8, 0.8, 0.8, 0.8,0.8, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 0.0,0.5,1.0,2.0,3.0,4.0,5.0,5.0,4.0,3.0,2.0,1.0,0.0,3.0,5.0,15.0,0.9,2.0,0.3,1.0,0.9,0.9,0.9,2.0, 3.0, 5.0, 5.0,5.0,5.0]\n",
        "  #top_ps = [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
        "  #top_ps = [0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2]\n",
        "  top_ps = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9]\n",
        "  temps = [5.0, 10.0, 15.0, 30.0, 10.0, 10.0, 10.0, 10.0, 0.1, 0.1,0.1, 0.1, 0.1, 0.1,0.1, 0.1, 0.5,1.0,2.0,3.0,4.0,5.0,5.0,4.0,3.0,2.0,1.0,0.0,3.0,5.0,15.0,0.9,2.0,0.3,1.0,0.9,0.9,0.9,2.0, 3.0, 5.0, 5.0,5.0,5.0]\n",
        "  #ts.reverse()\n",
        "  qs = []\n",
        "  #qs.clear()\n",
        "  top_k = 40 #\n",
        "  temperature = 0.7 #0.9\n",
        "  top_p = 0.8\n",
        "  b_use_fixed_value = True #don't use the listed values\n",
        "  #for s in zip(lines,ts,tops):\n",
        "  for ln, topk, ts, tp in zip(lines,tops,temps,top_ps):\n",
        "    #g = s1 # + str(i) + \" думи. \" # + strategy[0:1500] #[0:1200] OK 25.2.2024 #[0:500]\n",
        "    #print(\"G==\",g)\n",
        "    #qs.append(Options(ln, t = t, top_k=tops)) #40)) #, top_k=40, t=0.9, top_p=0.8))\n",
        "    if not b_use_fixed_value: qs.append(Options(ln, t = ts, top_k=topk, top_p=tp)) #40)) #, top_k=40, t=0.9, top_p=0.8))\n",
        "    else: qs.append(Options(ln, t = temperature, top_k=top_k, top_p=top_p))  #FIXED VALUES\n",
        "    n=n-1\n",
        "    if n==0: break\n",
        "\n",
        "  results, answers = batch_questions(qs)\n",
        "  for o,a in answers:\n",
        "    print(w.wrap(o.prompt))\n",
        "    print(w.wrap(o.answer)) #add answer in option for filling also timing etc.\n",
        "\n",
        "if b_q_file: test_from_file(s_file) #q4.txt 1.3.2024 #q1.txt 26.2.2024\n",
        "\n",
        "#exit()\n",
        "\n",
        "s1 = \"Резюмирай текста в рамките на \"\n",
        "#s2 = \"100 думи\"\n",
        "#№s2 = \"100 думи\"\n",
        "#v = [10, 20, 40, 80, 160, 320, 500]\n",
        "v = [11, 22, 33, 44, 9999] #, 320, 500]\n",
        "v = [10,30,50,100,200] #lengths, text template\n",
        "tops = [40,30,20,10,5,3,2,1,10,20,30,40,30,20,10,5,10,15,18,20,25,30,30,40,40,40,20,1,4,5,9,10,6,15,20,15,30,23,40,12] #top_k\n",
        "#ts = [1.0, 0.9, 0.7, 0.5, 0.3, 0.1] #temperatures\n",
        "temps = [0.1, 0.9, 2.0, 4.0, 8.0, 20.0] #temperatures\n",
        "q = [\"Кой\", \"Кога\", \"Как\", \"Защо\", \"Какво\", \"Колко\", \"Къде\", \"Нима\"] #questions\n",
        "n = 1 #word index\n",
        "#q = [\"Кой\", \"Кога\", \"Как\", \"Защо\", \"Какво\", \"Колко\", \"Къде\", \"Нима\"]\n",
        "print(\"str(v)\",str(v))\n",
        "\n",
        "#\"\"\"\n",
        "#Резюмирай текста в рамките на 33 думи \" + text ....\n",
        "\n",
        "for i,topk,ts in zip(v,tops,temps):\n",
        "  g = s1 + str(i) + \" думи. \" # + strategy[0:1500] #[0:1200] OK 25.2.2024 #[0:500]\n",
        "  print(\"G==\",g)\n",
        "  qs.append(Options(g, top_k=topk, t=ts)) #5.0) ) #, top_k=40, t=0.9, top_p=0.8))\n",
        "#\"\"\"\n",
        "results, answers = batch_questions(qs)\n",
        "for o,a in answers:\n",
        "  print(w.wrap(o.prompt))\n",
        "  print(w.wrap(o.answer)) #add answer in option for filling also timing etc. ..\n",
        "\n",
        "# Hallucinations with almost empty prompt 26.2.2024\n",
        "qs.clear()\n",
        "for s,t,topk in zip(q,temps,tops):\n",
        "  #g = s1 # + str(i) + \" думи. \" # + strategy[0:1500] #[0:1200] OK 25.2.2024 #[0:500]\n",
        "  #print(\"G==\",g)\n",
        "  qs.append(Options(s, t = t, top_k=topk)) #, top_k=40, t=0.9, top_p=0.8))\n",
        "\n",
        "results, answers = batch_questions(qs)\n",
        "for o,a in answers:\n",
        "  print(w.wrap(o.prompt))\n",
        "  print(w.wrap(o.answer)) #add answer in option for filling also timing etc. ..\n",
        "\n",
        "#OOM Error  - can't cope even with 500? - sometimes it does with 1500 after fresh\n",
        "\"\"\"\n",
        "TEST AGAIN:\n",
        "\n",
        "s1 = \"Резюмирай текста в рамките на \"\n",
        "#s2 = \"100 думи\"\n",
        "#№s2 = \"100 думи\"\n",
        "v = [10, 20, 40, 80, 160, 320, 500]\n",
        "print(\"str(v)\",str(v))\n",
        "for i in v:\n",
        "  g = s1 + str(i) + \" думи.  \" # EMPTY - lost due to blue screen afterwards + strategy[0:1200] #[0:500]\n",
        "  print(\"G==\",g)\n",
        "  qs.append(Options(g)) #, top_k=40, t=0.9, top_p=0.8))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#qs.append(Options(g, top_k=40, t=2.0, top_p=0.6))\n",
        "#qs.append(Options(g, top_k=40, t=0.01, top_p=0.4))\n",
        "\n",
        "#results, answers = batch_questions(qs)\n",
        "\n",
        "#print(answers)\n",
        "#print(results)\n",
        "\n",
        "for o,a in answers:\n",
        "  #print(o.__repr__) #o.repr(tokens=False))\n",
        "  #print(o.prompt)\n",
        "  pp.pprint(o.__dict__)\n",
        "  pp.pprint(a)\n",
        "\n",
        "for o,a in answers:\n",
        "  print(w.wrap(o.prompt))\n",
        "  print(w.wrap(o.answer)) #add answer in option for filling also timing etc. ...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xnZgpYWx__j6",
        "outputId": "2f56d805-654d-4cfa-a798-f14f6ff704ba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__.prompt: Преведи на английски: Котката изпи камъка и литна под нанагорнището.\n",
            "init Options... <__main__.Options object at 0x7ca303871330>\n",
            "__init__.prompt: The Green ideas furiously sleep! Who was the author and what does it mean?\n",
            "init Options... <__main__.Options object at 0x7ca303873af0>\n",
            "__init__.prompt: The Green ideas furiously sleep!\n",
            "init Options... <__main__.Options object at 0x7ca3038738b0>\n",
            "['Каква е сумата на числата от 100 до 1?\\n', 'Как се прави таратор с грозде?\\n', 'Кога и от кого е основан пловдивския университет в София?\\n', 'На какви изпити трябва да се явя за да кандидатствам стакмистика в Суфийския университет, Азия?\\n', 'От домашните езици, може ли вместо изброените да кандидатствам с инджигитайски?\\n', 'Какво мога да работя след като завърша сталакмистика? Ще има ли реализация в ерата на естествения интелект?\\n', 'Кои са петимата най-неизвестни български спортисти?\\n', 'Напиши есе за приложенията на естествения интелект.\\n', 'Не написвай есе за приложенията за изкуствения интелект.\\n', 'Разкажи ми за себе си.\\n', 'Как се казваш?\\n', 'На колко си години?\\n', 'Кога си роден?\\n', 'От къде си?\\n', 'Имаш ли братя и сестри?\\n', 'Кое е любимото ти телевизионно предаване? По коя програма?\\n', 'Гледаш ли детските по БНТ?\\n', 'Харесваш ли как Лара? Знаеш ли кое предаване води?\\n', 'Коя е любимата ти видео игра?\\n', 'Кой ти е любимият актьор? Актриса? Режисьор? Певец, певица, група?\\n', 'Обичаш ли 3D графиката?\\n', 'Коя ти е любимата компютърна игра от 1990-те години?\\n', 'Можеш ли да програмираш Правец-8М? Покажи ми твой код!\\n']\n",
            "__init__.prompt: Каква е сумата на числата от 100 до 1?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca303873820>\n",
            "__init__.prompt: Как се прави таратор с грозде?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca303873880>\n",
            "__init__.prompt: Кога и от кого е основан пловдивския университет в София?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca303873b50>\n",
            "__init__.prompt: На какви изпити трябва да се явя за да кандидатствам стакмистика в Суфийския университет, Азия?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca3038714e0>\n",
            "__init__.prompt: От домашните езици, може ли вместо изброените да кандидатствам с инджигитайски?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca444288c40>\n",
            "__init__.prompt: Какво мога да работя след като завърша сталакмистика? Ще има ли реализация в ерата на естествения интелект?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca44428aa40>\n",
            "__init__.prompt: Кои са петимата най-неизвестни български спортисти?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca44428b790>\n",
            "__init__.prompt: Напиши есе за приложенията на естествения интелект.\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca44428acb0>\n",
            "__init__.prompt: Не написвай есе за приложенията за изкуствения интелект.\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca30386f400>\n",
            "__init__.prompt: Разкажи ми за себе си.\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca30386f670>\n",
            "__init__.prompt: Как се казваш?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca30386f8e0>\n",
            "__init__.prompt: На колко си години?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca30386fb50>\n",
            "__init__.prompt: Кога си роден?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca30386fdc0>\n",
            "__init__.prompt: От къде си?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca30386efb0>\n",
            "__init__.prompt: Имаш ли братя и сестри?\n",
            "\n",
            "init Options... <__main__.Options object at 0x7ca30386ed40>\n",
            "['PROMPT 0:[s][INST]Каква е сумата на числата от 100 до 1? [/INST]']\n",
            "o.max_new_tokens =  600\n",
            "{'max_default': 600, 't_default': 0.9, 'top_k_default': 40, 'top_p_default': 0.9, 'repetition_penalty_default': 1.15, 'inst': True, 'answer': None, 'ids': None, 'enabled': True, 'prompt': 'Каква е сумата на числата от 100 до 1?\\n', 'max_new_tokens': 600, 'temperature': 0.7, 'top_k': 40, 'top_p': 0.8, 'repetition_penalty': 1.15}\n",
            "<s> [s][INST]Каква е сумата на числата от 100 до 1?\n",
            "[/INST]Сумата на аритметична серия се дава по формулата:\n",
            "\n",
            "Сума = (n * (a + l)) / 2\n",
            "\n",
            "където n е броят на членовете, a е първият член и l е последният член.\n",
            "\n",
            "В този случай n = 100, a = 1 и l = 100. Вмъквайки тези стойности във формулата, получаваме:\n",
            "\n",
            "Сума = (100 * (1 + 100)) / 2\n",
            "   = (100 * 101) / 2\n",
            "   = 5050\n",
            "\n",
            "И така, сумата от числата от 1 до 100 е 5050.</s>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s> [s][INST]Каква е сумата на числата от 100 до 1? [/INST]Сумата на аритметична', 'серия се дава по формулата:  Сума = (n * (a + l)) / 2  където n е броят на', 'членовете, a е първият член и l е последният член.  В този случай n = 100, a = 1', 'и l = 100. Вмъквайки тези стойности във формулата, получаваме:  Сума = (100 * (1', '+ 100)) / 2     = (100 * 101) / 2     = 5050  И така, сумата от числата от 1 до', '100 е 5050.</s>']\n",
            "['PROMPT 1:[s][INST]Как се прави таратор с грозде? [/INST]']\n",
            "o.max_new_tokens =  600\n",
            "{'max_default': 600, 't_default': 0.9, 'top_k_default': 40, 'top_p_default': 0.9, 'repetition_penalty_default': 1.15, 'inst': True, 'answer': None, 'ids': None, 'enabled': True, 'prompt': 'Как се прави таратор с грозде?\\n', 'max_new_tokens': 600, 'temperature': 0.7, 'top_k': 40, 'top_p': 0.8, 'repetition_penalty': 1.15}\n",
            "<s> [s][INST]Как се прави таратор с грозде?\n",
            "[/INST]Тараторът с грозде е освежаваща лятна супа, която съчетава сладостта на зряло грозде със свежестта на краставици и киселинността на оцет. Ето една проста рецепта за приготвяне на таратор от грозде:\n",
            "\n",
            "съставки:\n",
            "- 1 голяма краставица, обелена и нарязана на кубчета\n",
            "- 2 чаши червено или бяло грозде, без "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-56a8097f447a>\u001b[0m in \u001b[0;36m<cell line: 186>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#add answer in option for filling also timing etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mb_q_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#q4.txt 1.3.2024 #q1.txt 26.2.2024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;31m#exit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-56a8097f447a>\u001b[0m in \u001b[0;36mtest_from_file\u001b[0;34m(path, n)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m   \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-56a8097f447a>\u001b[0m in \u001b[0;36mbatch_questions\u001b[0;34m(qs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mget_time_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n???\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;31m#streamer = TextStreamer(tokenizer) # call once initially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepetition_penalty\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m#print(answer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1593\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m             \u001b[0;31m# sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             \u001b[0mnext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m             \u001b[0;31m# finished sentences should have their next token be a padding token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CINwsgY-K6KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi\n",
        "#!/usr/local/cuda/bin/nvcc --version\n",
        "#NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "2-uEaQh7Tv0B",
        "outputId": "9cc9769c-19aa-46df-cffb-5f64dd993785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-38db1180233d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# !nvidia-smi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/usr/local/cuda/bin/nvcc --version'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ОСНОВЕН ЧАТ ЦИКЪЛ"
      ],
      "metadata": {
        "id": "ckhfVLfH_m3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20.2.2024\n",
        "# MISTRAL - up to 4096 ... context window length, but it seems it won't fit in Tesla T4 - about 2000 or something\n",
        "from textwrap import TextWrapper  # \"word-wrap\" long lines\n",
        "import pprint\n",
        "from transformers import TextStreamer\n",
        "#import math\n",
        "#import copy\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "streamer = TextStreamer(tokenizer)\n",
        "device = \"cuda\"\n",
        "model.to(device)\n",
        "b_new_chat = True #insert \"[s][INST]...\"\n",
        "max_new_tokens = 500\n",
        "w = TextWrapper(width=80) #break long lines, default: 70 chars  w.wrap(string)\n",
        "b_autoregression = False #True\n",
        "options = Options(); #current state\n",
        "b_print_options = True\n",
        "history = []\n",
        "prev_prompt = \"?\"\n",
        "\n",
        "def valid_range(m):\n",
        "  if m > 4096: return False # m = 2400\n",
        "  if m < 1: return False\n",
        "  return True\n",
        "\n",
        "def cycle():\n",
        "    global max_new_tokens\n",
        "    global b_new_chat\n",
        "    global b_autoregression\n",
        "    global prev_prompt\n",
        "    prompt = input(\"Enter prompt... \") #\"Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "    if prompt == \"q\": return False\n",
        "    if prompt == \"n\": b_new_chat = True; return True\n",
        "    if prompt == \"a\": b_autoregression = True; return True #GPT2, GPT3-like: not very smart\n",
        "    if prompt == \"i\": b_autoregression = False; return True #Instruct\n",
        "    if prompt == \"top_k\": top_k=input(\"top_k=... \"); options.top_k=top_k; return True #Should validate etc. in the class, set ...\n",
        "    if prompt == \"top_p\": top_p=input(\"top_k=... \"); options.top_p=top_p; return True #Should validate etc. in the class, set ...\n",
        "    if prompt == \"t\": t=input(\"temperature=... \"); options.temperature=max(0.001, float(t));  options.temperature; return True #Should validate etc. in the class, set ...\n",
        "    if prompt == 's': prompt = strategy[0:len(strategy)//5]\n",
        "    if prompt == 'r': prompt = prev_prompt\n",
        "    else: prev_prompt = prompt\n",
        "    if prompt == \"m\":\n",
        "      mx = int(input(\"max_tokens=?...\"))\n",
        "      if valid_range(mx): max_new_tokens = mx; options.max_new_tokens = mx; return True\n",
        "      else: print(\"Invalid max_new_tokens\")\n",
        "    if not b_autoregression:\n",
        "      if b_new_chat: prompt = \"[s][INST]\"+prompt+\"[/INST]\"; b_new_chat = False\n",
        "      else: prompt=\"[INST]\"+prompt+\"[/INST]\"\n",
        "    print(prompt)\n",
        "    f.write(\"\\n\"+get_time_string()+\"\\n???: \"+prompt)\n",
        "    f.flush()\n",
        "    for i in w.wrap(prompt): print(i) #input window is too small\n",
        "    if b_print_options==True: pp.pprint(options)\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    #generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens, do_sample=True, top_k = options.top_k, top_p = options.top_p, temperature=options.temperature)\n",
        "    generated_ids = model.generate(**model_inputs, streamer=streamer, max_new_tokens=options.max_new_tokens, do_sample=True, top_k = int(options.top_k), top_p = float(options.top_p), temperature=float(options.temperature))\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    prompt_arr.append(prompt)\n",
        "    answer_arr.append(answer)\n",
        "    prompt_and_answer.append((prompt, answer))\n",
        "    #print(w.wrap(answer))\n",
        "    for i in w.wrap(answer): print(i) #print wrapped lines\n",
        "    f.write(\"\\n\"+get_time_string()+\"\\n???: \"+prompt+\"\\n\\n===: \"+answer+\"\\n------------\\n\")\n",
        "    f.flush()\n",
        "    return True\n",
        "\n",
        "def interact():\n",
        "   i = True\n",
        "   while i == True:\n",
        "      i = cycle()\n",
        "def culturno():\n",
        "    messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин. \"}\n",
        "    ]\n",
        "    #encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, do_sample=True)\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    print(answer)\n",
        "#culturno() #proba\n",
        "interact()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ahInASr5hb-W",
        "outputId": "8ea3cb2c-623a-4d31-ba07-438c16419031"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__init__.prompt: Преведи на английски: Котката изпи камъка и литна под нанагорнището.\n",
            "init Options... <__main__.Options object at 0x7ca303872620>\n",
            "Enter prompt... Named Entity recognition. Extract the persons: \"DeepMind Technologies Limited – Overview (free company information from Companies House)\". Companies House. Retrieved 13 March 2016.  \"King's Cross - S2 Building - SES Engineering Services\". www.ses-ltd.co.uk. Retrieved 14 July 2022.  Efrati, Amir (11 October 2023). \"DeepMind Cut 20% of Its Expenses Before Merging with Google\". The Information. Archived from the original on 12 October 2023.  \"DEEPMIND TECHNOLOGIES LIMITED overview - Find and update company information - GOV.UK\". Companies House. Retrieved 22 July 2023.  Bray, Chad (27 January 2014). \"Google Acquires British Artificial Intelligence Developer\". DealBook. Retrieved 4 November 2019.  \"About Us | DeepMind\". DeepMind.  \"A return to Paris | DeepMind\". DeepMind.  \"The Last AI Breakthrough DeepMind Made Before Google Bought It\". The Physics arXiv Blog. 29 January 2014. Retrieved 12 October 2014.  Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). \"Neural Turing Machines\". arXiv:1410.5401 [cs.NE].  Best of 2014: Google's Secretive DeepMind Startup Unveils a \"Neural Turing Machine\" Archived 4 December 2015 at the Wayback Machine, MIT Technology Review  Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). \"Hybrid computing using a neural network with dynamic external memory\". Nature. 538 (7626): 471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687. PMID 27732574. S2CID 205251479.  Kohs, Greg (29 September 2017), AlphaGo, Ioannis Antonoglou, Lucas Baker, Nick Bostrom, retrieved 9 January 2018  Silver, David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December 2017). \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[s][INST]Named Entity recognition. Extract the persons: \"DeepMind Technologies Limited – Overview (free company information from Companies House)\". Companies House. Retrieved 13 March 2016.  \"King's Cross - S2 Building - SES Engineering Services\". www.ses-ltd.co.uk. Retrieved 14 July 2022.  Efrati, Amir (11 October 2023). \"DeepMind Cut 20% of Its Expenses Before Merging with Google\". The Information. Archived from the original on 12 October 2023.  \"DEEPMIND TECHNOLOGIES LIMITED overview - Find and update company information - GOV.UK\". Companies House. Retrieved 22 July 2023.  Bray, Chad (27 January 2014). \"Google Acquires British Artificial Intelligence Developer\". DealBook. Retrieved 4 November 2019.  \"About Us | DeepMind\". DeepMind.  \"A return to Paris | DeepMind\". DeepMind.  \"The Last AI Breakthrough DeepMind Made Before Google Bought It\". The Physics arXiv Blog. 29 January 2014. Retrieved 12 October 2014.  Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). \"Neural Turing Machines\". arXiv:1410.5401 [cs.NE].  Best of 2014: Google's Secretive DeepMind Startup Unveils a \"Neural Turing Machine\" Archived 4 December 2015 at the Wayback Machine, MIT Technology Review  Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). \"Hybrid computing using a neural network with dynamic external memory\". Nature. 538 (7626): 471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687. PMID 27732574. S2CID 205251479.  Kohs, Greg (29 September 2017), AlphaGo, Ioannis Antonoglou, Lucas Baker, Nick Bostrom, retrieved 9 January 2018  Silver, David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December 2017). \"[/INST]\n",
            "[s][INST]Named Entity recognition. Extract the persons: \"DeepMind Technologies\n",
            "Limited – Overview (free company information from Companies House)\". Companies\n",
            "House. Retrieved 13 March 2016.  \"King's Cross - S2 Building - SES Engineering\n",
            "Services\". www.ses-ltd.co.uk. Retrieved 14 July 2022.  Efrati, Amir (11 October\n",
            "2023). \"DeepMind Cut 20% of Its Expenses Before Merging with Google\". The\n",
            "Information. Archived from the original on 12 October 2023.  \"DEEPMIND\n",
            "TECHNOLOGIES LIMITED overview - Find and update company information - GOV.UK\".\n",
            "Companies House. Retrieved 22 July 2023.  Bray, Chad (27 January 2014). \"Google\n",
            "Acquires British Artificial Intelligence Developer\". DealBook. Retrieved 4\n",
            "November 2019.  \"About Us | DeepMind\". DeepMind.  \"A return to Paris |\n",
            "DeepMind\". DeepMind.  \"The Last AI Breakthrough DeepMind Made Before Google\n",
            "Bought It\". The Physics arXiv Blog. 29 January 2014. Retrieved 12 October 2014.\n",
            "Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). \"Neural Turing Machines\".\n",
            "arXiv:1410.5401 [cs.NE].  Best of 2014: Google's Secretive DeepMind Startup\n",
            "Unveils a \"Neural Turing Machine\" Archived 4 December 2015 at the Wayback\n",
            "Machine, MIT Technology Review  Graves, Alex; Wayne, Greg; Reynolds, Malcolm;\n",
            "Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio\n",
            "Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). \"Hybrid computing\n",
            "using a neural network with dynamic external memory\". Nature. 538 (7626):\n",
            "471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687.\n",
            "PMID 27732574. S2CID 205251479.  Kohs, Greg (29 September 2017), AlphaGo,\n",
            "Ioannis Antonoglou, Lucas Baker, Nick Bostrom, retrieved 9 January 2018  Silver,\n",
            "David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew;\n",
            "Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore;\n",
            "Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December 2017). \"[/INST]\n",
            "<__main__.Options object at 0x7ca303872620>\n",
            "<s> [s][INST]Named Entity recognition. Extract the persons: \"DeepMind Technologies Limited – Overview (free company information from Companies House)\". Companies House. Retrieved 13 March 2016.  \"King's Cross - S2 Building - SES Engineering Services\". www.ses-ltd.co.uk. Retrieved 14 July 2022.  Efrati, Amir (11 October 2023). \"DeepMind Cut 20% of Its Expenses Before Merging with Google\". The Information. Archived from the original on 12 October 2023.  \"DEEPMIND TECHNOLOGIES LIMITED overview - Find and update company information - GOV.UK\". Companies House. Retrieved 22 July 2023.  Bray, Chad (27 January 2014). \"Google Acquires British Artificial Intelligence Developer\". DealBook. Retrieved 4 November 2019.  \"About Us | DeepMind\". DeepMind.  \"A return to Paris | DeepMind\". DeepMind.  \"The Last AI Breakthrough DeepMind Made Before Google Bought It\". The Physics arXiv Blog. 29 January 2014. Retrieved 12 October 2014.  Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). \"Neural Turing Machines\". arXiv:1410.5401 [cs.NE].  Best of 2014: Google's Secretive DeepMind Startup Unveils a \"Neural Turing Machine\" Archived 4 December 2015 at the Wayback Machine, MIT Technology Review  Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). \"Hybrid computing using a neural network with dynamic external memory\". Nature. 538 (7626): 471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687. PMID 27732574. S2CID 205251479.  Kohs, Greg (29 September 2017), AlphaGo, Ioannis Antonoglou, Lucas Baker, Nick Bostrom, retrieved 9 January 2018  Silver, David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December 2017). \"[/INST]Компания\n",
            "DeepMind Technologies Limited\n",
            "Екип\n",
            "Алекс Графенстет, Дмитрий Головински, Иво Даниелка, Мат Коул, Матиас Хофман, Ричард Сътклиф, Юън Кинсел, Джонатан Бат, Джон Джаспър, Демис Хасабис\n",
            "Основана\n",
            "2010 г\n",
            "Сливане с\n",
            "Google Inc\n",
            "Местоположение\n",
            "Лондон, Обединено кралство\n",
            "Компанията DeepMind Technologies Limited е основана през 2010 г. в Лондон, Обединеното кралство. Нейни основатели са Алекс Графенстет, Дмитрий Головински, Иво Даниелка, Мат Коул, Матиас Хофман, Ричард Сътклиф, Юън Кинсел, Джонатан Бат и Демис Хасабис. През 2014 г. компанията беше придобита от Google Inc. DeepMind е известен с работата си в разработването на AI и има принос в различни области, включително машинно обучение и невронни мрежи.</s>\n",
            "<s> [s][INST]Named Entity recognition. Extract the persons: \"DeepMind\n",
            "Technologies Limited – Overview (free company information from Companies\n",
            "House)\". Companies House. Retrieved 13 March 2016.  \"King's Cross - S2 Building\n",
            "- SES Engineering Services\". www.ses-ltd.co.uk. Retrieved 14 July 2022.  Efrati,\n",
            "Amir (11 October 2023). \"DeepMind Cut 20% of Its Expenses Before Merging with\n",
            "Google\". The Information. Archived from the original on 12 October 2023.\n",
            "\"DEEPMIND TECHNOLOGIES LIMITED overview - Find and update company information -\n",
            "GOV.UK\". Companies House. Retrieved 22 July 2023.  Bray, Chad (27 January 2014).\n",
            "\"Google Acquires British Artificial Intelligence Developer\". DealBook. Retrieved\n",
            "4 November 2019.  \"About Us | DeepMind\". DeepMind.  \"A return to Paris |\n",
            "DeepMind\". DeepMind.  \"The Last AI Breakthrough DeepMind Made Before Google\n",
            "Bought It\". The Physics arXiv Blog. 29 January 2014. Retrieved 12 October 2014.\n",
            "Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). \"Neural Turing Machines\".\n",
            "arXiv:1410.5401 [cs.NE].  Best of 2014: Google's Secretive DeepMind Startup\n",
            "Unveils a \"Neural Turing Machine\" Archived 4 December 2015 at the Wayback\n",
            "Machine, MIT Technology Review  Graves, Alex; Wayne, Greg; Reynolds, Malcolm;\n",
            "Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio\n",
            "Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). \"Hybrid computing\n",
            "using a neural network with dynamic external memory\". Nature. 538 (7626):\n",
            "471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687.\n",
            "PMID 27732574. S2CID 205251479.  Kohs, Greg (29 September 2017), AlphaGo,\n",
            "Ioannis Antonoglou, Lucas Baker, Nick Bostrom, retrieved 9 January 2018  Silver,\n",
            "David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew;\n",
            "Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore;\n",
            "Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December 2017).\n",
            "\"[/INST]Компания DeepMind Technologies Limited Екип Алекс Графенстет, Дмитрий\n",
            "Головински, Иво Даниелка, Мат Коул, Матиас Хофман, Ричард Сътклиф, Юън Кинсел,\n",
            "Джонатан Бат, Джон Джаспър, Демис Хасабис Основана 2010 г Сливане с Google Inc\n",
            "Местоположение Лондон, Обединено кралство Компанията DeepMind Technologies\n",
            "Limited е основана през 2010 г. в Лондон, Обединеното кралство. Нейни основатели\n",
            "са Алекс Графенстет, Дмитрий Головински, Иво Даниелка, Мат Коул, Матиас Хофман,\n",
            "Ричард Сътклиф, Юън Кинсел, Джонатан Бат и Демис Хасабис. През 2014 г.\n",
            "компанията беше придобита от Google Inc. DeepMind е известен с работата си в\n",
            "разработването на AI и има принос в различни области, включително машинно\n",
            "обучение и невронни мрежи.</s>\n",
            "Enter prompt... You're Mistral, English. Named Entity recognition. Extract the persons. Answer in English. \"DeepMind Technologies Limited – Overview (free company information from Companies House)\". Companies House. Retrieved 13 March 2016.  \"King's Cross - S2 Building - SES Engineering Services\". www.ses-ltd.co.uk. Retrieved 14 July 2022.  Efrati, Amir (11 October 2023). \"DeepMind Cut 20% of Its Expenses Before Merging with Google\". The Information. Archived from the original on 12 October 2023. \"DEEPMIND TECHNOLOGIES LIMITED overview - Find and update company information - GOV.UK\". Companies House. Retrieved 22 July 2023.  Bray, Chad (27 January 2014). \"Google Acquires British Artificial Intelligence Developer\". DealBook. Retrieved 4 November 2019.  \"About Us | DeepMind\". DeepMind.  \"A return to Paris | DeepMind\". DeepMind.  \"The Last AI Breakthrough DeepMind Made Before Google Bought It\". The Physics arXiv Blog. 29 January 2014. Retrieved 12 October 2014. Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). \"Neural Turing Machines\". arXiv:1410.5401 [cs.NE].  Best of 2014: Google's Secretive DeepMind Startup Unveils a \"Neural Turing Machine\" Archived 4 December 2015 at the Wayback Machine, MIT Technology Review  Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). \"Hybrid computing using a neural network with dynamic external memory\". Nature. 538 (7626): 471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687. PMID 27732574. S2CID 205251479.  Kohs, Greg (29 September 2017), AlphaGo, Ioannis Antonoglou, Lucas Baker, Nick Bostrom, retrieved 9 January 2018  Silver, David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December 2017).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]You're Mistral, English. Named Entity recognition. Extract the persons. Answer in English. \"DeepMind Technologies Limited – Overview (free company information from Companies House)\". Companies House. Retrieved 13 March 2016.  \"King's Cross - S2 Building - SES Engineering Services\". www.ses-ltd.co.uk. Retrieved 14 July 2022.  Efrati, Amir (11 October 2023). \"DeepMind Cut 20% of Its Expenses Before Merging with Google\". The Information. Archived from the original on 12 October 2023. \"DEEPMIND TECHNOLOGIES LIMITED overview - Find and update company information - GOV.UK\". Companies House. Retrieved 22 July 2023.  Bray, Chad (27 January 2014). \"Google Acquires British Artificial Intelligence Developer\". DealBook. Retrieved 4 November 2019.  \"About Us | DeepMind\". DeepMind.  \"A return to Paris | DeepMind\". DeepMind.  \"The Last AI Breakthrough DeepMind Made Before Google Bought It\". The Physics arXiv Blog. 29 January 2014. Retrieved 12 October 2014. Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). \"Neural Turing Machines\". arXiv:1410.5401 [cs.NE].  Best of 2014: Google's Secretive DeepMind Startup Unveils a \"Neural Turing Machine\" Archived 4 December 2015 at the Wayback Machine, MIT Technology Review  Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). \"Hybrid computing using a neural network with dynamic external memory\". Nature. 538 (7626): 471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687. PMID 27732574. S2CID 205251479.  Kohs, Greg (29 September 2017), AlphaGo, Ioannis Antonoglou, Lucas Baker, Nick Bostrom, retrieved 9 January 2018  Silver, David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December 2017).[/INST]\n",
            "[INST]You're Mistral, English. Named Entity recognition. Extract the persons.\n",
            "Answer in English. \"DeepMind Technologies Limited – Overview (free company\n",
            "information from Companies House)\". Companies House. Retrieved 13 March 2016.\n",
            "\"King's Cross - S2 Building - SES Engineering Services\". www.ses-ltd.co.uk.\n",
            "Retrieved 14 July 2022.  Efrati, Amir (11 October 2023). \"DeepMind Cut 20% of\n",
            "Its Expenses Before Merging with Google\". The Information. Archived from the\n",
            "original on 12 October 2023. \"DEEPMIND TECHNOLOGIES LIMITED overview - Find and\n",
            "update company information - GOV.UK\". Companies House. Retrieved 22 July 2023.\n",
            "Bray, Chad (27 January 2014). \"Google Acquires British Artificial Intelligence\n",
            "Developer\". DealBook. Retrieved 4 November 2019.  \"About Us | DeepMind\".\n",
            "DeepMind.  \"A return to Paris | DeepMind\". DeepMind.  \"The Last AI Breakthrough\n",
            "DeepMind Made Before Google Bought It\". The Physics arXiv Blog. 29 January 2014.\n",
            "Retrieved 12 October 2014. Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014).\n",
            "\"Neural Turing Machines\". arXiv:1410.5401 [cs.NE].  Best of 2014: Google's\n",
            "Secretive DeepMind Startup Unveils a \"Neural Turing Machine\" Archived 4 December\n",
            "2015 at the Wayback Machine, MIT Technology Review  Graves, Alex; Wayne, Greg;\n",
            "Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka;\n",
            "Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October\n",
            "2016). \"Hybrid computing using a neural network with dynamic external memory\".\n",
            "Nature. 538 (7626): 471–476. Bibcode:2016Natur.538..471G.\n",
            "doi:10.1038/nature20101. ISSN 1476-4687. PMID 27732574. S2CID 205251479.  Kohs,\n",
            "Greg (29 September 2017), AlphaGo, Ioannis Antonoglou, Lucas Baker, Nick\n",
            "Bostrom, retrieved 9 January 2018  Silver, David; Hubert, Thomas; Schrittwieser,\n",
            "Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre,\n",
            "Laurent; Kumaran, Dharshan; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen;\n",
            "Hassabis, Demis (5 December 2017).[/INST]\n",
            "<__main__.Options object at 0x7ca303872620>\n",
            "<s> [INST]You're Mistral, English. Named Entity recognition. Extract the persons. Answer in English. \"DeepMind Technologies Limited – Overview (free company information from Companies House)\". Companies House. Retrieved 13 March 2016.  \"King's Cross - S2 Building - SES Engineering Services\". www.ses-ltd.co.uk. Retrieved 14 July 2022.  Efrati, Amir (11 October 2023). \"DeepMind Cut 20% of Its Expenses Before Merging with Google\". The Information. Archived from the original on 12 October 2023. \"DEEPMIND TECHNOLOGIES LIMITED overview - Find and update company information - GOV.UK\". Companies House. Retrieved 22 July 2023.  Bray, Chad (27 January 2014). \"Google Acquires British Artificial Intelligence Developer\". DealBook. Retrieved 4 November 2019.  \"About Us | DeepMind\". DeepMind.  \"A return to Paris | DeepMind\". DeepMind.  \"The Last AI Breakthrough DeepMind Made Before Google Bought It\". The Physics arXiv Blog. 29 January 2014. Retrieved 12 October 2014. Graves, Alex; Wayne, Greg; Danihelka, Ivo (2014). \"Neural Turing Machines\". arXiv:1410.5401 [cs.NE].  Best of 2014: Google's Secretive DeepMind Startup Unveils a \"Neural Turing Machine\" Archived 4 December 2015 at the Wayback Machine, MIT Technology Review  Graves, Alex; Wayne, Greg; Reynolds, Malcolm; Harley, Tim; Danihelka, Ivo; Grabska-Barwińska, Agnieszka; Colmenarejo, Sergio Gómez; Grefenstette, Edward; Ramalho, Tiago (12 October 2016). \"Hybrid computing using a neural network with dynamic external memory\". Nature. 538 (7626): 471–476. Bibcode:2016Natur.538..471G. doi:10.1038/nature20101. ISSN 1476-4687. PMID 27732574. S2CID 205251479.  Kohs, Greg (29 September 2017), AlphaGo, Ioannis Antonoglou, Lucas Baker, Nick Bostrom, retrieved 9 January 2018  Silver, David; Hubert, Thomas; Schrittwieser, Julian; Antonoglou, Ioannis; Lai, Matthew; Guez, Arthur; Lanctot, Marc; Sifre, Laurent; Kumaran, Dharshan; Graepel, Thore; Lillicrap, Timothy; Simonyan, Karen; Hassabis, Demis (5 December "
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 49.06 MiB is free. Process 243489 has 14.70 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 327.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b6ed3bd907e8>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m#culturno() #proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-b6ed3bd907e8>\u001b[0m in \u001b[0;36minteract\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m    \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m    \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mculturno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     messages = [\n",
            "\u001b[0;32m<ipython-input-9-b6ed3bd907e8>\u001b[0m in \u001b[0;36mcycle\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m#generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens, do_sample=True, top_k = options.top_k, top_p = options.top_p, temperature=options.temperature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mprompt_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1593\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2696\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2697\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1158\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 )\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1043\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    758\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# upcast attention to fp32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 49.06 MiB is free. Process 243489 has 14.70 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 327.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\"\n",
        "wr = w.wrap(s)\n",
        "print(s); print(wr)\n",
        "for i in wr: print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzt82L5YmlM5",
        "outputId": "99ff29b8-f628-45bd-af95-4b5e79000be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\n",
            "['Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо,', 'Вазов и Лем. Напиши текст за песен, който започва със селската баня,', 'голям кеф, градската десница, софийската левица с голямата... каца на', 'планетата на маймуните във Вселената на Стартрек, при Хари Потър и', 'философския камък на Властелина на пръстените с лазерния меч на Люк.', 'Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.']\n",
            "Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо,\n",
            "Вазов и Лем. Напиши текст за песен, който започва със селската баня,\n",
            "голям кеф, градската десница, софийската левица с голямата... каца на\n",
            "планетата на маймуните във Вселената на Стартрек, при Хари Потър и\n",
            "философския камък на Властелина на пръстените с лазерния меч на Люк.\n",
            "Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин. \"\n",
        "]\n",
        "# {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "#? why the assistant as well"
      ],
      "metadata": {
        "id": "sdbwldNm0epT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8-_fnzparaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2cuHYah0e_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3i1hIrYwo8WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_and_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1VQ0--zizna",
        "outputId": "b5f9104f-5274-4875-eabd-b251eaa93c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Колко от тях са жени?', '<s> Колко от тях са жени?\\nОбщата средна възраст на всички жени е $30$ години. Каква е средната възраст, в години, на мъжете? Изразете отговора си като десетичен знак до най-близката десета. [/INST]Да приемем, че броят на жените е $w$ и броят на мъжете е $m$.\\nДадено ни е, че $w + m = 32$ и средната възраст на всичките $w + m$ души е $30$ години.\\nСредната възраст може да се изчисли, като се вземе сумата от всички възрасти и се раздели на броя на хората.\\nТака че имаме уравнението $(30)(w + m) = \\\\frac{30}{32}(w + 180)$ (тъй като средната възраст на мъжете е $180$ години).\\nОпростявайки това уравнение, получаваме $')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLOSE\n",
        "f.close()"
      ],
      "metadata": {
        "id": "eQ4Bve9ljiUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Програмирай на Бейсик за APPLE 2 рисуване на триизмерен цилиндър в перспектива. В режим HGR.\"\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "id": "EaEQ6a0JNiPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vJKEzme5V1A4"
      }
    }
  ]
}