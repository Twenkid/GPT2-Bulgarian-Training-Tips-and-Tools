{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9d8e1a675d9643e49b8c2ecbfd46bcf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c5414546b17460ca9f14b85f33087ec",
              "IPY_MODEL_a6cae2325b3c4beebc4928295c291db4",
              "IPY_MODEL_17ca96dfaa8e464b8be91c7c540ed8ac"
            ],
            "layout": "IPY_MODEL_c31d24aabffd456bad6f7ebc8dbf5e5b"
          }
        },
        "7c5414546b17460ca9f14b85f33087ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f8c9d9df4e5474991495f7e6a2a0589",
            "placeholder": "​",
            "style": "IPY_MODEL_91d7a6771e734470b311a7423ac09ffd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a6cae2325b3c4beebc4928295c291db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8cbe2481ab54b81b33f004ce1f45c0b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1915360d2892466a883adf297d268262",
            "value": 2
          }
        },
        "17ca96dfaa8e464b8be91c7c540ed8ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1faf498d44ed474eba3b018d558e1c50",
            "placeholder": "​",
            "style": "IPY_MODEL_9060c529dbed4a8093189d7479ce585a",
            "value": " 2/2 [00:00&lt;00:00,  2.30it/s]"
          }
        },
        "c31d24aabffd456bad6f7ebc8dbf5e5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f8c9d9df4e5474991495f7e6a2a0589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91d7a6771e734470b311a7423ac09ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8cbe2481ab54b81b33f004ce1f45c0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1915360d2892466a883adf297d268262": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1faf498d44ed474eba3b018d558e1c50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9060c529dbed4a8093189d7479ce585a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools/blob/main/bggpt_sacred_computer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k_lKxlssvJTU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GP7wu5oYovWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g8B-4Fo6vIEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **BgGPT в Colab - безплатно на Tesla T4 или TPU**\n",
        "#### Автор: Тодор Арнаудов - Тош от **СВЕЩЕНИЯТ СМЕТАЧ** - Институт за Мислещи машини, творчество и развитие на човека - основан през 2000 г.\n",
        "* http://github.com/twenkid\n",
        "* http://artificial-mind.blogspot.com\n",
        "* http://research.twenkid.com\n",
        "* http://eim.twenkid.com\n",
        "\n",
        "**Видеоръководства:** от канала \"Twenkid Studio - Artificial Mind (todprog): https://www.youtube.com/channel/UCgyhnsM9ed292HUAObXUsvw\n",
        "\n",
        "**Версии**:\n",
        "* 19.2.2024: първа, единични заявки;\n",
        "* 20.2: Цикъл с извиквания, промяна на дължината на породения текст, [INST]...[/INST], промяна на max_... и др.\n",
        "* 22.2: вкл/изкл [INST], ... TextWrapper ... и др.\n",
        "\n",
        "Тази тетрадка е качена в: https://github.com/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools\n",
        "...\n",
        "\n",
        "Тош, тогава още тийнейджър, е автор на \"пророческата\" интердисциплинарна **\"Теория на разума и Вселената (2001-2004)\"** https://github.com/Twenkid/Theory-of-Universe-and-Mind за общ изкуствен интелект и др., която е в основата на **Първия в света интердисциплинарен курс по Универсален изкуствен разум (Artificial General Intelligence)**, който създава и преподава в ПУ \"Паисий Хилендарски\" през 2010 г. https://artificial-mind.blogspot.com/2010/04/universal-artificial-intelligence.html\n",
        "Тош е автор на **Стратегията за развитие на България чрез свръхинтердисциплинарен институт за Изкуствен интелект и създаване на универсални мислещи машини от 2003 г.**, която INSAIT преоткрива с 20 години закъснение и над 200 милиона лв по-голям бюджет: **\"Как бих инвестирал един милион с най-голяма полза за развитието на страната\"**:\n",
        "* https://www.oocities.org/todprog/ese/proekt.htm\n",
        "* https://artificial-mind.blogspot.com/2020/07/interdisciplinary-research-institute.html\n",
        "... на безплатните синтезатори на реч\n",
        "* **\"Глас 2004\" и \"Тошко 2\"**: https://github.com/Twenkid/Toshko_2\n",
        "\n",
        "* ... на **безплатния интелигентнен речник-помощник на преводача \"Smarty\" (2007)** който беше най-\"умното\" подобно приложение в света, с наченки на разбиране на контекста: https://github.com/Twenkid/Smarty\n",
        "\n",
        "* ...на всестранния проект **\"Вси, или Специалист по всичко\"** за инфраструктура за Общ изкуствен интелект, 2022:\n",
        "https://github.com/Twenkid/Vsy-Jack-Of-All-Trades-AGI-Bulgarian-Internet-Archive-And-Search-Engine\n",
        "\n",
        "* Обучава голям езиков модел на българскив Колаб **GPT2-BG Medium, обучен в Колаб през 2021 г.**: - ръководство за обучение, за пораждане на по-\"творчески\" текстове и самият модел:\n",
        "https://github.com/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools\n",
        "\n",
        "и др.\n",
        "\n",
        "Интервю от 2009 г.: **Тодор Арнаудов: Ще създам мислеща машина, която ще се самоусложнява.\n",
        "Фантазьори и авантюристи правят великите открития. Работата на скептиците е да отричат, а после да не вярват на собствените си очи**\n",
        "\n",
        "https://artificial-mind.blogspot.com/2009/11/dreamers-and-adventurists-do-big.html\n",
        "\n",
        "**СВЕЩЕНИЯТ СМЕТАЧ** търси всякакви партньори, съдружници, спонсори, съмишленици. Виж например \"issue\"-то в проекта \"Вси\":\n",
        "https://github.com/Twenkid/Vsy-Jack-Of-All-Trades-AGI-Bulgarian-Internet-Archive-And-Search-Engine/issues/10\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "19.2.2024\n",
        "\n",
        "BgGPT:\n",
        "\n",
        "https://bggpt.ai\n",
        "\n",
        "https://huggingface.co/INSAIT-Institute/BgGPT-7B-Instruct-v0.1/blob/main/README.md?code=true\n",
        "\n",
        "https://bggpt.ai/blog/2024-02-18-launching-the-first-free-and-open-bulgarian-llm/\n",
        "\n",
        "https://huggingface.co/INSAIT-Institute/BgGPT-7B-Instruct-v0.1/tree/main\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PD8yuacUV4bI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwTabNcJXaxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "31vvLOvzZcwh",
        "outputId": "3345f093-4334-40ef-8836-b000c5723b57"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f7e957707e4b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huggingface-cli download INSAIT-Institute/BgGPT-7B-Instruct-v0.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ],
      "source": [
        "# !huggingface-cli download INSAIT-Institute/BgGPT-7B-Instruct-v0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Инсталиране на библиотеки ..."
      ],
      "metadata": {
        "id": "ZLTBxZS1KpQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging ninja\n",
        "!pip install accelerate\n",
        "#!pip install flash-attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdfHe2_2Zu1t",
        "outputId": "82bd60ed-4015-4858-ce00-fc563e0bc391"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "dWSlbIaEbgKE",
        "outputId": "08fbbd83-71fb-4e31-c5f4-6236f753ea49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-04578d045c84>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# List of Available Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Списък с достъпни модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Заредете модела\n",
        "Може да бъде и друг: с път до адрес в huggingface, профил/име-на-модел. Виж в следващата клетка."
      ],
      "metadata": {
        "id": "VzXKbK5p-KRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    #use_flash_attn_2=True,\n",
        ")\n",
        "#device_map=\"auto\",\n",
        "#low_cpu_mem_usage=True, # or crashes\n",
        "#use_flash_attn_2=True,\n",
        "\n",
        "# mistralai/Mistral-7B-Instruct-v0.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "9d8e1a675d9643e49b8c2ecbfd46bcf7",
            "7c5414546b17460ca9f14b85f33087ec",
            "a6cae2325b3c4beebc4928295c291db4",
            "17ca96dfaa8e464b8be91c7c540ed8ac",
            "c31d24aabffd456bad6f7ebc8dbf5e5b",
            "0f8c9d9df4e5474991495f7e6a2a0589",
            "91d7a6771e734470b311a7423ac09ffd",
            "d8cbe2481ab54b81b33f004ce1f45c0b",
            "1915360d2892466a883adf297d268262",
            "1faf498d44ed474eba3b018d558e1c50",
            "9060c529dbed4a8093189d7479ce585a"
          ]
        },
        "id": "zjz9UBdoalFp",
        "outputId": "75c53e4d-73e4-42ab-da81-cfa0aaf41ab8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d8e1a675d9643e49b8c2ecbfd46bcf7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of Available Models\n",
        "# Списък с достъпни модели\n",
        "help(AutoModelForCausalLM.from_pretrained)"
      ],
      "metadata": {
        "id": "zRtq8eV1_OHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_8IfSlcGV2tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EXPERIMENTS\n",
        "===========\n",
        "It doesn't load these mistrals, check formats etc.\n",
        "see also, try:\n",
        "INSAIT-Institute/BgGPT-7B-Instruct-v0.1\n",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "#TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
        "#TheBloke/mistral-7b-instruct-v0.1.Q6_K.gguf\n",
        "#TheBloke/mistral-7b-instruct-v0.1.Q8_0.gguf\n",
        "\n",
        "#TheBloke...: 7.63, 8.44, 10.20 G\n",
        "\"\"\"\n",
        "## LOAD ANOTHER MODEL | ЗАРЕДИ ДРУГ МОДЕЛ\n",
        "\"\"\"\n",
        "ms =[\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.1\"] #\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q6_K.gguf\" #TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "n = 0\n",
        "m = ms[n]\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=m,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "device = \"cuda\"\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8qRWFP3rllY-",
        "outputId": "9f91281f-474d-4060-cb2d-e3c687ad440a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1c7b6a1e6047>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Първо пораждане"
      ],
      "metadata": {
        "id": "d__abiUw-Axk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = AutoModelForCausalLM.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\") #mistralai/Mistral-7B-v0.1\")\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\")\n",
        "device = \"cuda\"\n",
        "#prompt = \"Как се правят принджиничени шляпунцели?\"\n",
        "#prompt = \"Откъде мога да си купя шляпунцели със зеленчуци и шоколад? Обичам да ги мажа с приндиджлячки, но оборвляквам шакалакщряк? Нали? Обясни ми\"\n",
        "prompt = \"Резюмирай в 30 думи: Изброй седемте най-нископлатени тежкоатлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "#Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "prompt = \"[s][INST]\"+prompt+\"[/INST]\" # better Question Answering\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "model.to(device)\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=False, temperature=0.7,top_k=50, repetition_penalty=3.0, top_p=0.8)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "xaelArdPfcam",
        "outputId": "9efbf3bc-84ad-463b-95bb-3caea5ad90db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> [s][INST]Резюмирай в 30 думи: Изброй седемте най-нископлатени тежкоатлети от България. Колко от тях са жени? Кои бягат най-бързо?[/INST]Седем ниско платена тежка категория боксьори включват Даниел Асенов, Симеон Чамов и Тинко Банабаков (мъже), както Стойка Петрова , Деница Елисова & Златислава Чуканова за жените; всички печелят под $15k на битка с изключение може да е имал няколко попъри през кариерата си</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9tihspfEnqCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "0qTPd7sClOg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка на лога за история на запитванията"
      ],
      "metadata": {
        "id": "czoVaeOj-8vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Помощен код за сърхраняване на историята във файл\n",
        "# Utility code for saving to a log file\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def get_time_string():\n",
        "  return datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "\n",
        "#fname = \"llm_log.txt\" # in the current directory; first version\n",
        "fname = \"llm_log_\" + get_time_string() + \".txt\" #unique file\n",
        "\n",
        "f = open(fname, \"at\", encoding=\"utf-8\") # if the file exists, it will be appended at (instead of wt)\n",
        "\n",
        "#Then use f.write ...\n",
        "prompt_arr = []\n",
        "answer_arr = []\n",
        "prompt_and_answer = []\n",
        "max_new_tokens = 250"
      ],
      "metadata": {
        "id": "80BSoWMUlgTf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Read a list of prompts. One per line\n",
        "openfile ...\n",
        "300, ...\n",
        "'''\n"
      ],
      "metadata": {
        "id": "IhSz1Y4zkmF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Language utils ... leverage with nltk, spacy etc.\n",
        "def add_noise(txt):\n",
        "  pass #insert words..."
      ],
      "metadata": {
        "id": "pst4UCdo_v_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sacred Computer's AGI strategy from 2003:\n",
        "# https://www.oocities.org/todprog/ese/proekt.htm\n",
        "# https://artificial-mind.blogspot.com/2020/07/interdisciplinary-research-institute.html\n",
        "# Try resuming\n",
        "strategy = 'Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектиране, мултимедия, текстообработка, преводачи, игри и др. приложни програми.          Целта на Института ще бъде програмно създаване на ММ, притежаваща универсални възможности за обмен на информация с други изчислителни машини, в  частност роботизирани модули. Роботите, създавани от робототехническия отдел, ще бъдат, освен начин за използване на ИР за физически дейности, още средство  за привличане на вниманието на обществеността и за реклама на Института.          След като бъде осъществена Мислеща машина, тя ще може да се използва във всякакви творчески сфери на човешката дейност и в работата на самия Институт.          Предполагам, че след Откритието и създаването на ММ, работеща на стандартни компютри, Институтът ще се \"опаричи\" и ще получи възможност да  обособи проектантски отдел за разработване на нови цялостни изчислителни  системи, пригодени специално за работата на Машината.          Като завършек бих цитирал няколко факта и имена на млади български \"кандидат-творци на разум\".          Преди няколко месеца Бистра Дилкина, завършваща тази година университета \"Саймън Фрейзър\", спечели мащабно  състезание по програмиране, от областта на ИР, в САЩ и с блестящия си ум привлече вниманието на научните среди.          Ахмед Мерчев, 19-годишен, е основател и ръководител на проекта за човекоподобен робот с умствени и физически възможности сходни с човешките - \"Кибертрон\". По-малко от година след обявяването на проекта, в \"Кибертрон\" постигнаха действителни резултати по робототехническото осъществяване на тялото и получиха признание от БАН.          Авторът на това есе, почти 19-годишен, е основател на дружество \"Разум\", което има за  цел \"разнищването\" на разума. Понастоящем то свързва двама изследователи, чиято стратегия е да разберат действието на мисълта чрез многостранно опознаване и овладяване на  науките и изкуствата. За Илиян Георгиев, студент в САЩ, добре  говори кореспонденцията му с Марвин Мински - един от \"бащите\" на науката за Изкуствения разум, от когото новите идеи не спират да бликат до днес.          Новите идеи не спират да извират от младите български  учени, за които съм убеден, че ако бъдат поставени в благоприятни условия за  работа, ще успеят да направят от поточето река, достатъчно пълноводна, така  че Мислеща машина да \"заплава\" по нея от българско \"пристанище\".'\n",
        "print(strategy)\n",
        "\n",
        "#Too long\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-svs39DjlGC",
        "outputId": "cae506e6-b5d1-4271-c7be-0f413311c345"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектиране, мултимедия, текстообработка, преводачи, игри и др. приложни програми.          Целта на Института ще бъде програмно създаване на ММ, притежаваща универсални възможности за обмен на информация с други изчислителни машини, в  частност роботизирани модули. Роботите, създавани от робототехническия отдел, ще бъдат, освен начин за използване на ИР за физически дейности, още средство  за привличане на вниманието на обществеността и за реклама на Института.          След като бъде осъществена Мислеща машина, тя ще може да се използва във всякакви творчески сфери на човешката дейност и в работата на самия Институт.          Предполагам, че след Откритието и създаването на ММ, работеща на стандартни компютри, Институтът ще се \"опаричи\" и ще получи възможност да  обособи проектантски отдел за разработване на нови цялостни изчислителни  системи, пригодени специално за работата на Машината.          Като завършек бих цитирал няколко факта и имена на млади български \"кандидат-творци на разум\".          Преди няколко месеца Бистра Дилкина, завършваща тази година университета \"Саймън Фрейзър\", спечели мащабно  състезание по програмиране, от областта на ИР, в САЩ и с блестящия си ум привлече вниманието на научните среди.          Ахмед Мерчев, 19-годишен, е основател и ръководител на проекта за човекоподобен робот с умствени и физически възможности сходни с човешките - \"Кибертрон\". По-малко от година след обявяването на проекта, в \"Кибертрон\" постигнаха действителни резултати по робототехническото осъществяване на тялото и получиха признание от БАН.          Авторът на това есе, почти 19-годишен, е основател на дружество \"Разум\", което има за  цел \"разнищването\" на разума. Понастоящем то свързва двама изследователи, чиято стратегия е да разберат действието на мисълта чрез многостранно опознаване и овладяване на  науките и изкуствата. За Илиян Георгиев, студент в САЩ, добре  говори кореспонденцията му с Марвин Мински - един от \"бащите\" на науката за Изкуствения разум, от когото новите идеи не спират да бликат до днес.          Новите идеи не спират да извират от младите български  учени, за които съм убеден, че ако бъдат поставени в благоприятни условия за  работа, ще успеят да направят от поточето река, достатъчно пълноводна, така  че Мислеща машина да \"заплава\" по нея от българско \"пристанище\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(strategy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBa23r2AlX5V",
        "outputId": "becfeeca-2be7-4db7-acf3-8c2d7f2e73ac"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3483"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Пакетно извикване на много въпроси, заявки и т.н."
      ],
      "metadata": {
        "id": "Fc7fzn10anz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 24.2.2024\n",
        "# Batch calling of prompts\n",
        "# qs ... (text,  max_len, {params} ...) --> create future structures, more info etc.\n",
        "# or (text, options, {params //for the model//} )  some options may overide the model params\n",
        "# or (text, options) ... opt... contains the params //separate text for dynamically changing the options?\n",
        "# params: top_k, temp, ... options: multiple generation (compare), sliding-window generation, scanning top_k, temperature, \"augmented\", scrambled etc.\n",
        "# A finding: it can answer long questions, even 500?+ characters, out of memory (it's close to the edge, 14.7 GB etc.)!!!\n",
        "\n",
        "#option = {'max: 200'}\n",
        "import copy #not used yet\n",
        "from textwrap import TextWrapper\n",
        "\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()\n",
        "w = TextWrapper(width=80)\n",
        "\n",
        "\n",
        "ls = [\"Иван има 8 плода. Чочко притежава 3 банана. Пенка държи в джоба си шест ягоди. Кое дете има най-много на брой плодове? Кое има най-малко?\",\n",
        "\"Иван има 8 плода. Слънцето пече силно през пролетта. Чочко притежава 3 банана. Маймуните обичат да ядат банани. Пенка държи в джоба си шест ягоди. В градините растат и са червени. Кое дете има най-много на брой плодове? Кое има най-малко?\",\n",
        "\"Чучолина е майка на Магда. Стоянка е леля на Мимето. Цона е дъщеря на Стоянка.  Магда е приятелка на Стоянка. Какви са Магда и Чучолина?\"]\n",
        "\n",
        "class Options:\n",
        "    \"\"\"\n",
        "    __init__(self, general=None, options=None):\n",
        "      self.max_default = 200\n",
        "      if options!=None: self = copy.deepCopy(options) # future use if more complex structrues etc. are used\n",
        "      if general!=None:\n",
        "         self.max_new_tokens = general.max_new_tokens\n",
        "         self.temperature = general.tempperature\n",
        "         self.top_k = general.top_k\n",
        "         self.repetition_penalty = general.repetition_penalty\n",
        "         print(\"init options...\", self)\n",
        "\n",
        "    #general=None,\n",
        "    \"\"\"\n",
        "    def getDefaultPrompt(hint):\n",
        "        return \"Преведи на английски: Котката изпи камъка и литна под нанагорнището.\"\n",
        "\n",
        "\n",
        "    #enabled -- allows to disable some questions etc. without removing them from the list\n",
        "    #if copying: prompt can be null or whatever, it's overwritten by options\n",
        "    #def __init__(self, options=None, prompt=None, enabled = True, inst=True, max=0, t=0, top_k=0, top_p=0, repetition_penalty=0): # #0 means default\n",
        "    def __init__(self, prompt=None, enabled = True, inst=True, max=0, t=0, top_k=0, top_p=0, repetition_penalty=0): # #0 means default\n",
        "       self.max_default = 200\n",
        "       self.t_default = 0.9\n",
        "       self.top_k_default = 3\n",
        "       self.top_p_default = 0.9\n",
        "       #self.max_new_tokens_defau\n",
        "       self.repetition_penalty_default = 1.5\n",
        "       self.inst = inst #[INST][/INST]\n",
        "       self.answer = None\n",
        "       self.ids = None\n",
        "       self.enabled = True\n",
        "\n",
        "       #if options!=None: self = copy.deepcopy(options); print(self) # but still may override the parames future use if more complex structrues etc. are used\n",
        "       if prompt==None: self.prompt = self.getDefaultPrompt()\n",
        "       else: self.prompt = prompt\n",
        "       #elif self.prompt==None: self.prompt = prompt #when copy ctr\n",
        "       print(\"__init__.prompt: \"+self.prompt)\n",
        "\n",
        "       if (max==0): self.max_new_tokens = self.max_default  # could be dictionaries etc.\n",
        "       else: self.max_new_tokens = max\n",
        "       if (t==0): self.temperature = self.t_default\n",
        "       else: self.temperature = t\n",
        "       if (top_k==0): self.top_k = self.top_k_default\n",
        "       else: self.top_k = top_k\n",
        "       if (top_p==0): self.top_p = self.top_p_default\n",
        "       else: self.top_p = top_p\n",
        "       if (repetition_penalty==0): self.repetition_penalty = self.repetition_penalty_default # -1\n",
        "       else:  self.repetition_penalty = repetition_penalty\n",
        "       print(\"init Options...\", self)\n",
        "\n",
        "#call if not initialized\n",
        "def init_model(m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"):\n",
        "  #m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "  device = \"cuda\"\n",
        "  model.to(device)\n",
        "\n",
        "b_new = False\n",
        "b_inst = False # True\n",
        "b_store_ids = False #store token ids - for debugging etc. if true it complicates the simple printing of all params (too much numbers)\n",
        "\n",
        "def batch_questions(qs):\n",
        "  answers = []\n",
        "  results = []\n",
        "\n",
        "  for n,o in enumerate(qs):\n",
        "    #prompt, o = q #could contain control commands such as new [INST] session etc. ... title?... in the output\n",
        "    prompt = o.prompt\n",
        "    if prompt == \"m\": b_new_chat = True\n",
        "    elif prompt == \"t\": title = prompt; continue\n",
        "    if b_inst: prompt = \"[s][INST]\"+prompt+\"[/INST]\"\n",
        "    s = w.wrap(f\"PROMPT {n}:{prompt}\") #,n, prompt)\n",
        "    print(s)\n",
        "\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    print(\"o.max_new_tokens = \", o.max_new_tokens)\n",
        "    #pp.pprint(o)\n",
        "    #pp.pprint(o.__dict__) #__repr__())\n",
        "    print(o.__dict__) #__repr__())\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=o.max_new_tokens, do_sample=True, top_k = o.top_k, top_p = o.top_p, temperature = float(o.temperature), repetition_penalty = o.repetition_penalty )\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    #print(answer)\n",
        "    print(w.wrap(answer))\n",
        "    #record also the tokens!\n",
        "    results.append( (o, generated_ids, answer)) #options contain all...\n",
        "    answers.append( (o, answer))\n",
        "    o.answer = answer\n",
        "    if b_store_ids: o.ids = generated_ids\n",
        "  return (results, answers)\n",
        "\n",
        "qs = []\n",
        "opt = Options()\n",
        "opt.prompt = \"Тръгнал лос с дълъг кос, през града към нос Калиакра!\"\n",
        "qs.append(opt)\n",
        "\n",
        "g = \"The Green ideas furiously sleep!\"\n",
        "#opt_2 = Options(\"Зелените идеи яростно спят!\", t=3.0, top_k=5, top_p=0.4)\n",
        "opt_2 = Options(\"The Green ideas furiously sleep!\", max=40, t=0.9, top_k=40, top_p=0.4)\n",
        "qs.append(opt_2)\n",
        "\n",
        "qs.clear()\n",
        "#for s in ls:\n",
        "#  qs.append(Options(s))\n",
        "\n",
        "qs.append(opt_2)\n",
        "qs.append(Options(g, t=0.0, max=50))\n",
        "#qs.append(Options(g, t=0.0))\n",
        "#qs.append(Options(g, t=2.0))\n",
        "\"\"\"\n",
        "qs.append(Options(g, t=3.0))\n",
        "qs.append(Options(g, top_k=30, t=0.9, top_p=0.8))\n",
        "qs.append(Options(g, top_k=20, t=0.9, top_p=0.8))\n",
        "qs.append(Options(g, top_k=10, t=0.9, top_p=0.8))\n",
        "qs.append(Options(g, top_k=5, t=0.9, top_p=0.8))\n",
        "\"\"\"\n",
        "\n",
        "qs.clear()\n",
        "opt_2.prompt = strategy\n",
        "s1 = \"Резюмирай текста в рамките на \"\n",
        "#s2 = \"100 думи\"\n",
        "#№s2 = \"100 думи\"\n",
        "v = [10, 20, 40, 80, 160, 320, 500]\n",
        "print(\"str(v)\",str(v))\n",
        "for i in v:\n",
        "  g = s1 + str(i) + \" думи.  \" + strategy[0:1200] #[0:500]\n",
        "  print(\"G==\",g)\n",
        "  qs.append(Options(g)) #, top_k=40, t=0.9, top_p=0.8))\n",
        "\n",
        "#qs.append(Options(g, top_k=40, t=2.0, top_p=0.6))\n",
        "#qs.append(Options(g, top_k=40, t=0.01, top_p=0.4))\n",
        "\n",
        "results, answers = batch_questions(qs)\n",
        "\n",
        "#print(answers)\n",
        "#print(results)\n",
        "\n",
        "for o,a in answers:\n",
        "  #print(o.__repr__) #o.repr(tokens=False))\n",
        "  #print(o.prompt)\n",
        "  pp.pprint(o.__dict__)\n",
        "  pp.pprint(a)\n",
        "\n",
        "for o,a in answers:\n",
        "  print(w.wrap(o.prompt))\n",
        "  print(w.wrap(o.answer)) #add answer in option for filling also timing etc. ...\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xnZgpYWx__j6",
        "outputId": "8873d1d0-9d72-44d6-c97a-47ec4bbb51c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__.prompt: Преведи на английски: Котката изпи камъка и литна под нанагорнището.\n",
            "init Options... <__main__.Options object at 0x7ad1e5375300>\n",
            "__init__.prompt: The Green ideas furiously sleep!\n",
            "init Options... <__main__.Options object at 0x7ad1e5376830>\n",
            "__init__.prompt: The Green ideas furiously sleep!\n",
            "init Options... <__main__.Options object at 0x7ad1e5375f60>\n",
            "str(v) [10, 20, 40, 80, 160, 320, 500]\n",
            "G== Резюмирай текста в рамките на 10 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "__init__.prompt: Резюмирай текста в рамките на 10 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "init Options... <__main__.Options object at 0x7ad1e5375420>\n",
            "G== Резюмирай текста в рамките на 20 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "__init__.prompt: Резюмирай текста в рамките на 20 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "init Options... <__main__.Options object at 0x7ad1e5375480>\n",
            "G== Резюмирай текста в рамките на 40 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "__init__.prompt: Резюмирай текста в рамките на 40 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "init Options... <__main__.Options object at 0x7ad1e5377c10>\n",
            "G== Резюмирай текста в рамките на 80 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "__init__.prompt: Резюмирай текста в рамките на 80 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "init Options... <__main__.Options object at 0x7ad1e5375930>\n",
            "G== Резюмирай текста в рамките на 160 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "__init__.prompt: Резюмирай текста в рамките на 160 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "init Options... <__main__.Options object at 0x7ad1e5377d90>\n",
            "G== Резюмирай текста в рамките на 320 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "__init__.prompt: Резюмирай текста в рамките на 320 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "init Options... <__main__.Options object at 0x7ad1e533ee90>\n",
            "G== Резюмирай текста в рамките на 500 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "__init__.prompt: Резюмирай текста в рамките на 500 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира\n",
            "init Options... <__main__.Options object at 0x7ad1e533c1c0>\n",
            "['PROMPT 0:Резюмирай текста в рамките на 10 думи.  Според моята стратегия би се', 'основал научно-изследователски Институт, който ще обединява информатици,', 'инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи,', 'владеещи много езици; творци в различни изкуства - писатели и поети, композитори', 'и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще', 'бъдат, с предимство, имащи знания и умения в повече области, едновременно учени', 'и творци, защото целта на търсенията ще бъде да се открие общото между всички', 'прояви на разума, между науките и изкуствата. Формата на мисълта е различна в', 'различните изяви на мисленето, но същината й, механизмите, които стоят в', 'основата, са едни и същи и се променят само данните, с които тя работи - слово,', 'звук, изображения, последователности от изображения, отвлечени понятия и пр.', 'Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\"', 'даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на', 'таланта им в изследванията.          Институтът ще има програмна къща, в която', '\"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките', 'на Института по пътя към ИР: програми за автоматизирано проектира']\n",
            "o.max_new_tokens =  200\n",
            "{'max_default': 200, 't_default': 0.9, 'top_k_default': 3, 'top_p_default': 0.9, 'repetition_penalty_default': 1.5, 'inst': True, 'answer': None, 'ids': None, 'enabled': True, 'prompt': 'Резюмирай текста в рамките на 10 думи.  Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектира', 'max_new_tokens': 200, 'temperature': 0.9, 'top_k': 3, 'top_p': 0.9, 'repetition_penalty': 1.5}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 13.06 MiB is free. Process 438078 has 14.73 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 249.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-086988a79d1e>\u001b[0m in \u001b[0;36m<cell line: 154>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m#qs.append(Options(g, top_k=40, t=0.01, top_p=0.4))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;31m#print(answers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-086988a79d1e>\u001b[0m in \u001b[0;36mbatch_questions\u001b[0;34m(qs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m#pp.pprint(o.__dict__) #__repr__())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#__repr__())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepetition_penalty\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m#print(answer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1526\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2623\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 )\n\u001b[1;32m   1038\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1040\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# upcast attention to fp32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 13.06 MiB is free. Process 438078 has 14.73 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 249.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ОСНОВЕН ЧАТ ЦИКЪЛ"
      ],
      "metadata": {
        "id": "ckhfVLfH_m3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20.2.2024\n",
        "# MISTRAL - up to 4096 ... context window length, but it seems it won't fit in Tesla T4 - about 2000 or something\n",
        "from textwrap import TextWrapper  # \"word-wrap\" long lines\n",
        "import pprint\n",
        "#import math\n",
        "#import copy\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "device = \"cuda\"\n",
        "model.to(device)\n",
        "b_new_chat = True #insert \"[s][INST]...\"\n",
        "max_new_tokens = 500\n",
        "w = TextWrapper(width=80) #break long lines, default: 70 chars  w.wrap(string)\n",
        "b_autoregression = False #True\n",
        "options = Options(); #current state\n",
        "b_print_options = True\n",
        "\n",
        "def valid_range(m):\n",
        "  if m > 4096: return False # m = 2400\n",
        "  if m < 1: return False\n",
        "  return True\n",
        "\n",
        "def cycle():\n",
        "    global max_new_tokens\n",
        "    global b_new_chat\n",
        "    global b_autoregression\n",
        "    prompt = input(\"Enter prompt... \") #\"Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "    if prompt == \"q\": return False\n",
        "    if prompt == \"n\": b_new_chat = True; return True\n",
        "    if prompt == \"a\": b_autoregression = True; return True #GPT2, GPT3-like: not very smart\n",
        "    if prompt == \"i\": b_autoregression = False; return True #Instruct\n",
        "    if prompt == \"top_k\": top_k=input(\"top_k=... \"); options.top_k=top_k; return True #Should validate etc. in the class, set ...\n",
        "    if prompt == \"top_p\": top_p=input(\"top_k=... \"); options.top_p=top_p; return True #Should validate etc. in the class, set ...\n",
        "    if prompt == \"t\": t=input(\"temperature=... \"); options.temperature=max(0.001, float(t));  options.temperature; return True #Should validate etc. in the class, set ...\n",
        "    if prompt == 's': prompt = strategy[0:len(strategy)//5]\n",
        "    if prompt == \"m\":\n",
        "      mx = int(input(\"max_tokens=?...\"))\n",
        "      if valid_range(mx): max_new_tokens = mx; options.max_new_tokens = mx; return True\n",
        "      else: print(\"Invalid max_new_tokens\")\n",
        "    if not b_autoregression:\n",
        "      if b_new_chat: prompt = \"[s][INST]\"+prompt+\"[/INST]\"; b_new_chat = False\n",
        "      else: prompt=\"[INST]\"+prompt+\"[/INST]\"\n",
        "    print(prompt)\n",
        "    for i in w.wrap(prompt): print(i) #input window is too small\n",
        "    if b_print_options==True: pp.pprint(options)\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    #generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens, do_sample=True, top_k = options.top_k, top_p = options.top_p, temperature=options.temperature)\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=options.max_new_tokens, do_sample=True, top_k = int(options.top_k), top_p = float(options.top_p), temperature=float(options.temperature))\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    prompt_arr.append(prompt)\n",
        "    answer_arr.append(answer)\n",
        "    prompt_and_answer.append((prompt, answer))\n",
        "    #print(w.wrap(answer))\n",
        "    for i in w.wrap(answer): print(i) #print wrapped lines\n",
        "    f.write(\"\\n\"+get_time_string()+\"\\n???: \"+prompt+\"\\n\\n===: \"+answer+\"\\n------------\\n\")\n",
        "    return True\n",
        "\n",
        "def interact():\n",
        "   i = True\n",
        "   while i == True:\n",
        "      i = cycle()\n",
        "def culturno():\n",
        "    messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин. \"}\n",
        "    ]\n",
        "    #encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, do_sample=True)\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    print(answer)\n",
        "#culturno() #proba\n",
        "interact()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "ahInASr5hb-W",
        "outputId": "223d9396-9e0d-4eca-959a-417bd376e2be"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__init__.prompt: Преведи на английски: Котката изпи камъка и литна под нанагорнището.\n",
            "init Options... <__main__.Options object at 0x7ee3615f5030>\n",
            "Enter prompt... Резюмирай текста в рамките на 30 думи\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[s][INST]Резюмирай текста в рамките на 30 думи[/INST]\n",
            "[s][INST]Резюмирай текста в рамките на 30 думи[/INST]\n",
            "<__main__.Options object at 0x7ee3615f5030>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 1.06 MiB is free. Process 390943 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 360.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-96ead0219d8b>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#culturno() #proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-96ead0219d8b>\u001b[0m in \u001b[0;36minteract\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m    \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m    \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mculturno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     messages = [\n",
            "\u001b[0;32m<ipython-input-24-96ead0219d8b>\u001b[0m in \u001b[0;36mcycle\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m#generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens, do_sample=True, top_k = options.top_k, top_p = options.top_p, temperature=options.temperature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mgenerated_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprompt_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1526\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2623\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 )\n\u001b[1;32m   1038\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1040\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/mistral/modeling_mistral.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 1.06 MiB is free. Process 390943 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 360.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\"\n",
        "wr = w.wrap(s)\n",
        "print(s); print(wr)\n",
        "for i in wr: print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzt82L5YmlM5",
        "outputId": "99ff29b8-f628-45bd-af95-4b5e79000be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\n",
            "['Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо,', 'Вазов и Лем. Напиши текст за песен, който започва със селската баня,', 'голям кеф, градската десница, софийската левица с голямата... каца на', 'планетата на маймуните във Вселената на Стартрек, при Хари Потър и', 'философския камък на Властелина на пръстените с лазерния меч на Люк.', 'Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.']\n",
            "Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо,\n",
            "Вазов и Лем. Напиши текст за песен, който започва със селската баня,\n",
            "голям кеф, градската десница, софийската левица с голямата... каца на\n",
            "планетата на маймуните във Вселената на Стартрек, при Хари Потър и\n",
            "философския камък на Властелина на пръстените с лазерния меч на Люк.\n",
            "Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин. \"\n",
        "]\n",
        "# {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "#? why the assistant as well"
      ],
      "metadata": {
        "id": "sdbwldNm0epT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8-_fnzparaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2cuHYah0e_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3i1hIrYwo8WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_and_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1VQ0--zizna",
        "outputId": "b5f9104f-5274-4875-eabd-b251eaa93c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Колко от тях са жени?', '<s> Колко от тях са жени?\\nОбщата средна възраст на всички жени е $30$ години. Каква е средната възраст, в години, на мъжете? Изразете отговора си като десетичен знак до най-близката десета. [/INST]Да приемем, че броят на жените е $w$ и броят на мъжете е $m$.\\nДадено ни е, че $w + m = 32$ и средната възраст на всичките $w + m$ души е $30$ години.\\nСредната възраст може да се изчисли, като се вземе сумата от всички възрасти и се раздели на броя на хората.\\nТака че имаме уравнението $(30)(w + m) = \\\\frac{30}{32}(w + 180)$ (тъй като средната възраст на мъжете е $180$ години).\\nОпростявайки това уравнение, получаваме $')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLOSE\n",
        "f.close()"
      ],
      "metadata": {
        "id": "eQ4Bve9ljiUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Програмирай на Бейсик за APPLE 2 рисуване на триизмерен цилиндър в перспектива. В режим HGR.\"\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "id": "EaEQ6a0JNiPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vJKEzme5V1A4"
      }
    }
  ]
}