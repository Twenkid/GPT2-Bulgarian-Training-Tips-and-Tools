{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "772973314d48467eb32004dbd0c6c29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa930dceb9ed42cdaa7584ab1c8ce9a5",
              "IPY_MODEL_f13fe8ef9e864fce9d20ce6819ba1386",
              "IPY_MODEL_537594a4d41147368198aa8c450942e0"
            ],
            "layout": "IPY_MODEL_39fbf4bfd75f43b2ab42b3ae6c0f5252"
          }
        },
        "aa930dceb9ed42cdaa7584ab1c8ce9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50c2873a1a1b481da9a2f437db9f463a",
            "placeholder": "​",
            "style": "IPY_MODEL_6cbcb9a96cea411983a9eb75dab91af4",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f13fe8ef9e864fce9d20ce6819ba1386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0158c357a2c3491299ca0e38f5394101",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13314b99675a471a888978386537cca3",
            "value": 2
          }
        },
        "537594a4d41147368198aa8c450942e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc8944c7eea04b1893160930728e0f1f",
            "placeholder": "​",
            "style": "IPY_MODEL_637c359ffbaa4f6d87d31f3f033153d6",
            "value": " 2/2 [00:00&lt;00:00,  6.18it/s]"
          }
        },
        "39fbf4bfd75f43b2ab42b3ae6c0f5252": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50c2873a1a1b481da9a2f437db9f463a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cbcb9a96cea411983a9eb75dab91af4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0158c357a2c3491299ca0e38f5394101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13314b99675a471a888978386537cca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc8944c7eea04b1893160930728e0f1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "637c359ffbaa4f6d87d31f3f033153d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools/blob/main/bggpt_sacred_computer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k_lKxlssvJTU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GP7wu5oYovWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g8B-4Fo6vIEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **BgGPT в Colab - безплатно на Tesla T4 или TPU**\n",
        "#### Автор: Тодор Арнаудов - Тош от **СВЕЩЕНИЯТ СМЕТАЧ** - Институт за Мислещи машини, творчество и развитие на човека - основан през 2000 г.\n",
        "* http://github.com/twenkid\n",
        "* http://artificial-mind.blogspot.com\n",
        "* http://research.twenkid.com\n",
        "* http://eim.twenkid.com\n",
        "\n",
        "**Видеоръководства:** от канала \"Twenkid Studio - Artificial Mind (todprog): https://www.youtube.com/channel/UCgyhnsM9ed292HUAObXUsvw\n",
        "\n",
        "**Версии**:\n",
        "* 19.2.2024: първа, единични заявки;\n",
        "* 20.2: Цикъл с извиквания, промяна на дължината на породения текст, [INST]...[/INST], промяна на max_... и др.\n",
        "* 22.2: вкл/изкл [INST], ... TextWrapper ... и др.\n",
        "\n",
        "Тази тетрадка е качена в: https://github.com/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools\n",
        "...\n",
        "\n",
        "Тош, тогава още тийнейджър, е автор на \"пророческата\" интердисциплинарна **\"Теория на разума и Вселената (2001-2004)\"** https://github.com/Twenkid/Theory-of-Universe-and-Mind за общ изкуствен интелект и др., която е в основата на **Първия в света интердисциплинарен курс по Универсален изкуствен разум (Artificial General Intelligence)**, който създава и преподава в ПУ \"Паисий Хилендарски\" през 2010 г. https://artificial-mind.blogspot.com/2010/04/universal-artificial-intelligence.html\n",
        "Тош е автор на **Стратегията за развитие на България чрез свръхинтердисциплинарен институт за Изкуствен интелект и създаване на универсални мислещи машини от 2003 г.**, която INSAIT преоткрива с 20 години закъснение и над 200 милиона лв по-голям бюджет: **\"Как бих инвестирал един милион с най-голяма полза за развитието на страната\"**:\n",
        "* https://www.oocities.org/todprog/ese/proekt.htm\n",
        "* https://artificial-mind.blogspot.com/2020/07/interdisciplinary-research-institute.html\n",
        "... на безплатните синтезатори на реч\n",
        "* **\"Глас 2004\" и \"Тошко 2\"**: https://github.com/Twenkid/Toshko_2\n",
        "\n",
        "* ... на **безплатния интелигентнен речник-помощник на преводача \"Smarty\" (2007)** който беше най-\"умното\" подобно приложение в света, с наченки на разбиране на контекста: https://github.com/Twenkid/Smarty\n",
        "\n",
        "* ...на всестранния проект **\"Вси, или Специалист по всичко\"** за инфраструктура за Общ изкуствен интелект, 2022:\n",
        "https://github.com/Twenkid/Vsy-Jack-Of-All-Trades-AGI-Bulgarian-Internet-Archive-And-Search-Engine\n",
        "\n",
        "* Обучава голям езиков модел на българскив Колаб **GPT2-BG Medium, обучен в Колаб през 2021 г.**: - ръководство за обучение, за пораждане на по-\"творчески\" текстове и самият модел:\n",
        "https://github.com/Twenkid/GPT2-Bulgarian-Training-Tips-and-Tools\n",
        "\n",
        "и др.\n",
        "\n",
        "Интервю от 2009 г.: **Тодор Арнаудов: Ще създам мислеща машина, която ще се самоусложнява.\n",
        "Фантазьори и авантюристи правят великите открития. Работата на скептиците е да отричат, а после да не вярват на собствените си очи**\n",
        "\n",
        "https://artificial-mind.blogspot.com/2009/11/dreamers-and-adventurists-do-big.html\n",
        "\n",
        "**СВЕЩЕНИЯТ СМЕТАЧ** търси всякакви партньори, съдружници, спонсори, съмишленици. Виж например \"issue\"-то в проекта \"Вси\":\n",
        "https://github.com/Twenkid/Vsy-Jack-Of-All-Trades-AGI-Bulgarian-Internet-Archive-And-Search-Engine/issues/10\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "19.2.2024\n",
        "\n",
        "BgGPT:\n",
        "\n",
        "https://bggpt.ai\n",
        "\n",
        "https://huggingface.co/INSAIT-Institute/BgGPT-7B-Instruct-v0.1/blob/main/README.md?code=true\n",
        "\n",
        "https://bggpt.ai/blog/2024-02-18-launching-the-first-free-and-open-bulgarian-llm/\n",
        "\n",
        "https://huggingface.co/INSAIT-Institute/BgGPT-7B-Instruct-v0.1/tree/main\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PD8yuacUV4bI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PwTabNcJXaxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "31vvLOvzZcwh",
        "outputId": "3345f093-4334-40ef-8836-b000c5723b57"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f7e957707e4b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'huggingface-cli download INSAIT-Institute/BgGPT-7B-Instruct-v0.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ],
      "source": [
        "# !huggingface-cli download INSAIT-Institute/BgGPT-7B-Instruct-v0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Инсталиране на библиотеки ..."
      ],
      "metadata": {
        "id": "ZLTBxZS1KpQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install packaging ninja\n",
        "!pip install accelerate\n",
        "#!pip install flash-attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdfHe2_2Zu1t",
        "outputId": "7bb2c082-5b1b-4d0b-cf9e-69cb29d6f7bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.2)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "dWSlbIaEbgKE",
        "outputId": "08fbbd83-71fb-4e31-c5f4-6236f753ea49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-04578d045c84>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# List of Available Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Списък с достъпни модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Заредете модела\n",
        "Може да бъде и друг: с път до адрес в huggingface, профил/име-на-модел. Виж в следващата клетка."
      ],
      "metadata": {
        "id": "VzXKbK5p-KRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upgrade transformers for flash_attn_2"
      ],
      "metadata": {
        "id": "2LW7lIWKJsgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "7CT7gIKIJqDu",
        "outputId": "ea8a489b-b38b-44f1-f321-3d46083569dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.37.2\n",
            "    Uninstalling transformers-4.37.2:\n",
            "      Successfully uninstalled transformers-4.37.2\n",
            "Successfully installed transformers-4.38.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only Ampere or newer! (RTX 3000 for PC etc. On Colab now it's just Tesla T4, 2018)\n",
        "!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m5aGZ_aKu6Y",
        "outputId": "4003ac17-3241-4f40-90c8-302960d180ce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.5.5.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.1.0+cu121)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.5.5-cp310-cp310-linux_x86_64.whl size=120352304 sha256=e70f2f0d4fae98c9ff9742404f3e067bbae56bd3212578ff81c30f73c5ae1a15\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/67/52/8b6d5fcffdd9e1ec868f554cdef8f03eedb4bf4dcac852fca2\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: einops, flash-attn\n",
            "Successfully installed einops-0.7.0 flash-attn-2.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    #use_flash_attention_2 = True  #Only for Geforce 3000+ etc.\n",
        ")\n",
        "#use_flash_attn_2=True,\n",
        "#If using GPU, uncomment low_cpu_mem\n",
        "#If using CPU: comment it (but probably out of memory... it couldn't fit the model)\n",
        "#device_map=\"auto\",\n",
        "#low_cpu_mem_usage=True, # or crashes\n",
        "#use_flash_attn_2=True,\n",
        "# mistralai/Mistral-7B-Instruct-v0.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "772973314d48467eb32004dbd0c6c29b",
            "aa930dceb9ed42cdaa7584ab1c8ce9a5",
            "f13fe8ef9e864fce9d20ce6819ba1386",
            "537594a4d41147368198aa8c450942e0",
            "39fbf4bfd75f43b2ab42b3ae6c0f5252",
            "50c2873a1a1b481da9a2f437db9f463a",
            "6cbcb9a96cea411983a9eb75dab91af4",
            "0158c357a2c3491299ca0e38f5394101",
            "13314b99675a471a888978386537cca3",
            "cc8944c7eea04b1893160930728e0f1f",
            "637c359ffbaa4f6d87d31f3f033153d6"
          ]
        },
        "id": "zjz9UBdoalFp",
        "outputId": "15ffed48-0b3c-4e34-9900-85038b94a742"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "772973314d48467eb32004dbd0c6c29b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / | grep bggpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Npu8I1xNKWD9",
        "outputId": "80bf82e5-404c-419f-c24b-f230b3d69ac5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: ‘/proc/62/task/62/net’: Invalid argument\n",
            "find: ‘/proc/62/net’: Invalid argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of Available Models\n",
        "# Списък с достъпни модели\n",
        "help(AutoModelForCausalLM.from_pretrained)"
      ],
      "metadata": {
        "id": "zRtq8eV1_OHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "e404dda8-207d-44f9-da7b-7352b5aaa61e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7dc02a2f820e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# List of Available Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Списък с достъпни модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_8IfSlcGV2tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "EXPERIMENTS\n",
        "===========\n",
        "It doesn't load these mistrals, check formats etc.\n",
        "see also, try:\n",
        "INSAIT-Institute/BgGPT-7B-Instruct-v0.1\n",
        "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "#TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n",
        "#TheBloke/mistral-7b-instruct-v0.1.Q6_K.gguf\n",
        "#TheBloke/mistral-7b-instruct-v0.1.Q8_0.gguf\n",
        "\n",
        "#TheBloke...: 7.63, 8.44, 10.20 G\n",
        "\"\"\"\n",
        "## LOAD ANOTHER MODEL | ЗАРЕДИ ДРУГ МОДЕЛ\n",
        "\"\"\"\n",
        "ms =[\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\", \"mistralai/Mistral-7B-Instruct-v0.1\"] #\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q6_K.gguf\" #TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "n = 0\n",
        "m = ms[n]\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=m,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "device = \"cuda\"\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8qRWFP3rllY-",
        "outputId": "9f91281f-474d-4060-cb2d-e3c687ad440a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1c7b6a1e6047>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Първо пораждане"
      ],
      "metadata": {
        "id": "d__abiUw-Axk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdJPzJHVV-RP",
        "outputId": "5fec3790-2f39-4ba9-e31f-e9da19a5ebd2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 25 03:38:49 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0              32W /  70W |  15071MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = AutoModelForCausalLM.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\") #mistralai/Mistral-7B-v0.1\")\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\")\n",
        "device = \"cuda\"\n",
        "#prompt = \"Как се правят принджиничени шляпунцели?\"\n",
        "#prompt = \"Откъде мога да си купя шляпунцели със зеленчуци и шоколад? Обичам да ги мажа с приндиджлячки, но оборвляквам шакалакщряк? Нали? Обясни ми\"\n",
        "#prompt = \"Резюмирай в 30 думи: Изброй седемте най-нископлатени тежкоатлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "prompt = \"Резюмирай в 30 думи: Изброй седемте най-високоплатени математици, програмисти, информатици и шофьори от България. Колко от тях са жени? Кои смятат най-бързо?\"\n",
        "\n",
        "prompt = \"Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "prompt = \"[s][INST]\"+prompt+\"[/INST]\" # better Question Answering\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "model.to(device) #if not moved in initialization\n",
        "\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=300, do_sample=False, temperature=0.9,top_k=40, repetition_penalty=1.2, top_p=0.8)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "xaelArdPfcam",
        "outputId": "f7abb179-ca76-4cf5-ff38-d10ba8ff8ef3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<s> [s][INST]Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?[/INST]1. Ивет Лалова - Спринтьорка (лека атлетика)\\n2. Мирела Демирева – Скок на височина (атлетика)\\n3. Габриела Петрова – Троен скок (лека атлетика)\\n4. Радослава Мавродиева – Гюле хвърляне (лека атлетика)\\n5. Тихомир Иванов – Височина на скок (лека атлетика)\\n6. Валентин Андреев - Чук хвърляне (лека атлетика)\\n7. Инна Ефтимова - 100 метра с препятствия (лека атлетика)\\n\\nОт този списък само една жена, Ивет Лалова, е спортистката, която се занимава със спринтове и тя не е най-бързата в списъка. Въпреки това, като цяло жените имат по-малко възможности да печелят високи доходи в спорта поради разликите във възнагражденията между половете.</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9tihspfEnqCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "0qTPd7sClOg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка на лога за история на запитванията"
      ],
      "metadata": {
        "id": "czoVaeOj-8vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Помощен код за сърхраняване на историята във файл\n",
        "# Utility code for saving to a log file\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def get_time_string():\n",
        "  return datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "\n",
        "#fname = \"llm_log.txt\" # in the current directory; first version\n",
        "fname = \"llm_log_\" + get_time_string() + \".txt\" #unique file\n",
        "\n",
        "f = open(fname, \"at\", encoding=\"utf-8\") # if the file exists, it will be appended at (instead of wt)\n",
        "\n",
        "#Then use f.write ...\n",
        "prompt_arr = []\n",
        "answer_arr = []\n",
        "prompt_and_answer = []\n",
        "max_new_tokens = 250"
      ],
      "metadata": {
        "id": "80BSoWMUlgTf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Read a list of prompts. One per line\n",
        "openfile ...\n",
        "300, ...\n",
        "'''\n"
      ],
      "metadata": {
        "id": "IhSz1Y4zkmF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Language utils ... leverage with nltk, spacy etc.\n",
        "def add_noise(txt):\n",
        "  pass #insert words..."
      ],
      "metadata": {
        "id": "pst4UCdo_v_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sacred Computer's AGI strategy from 2003:\n",
        "# https://www.oocities.org/todprog/ese/proekt.htm\n",
        "# https://artificial-mind.blogspot.com/2020/07/interdisciplinary-research-institute.html\n",
        "# Try resuming\n",
        "strategy = 'Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектиране, мултимедия, текстообработка, преводачи, игри и др. приложни програми.          Целта на Института ще бъде програмно създаване на ММ, притежаваща универсални възможности за обмен на информация с други изчислителни машини, в  частност роботизирани модули. Роботите, създавани от робототехническия отдел, ще бъдат, освен начин за използване на ИР за физически дейности, още средство  за привличане на вниманието на обществеността и за реклама на Института.          След като бъде осъществена Мислеща машина, тя ще може да се използва във всякакви творчески сфери на човешката дейност и в работата на самия Институт.          Предполагам, че след Откритието и създаването на ММ, работеща на стандартни компютри, Институтът ще се \"опаричи\" и ще получи възможност да  обособи проектантски отдел за разработване на нови цялостни изчислителни  системи, пригодени специално за работата на Машината.          Като завършек бих цитирал няколко факта и имена на млади български \"кандидат-творци на разум\".          Преди няколко месеца Бистра Дилкина, завършваща тази година университета \"Саймън Фрейзър\", спечели мащабно  състезание по програмиране, от областта на ИР, в САЩ и с блестящия си ум привлече вниманието на научните среди.          Ахмед Мерчев, 19-годишен, е основател и ръководител на проекта за човекоподобен робот с умствени и физически възможности сходни с човешките - \"Кибертрон\". По-малко от година след обявяването на проекта, в \"Кибертрон\" постигнаха действителни резултати по робототехническото осъществяване на тялото и получиха признание от БАН.          Авторът на това есе, почти 19-годишен, е основател на дружество \"Разум\", което има за  цел \"разнищването\" на разума. Понастоящем то свързва двама изследователи, чиято стратегия е да разберат действието на мисълта чрез многостранно опознаване и овладяване на  науките и изкуствата. За Илиян Георгиев, студент в САЩ, добре  говори кореспонденцията му с Марвин Мински - един от \"бащите\" на науката за Изкуствения разум, от когото новите идеи не спират да бликат до днес.          Новите идеи не спират да извират от младите български  учени, за които съм убеден, че ако бъдат поставени в благоприятни условия за  работа, ще успеят да направят от поточето река, достатъчно пълноводна, така  че Мислеща машина да \"заплава\" по нея от българско \"пристанище\".'\n",
        "print(strategy)\n",
        "\n",
        "#Too long\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-svs39DjlGC",
        "outputId": "2216757d-b74f-4f69-af3b-f22af7536cac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Според моята стратегия би се основал научно-изследователски Институт, който ще обединява информатици, инженери, изкуствоведи, езиковеди, философи, психолози, невролози; преводачи, владеещи много езици; творци в различни изкуства - писатели и поети, композитори и музиканти; художници, фотографи и филмови режисьори. Членовете на Института ще бъдат, с предимство, имащи знания и умения в повече области, едновременно учени и творци, защото целта на търсенията ще бъде да се открие общото между всички прояви на разума, между науките и изкуствата. Формата на мисълта е различна в различните изяви на мисленето, но същината й, механизмите, които стоят в основата, са едни и същи и се променят само данните, с които тя работи - слово, звук, изображения, последователности от изображения, отвлечени понятия и пр.          Институтът ще изпълнява и ролята на \"крило\", което намира, \"закриля и окриля\" даровити хора, за да подпомага развитието им и, ако те пожелаят, да се радва на таланта им в изследванията.          Институтът ще има програмна къща, в която \"между другото\" ще се произвежда \"умен\" приложен софтуер, използващ разработките на Института по пътя към ИР: програми за автоматизирано проектиране, мултимедия, текстообработка, преводачи, игри и др. приложни програми.          Целта на Института ще бъде програмно създаване на ММ, притежаваща универсални възможности за обмен на информация с други изчислителни машини, в  частност роботизирани модули. Роботите, създавани от робототехническия отдел, ще бъдат, освен начин за използване на ИР за физически дейности, още средство  за привличане на вниманието на обществеността и за реклама на Института.          След като бъде осъществена Мислеща машина, тя ще може да се използва във всякакви творчески сфери на човешката дейност и в работата на самия Институт.          Предполагам, че след Откритието и създаването на ММ, работеща на стандартни компютри, Институтът ще се \"опаричи\" и ще получи възможност да  обособи проектантски отдел за разработване на нови цялостни изчислителни  системи, пригодени специално за работата на Машината.          Като завършек бих цитирал няколко факта и имена на млади български \"кандидат-творци на разум\".          Преди няколко месеца Бистра Дилкина, завършваща тази година университета \"Саймън Фрейзър\", спечели мащабно  състезание по програмиране, от областта на ИР, в САЩ и с блестящия си ум привлече вниманието на научните среди.          Ахмед Мерчев, 19-годишен, е основател и ръководител на проекта за човекоподобен робот с умствени и физически възможности сходни с човешките - \"Кибертрон\". По-малко от година след обявяването на проекта, в \"Кибертрон\" постигнаха действителни резултати по робототехническото осъществяване на тялото и получиха признание от БАН.          Авторът на това есе, почти 19-годишен, е основател на дружество \"Разум\", което има за  цел \"разнищването\" на разума. Понастоящем то свързва двама изследователи, чиято стратегия е да разберат действието на мисълта чрез многостранно опознаване и овладяване на  науките и изкуствата. За Илиян Георгиев, студент в САЩ, добре  говори кореспонденцията му с Марвин Мински - един от \"бащите\" на науката за Изкуствения разум, от когото новите идеи не спират да бликат до днес.          Новите идеи не спират да извират от младите български  учени, за които съм убеден, че ако бъдат поставени в благоприятни условия за  работа, ще успеят да направят от поточето река, достатъчно пълноводна, така  че Мислеща машина да \"заплава\" по нея от българско \"пристанище\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(strategy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBa23r2AlX5V",
        "outputId": "becfeeca-2be7-4db7-acf3-8c2d7f2e73ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3483"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Пакетно извикване на много въпроси, заявки и т.н."
      ],
      "metadata": {
        "id": "Fc7fzn10anz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 24.2.2024\n",
        "# Batch calling of prompts\n",
        "# qs ... (text,  max_len, {params} ...) --> create future structures, more info etc.\n",
        "# or (text, options, {params //for the model//} )  some options may overide the model params\n",
        "# or (text, options) ... opt... contains the params //separate text for dynamically changing the options?\n",
        "# params: top_k, temp, ... options: multiple generation (compare), sliding-window generation, scanning top_k, temperature, \"augmented\", scrambled etc.\n",
        "# A finding: it can answer long questions, even 500?+ characters, out of memory (it's close to the edge, 14.7 GB etc.)!!!\n",
        "\n",
        "#option = {'max: 200'}\n",
        "import copy #not used yet\n",
        "from textwrap import TextWrapper\n",
        "\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()\n",
        "w = TextWrapper(width=80)\n",
        "\n",
        "\n",
        "ls = [\"Иван има 8 плода. Чочко притежава 3 банана. Пенка държи в джоба си шест ягоди. Кое дете има най-много на брой плодове? Кое има най-малко?\",\n",
        "\"Иван има 8 плода. Слънцето пече силно през пролетта. Чочко притежава 3 банана. Маймуните обичат да ядат банани. Пенка държи в джоба си шест ягоди. В градините растат и са червени. Кое дете има най-много на брой плодове? Кое има най-малко?\",\n",
        "\"Чучолина е майка на Магда. Стоянка е леля на Мимето. Цона е дъщеря на Стоянка.  Магда е приятелка на Стоянка. Какви са Магда и Чучолина?\"]\n",
        "\n",
        "class Options:\n",
        "    \"\"\"\n",
        "    __init__(self, general=None, options=None):\n",
        "      self.max_default = 200\n",
        "      if options!=None: self = copy.deepCopy(options) # future use if more complex structrues etc. are used\n",
        "      if general!=None:\n",
        "         self.max_new_tokens = general.max_new_tokens\n",
        "         self.temperature = general.tempperature\n",
        "         self.top_k = general.top_k\n",
        "         self.repetition_penalty = general.repetition_penalty\n",
        "         print(\"init options...\", self)\n",
        "\n",
        "    #general=None,\n",
        "    \"\"\"\n",
        "    def getDefaultPrompt(hint):\n",
        "        return \"Преведи на английски: Котката изпи камъка и литна под нанагорнището.\"\n",
        "\n",
        "\n",
        "    #enabled -- allows to disable some questions etc. without removing them from the list\n",
        "    #if copying: prompt can be null or whatever, it's overwritten by options\n",
        "    #def __init__(self, options=None, prompt=None, enabled = True, inst=True, max=0, t=0, top_k=0, top_p=0, repetition_penalty=0): # #0 means default\n",
        "    def __init__(self, prompt=None, enabled = True, inst=True, max=0, t=0, top_k=0, top_p=0, repetition_penalty=0): # #0 means default\n",
        "       self.max_default = 1000\n",
        "       self.t_default = 0.9\n",
        "       self.top_k_default = 40\n",
        "       self.top_p_default = 0.93\n",
        "       #self.max_new_tokens_defau\n",
        "       self.repetition_penalty_default = 1.2\n",
        "       self.inst = inst #[INST][/INST]\n",
        "       self.answer = None\n",
        "       self.ids = None\n",
        "       self.enabled = True\n",
        "\n",
        "       #if options!=None: self = copy.deepcopy(options); print(self) # but still may override the parames future use if more complex structrues etc. are used\n",
        "       if prompt==None: self.prompt = self.getDefaultPrompt()\n",
        "       else: self.prompt = prompt\n",
        "       #elif self.prompt==None: self.prompt = prompt #when copy ctr\n",
        "       print(\"__init__.prompt: \"+self.prompt)\n",
        "\n",
        "       if (max==0): self.max_new_tokens = self.max_default  # could be dictionaries etc.\n",
        "       else: self.max_new_tokens = max\n",
        "       if (t==0): self.temperature = self.t_default\n",
        "       else: self.temperature = t\n",
        "       if (top_k==0): self.top_k = self.top_k_default\n",
        "       else: self.top_k = top_k\n",
        "       if (top_p==0): self.top_p = self.top_p_default\n",
        "       else: self.top_p = top_p\n",
        "       if (repetition_penalty==0): self.repetition_penalty = self.repetition_penalty_default # -1\n",
        "       else:  self.repetition_penalty = repetition_penalty\n",
        "       print(\"init Options...\", self)\n",
        "\n",
        "#call if not initialized\n",
        "def init_model(m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"):\n",
        "  #m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
        "  tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "  device = \"cuda\"\n",
        "  model.to(device)\n",
        "\n",
        "b_new = False\n",
        "b_inst = True #False # True\n",
        "b_store_ids = False #store token ids - for debugging etc. if true it complicates the simple printing of all params (too much numbers)\n",
        "\n",
        "def batch_questions(qs):\n",
        "  answers = []\n",
        "  results = []\n",
        "\n",
        "  for n,o in enumerate(qs):\n",
        "    #prompt, o = q #could contain control commands such as new [INST] session etc. ... title?... in the output\n",
        "    prompt = o.prompt\n",
        "    if prompt == \"m\": b_new_chat = True\n",
        "    elif prompt == \"t\": title = prompt; continue\n",
        "    if b_inst: prompt = \"[s][INST]\"+prompt+\"[/INST]\"\n",
        "    s = w.wrap(f\"PROMPT {n}:{prompt}\") #,n, prompt)\n",
        "    f.write(prompt)\n",
        "    print(s)\n",
        "\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    print(\"o.max_new_tokens = \", o.max_new_tokens)\n",
        "    #pp.pprint(o)\n",
        "    #pp.pprint(o.__dict__) #__repr__())\n",
        "    print(o.__dict__) #__repr__())\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=o.max_new_tokens, do_sample=True, top_k = o.top_k, top_p = o.top_p, temperature = float(o.temperature), repetition_penalty = o.repetition_penalty )\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    #print(answer)\n",
        "    print(w.wrap(answer))\n",
        "    #record also the tokens!\n",
        "    results.append( (o, generated_ids, answer)) #options contain all...\n",
        "    answers.append( (o, answer))\n",
        "    o.answer = answer #copy.copy(answer)    )\n",
        "    f.write(\"\\n\"+get_time_string()+\"\\n???: \"+prompt+\"\\n\\n===: \"+answer+\"\\n------------\\n\")\n",
        "    f.flush()\n",
        "    if b_store_ids: o.ids = generated_ids\n",
        "  return (results, answers)\n",
        "\n",
        "qs = []\n",
        "opt = Options()\n",
        "opt.prompt = \"Тръгнал лос с дълъг кос, през града към нос Калиакра!\"\n",
        "qs.append(opt)\n",
        "\n",
        "g = \"The Green ideas furiously sleep!\"\n",
        "#opt_2 = Options(\"Зелените идеи яростно спят!\", t=3.0, top_k=5, top_p=0.4)\n",
        "opt_2 = Options(\"The Green ideas furiously sleep!\", max=40, t=0.9, top_k=40, top_p=0.4)\n",
        "qs.append(opt_2)\n",
        "\n",
        "qs.clear()\n",
        "#for s in ls:\n",
        "#  qs.append(Options(s))\n",
        "\n",
        "qs.append(opt_2)\n",
        "qs.append(Options(g, t=0.0, max=50))\n",
        "#qs.append(Options(g, t=0.0))\n",
        "#qs.append(Options(g, t=2.0))\n",
        "\"\"\"\n",
        "qs.append(Options(g, t=3.0))\n",
        "qs.append(Options(g, top_k=30, t=0.9, top_p=0.8))\n",
        "qs.append(Options(g, top_k=20, t=0.9, top_p=0.8))\n",
        "qs.append(Options(g, top_k=10, t=0.9, top_p=0.8))\n",
        "qs.append(Options(g, top_k=5, t=0.9, top_p=0.8))\n",
        "\"\"\"\n",
        "qs.clear()\n",
        "opt_2.prompt = strategy\n",
        "s1 = \"Резюмирай текста в рамките на \"\n",
        "#s2 = \"100 думи\"\n",
        "#№s2 = \"100 думи\"\n",
        "#v = [10, 20, 40, 80, 160, 320, 500]\n",
        "#v = [10, 20, 40, 80, 160, 320, 500]\n",
        "v = [33,33,33,33,33]\n",
        "tops = [40,30,20,10,5]\n",
        "print(\"str(v)\",str(v))\n",
        "for i,topk in zip(v,tops):\n",
        "  g = s1 + str(i) + \" думи. \" # + strategy[0:1500] #[0:1200] OK 25.2.2024 #[0:500]\n",
        "  print(\"G==\",g)\n",
        "  qs.append(Options(g, top_k=topk)) #, top_k=40, t=0.9, top_p=0.8))\n",
        "\n",
        "#OOM Error  - can't cope even with 500?\n",
        "\"\"\" TEST AGAIN:\n",
        "\n",
        "s1 = \"Резюмирай текста в рамките на \"\n",
        "#s2 = \"100 думи\"\n",
        "#№s2 = \"100 думи\"\n",
        "v = [10, 20, 40, 80, 160, 320, 500]\n",
        "print(\"str(v)\",str(v))\n",
        "for i in v:\n",
        "  g = s1 + str(i) + \" думи.  \" # EMPTY - lost due to blue screen afterwards + strategy[0:1200] #[0:500]\n",
        "  print(\"G==\",g)\n",
        "  qs.append(Options(g)) #, top_k=40, t=0.9, top_p=0.8))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#qs.append(Options(g, top_k=40, t=2.0, top_p=0.6))\n",
        "#qs.append(Options(g, top_k=40, t=0.01, top_p=0.4))\n",
        "\n",
        "results, answers = batch_questions(qs)\n",
        "\n",
        "#print(answers)\n",
        "#print(results)\n",
        "\n",
        "for o,a in answers:\n",
        "  #print(o.__repr__) #o.repr(tokens=False))\n",
        "  #print(o.prompt)\n",
        "  pp.pprint(o.__dict__)\n",
        "  pp.pprint(a)\n",
        "\n",
        "for o,a in answers:\n",
        "  print(w.wrap(o.prompt))\n",
        "  print(w.wrap(o.answer)) #add answer in option for filling also timing etc. ...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnZgpYWx__j6",
        "outputId": "a6e7932e-01f6-40b0-c829-67587281cbde"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__.prompt: Преведи на английски: Котката изпи камъка и литна под нанагорнището.\n",
            "init Options... <__main__.Options object at 0x7b5492efa020>\n",
            "__init__.prompt: The Green ideas furiously sleep!\n",
            "init Options... <__main__.Options object at 0x7b5492ef9fc0>\n",
            "__init__.prompt: The Green ideas furiously sleep!\n",
            "init Options... <__main__.Options object at 0x7b5492efa4d0>\n",
            "str(v) [33, 33, 33, 33, 33]\n",
            "G== Резюмирай текста в рамките на 33 думи. \n",
            "__init__.prompt: Резюмирай текста в рамките на 33 думи. \n",
            "init Options... <__main__.Options object at 0x7b5492efa2f0>\n",
            "G== Резюмирай текста в рамките на 33 думи. \n",
            "__init__.prompt: Резюмирай текста в рамките на 33 думи. \n",
            "init Options... <__main__.Options object at 0x7b5492efb0a0>\n",
            "G== Резюмирай текста в рамките на 33 думи. \n",
            "__init__.prompt: Резюмирай текста в рамките на 33 думи. \n",
            "init Options... <__main__.Options object at 0x7b5492efa140>\n",
            "G== Резюмирай текста в рамките на 33 думи. \n",
            "__init__.prompt: Резюмирай текста в рамките на 33 думи. \n",
            "init Options... <__main__.Options object at 0x7b5492ef93f0>\n",
            "G== Резюмирай текста в рамките на 33 думи. \n",
            "__init__.prompt: Резюмирай текста в рамките на 33 думи. \n",
            "init Options... <__main__.Options object at 0x7b5492efbe20>\n",
            "['PROMPT 0:[s][INST]Резюмирай текста в рамките на 33 думи. [/INST]']\n",
            "o.max_new_tokens =  1000\n",
            "{'max_default': 1000, 't_default': 0.9, 'top_k_default': 40, 'top_p_default': 0.93, 'repetition_penalty_default': 1.2, 'inst': True, 'answer': None, 'ids': None, 'enabled': True, 'prompt': 'Резюмирай текста в рамките на 33 думи. ', 'max_new_tokens': 1000, 'temperature': 0.9, 'top_k': 40, 'top_p': 0.93, 'repetition_penalty': 1.2}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Президентът Байдън', 'обяви национална извънредна ситуация по отношение на киберзаплахите,', 'подчертавайки необходимостта от засилени защити срещу враждебни субекти и', 'критична инфраструктура. Изявлението идва след поредица от атаки като', 'SolarWinds, които демонстрират заплахата от чуждестранни играчи за националната', 'сигурност на САЩ. Това решение ще позволи допълнителна федерална подкрепа за', 'подобряване на цялостната устойчивост и намаляване на рисковете за частния', 'сектор.</s>']\n",
            "['PROMPT 1:[s][INST]Резюмирай текста в рамките на 33 думи. [/INST]']\n",
            "o.max_new_tokens =  1000\n",
            "{'max_default': 1000, 't_default': 0.9, 'top_k_default': 40, 'top_p_default': 0.93, 'repetition_penalty_default': 1.2, 'inst': True, 'answer': None, 'ids': None, 'enabled': True, 'prompt': 'Резюмирай текста в рамките на 33 думи. ', 'max_new_tokens': 1000, 'temperature': 0.9, 'top_k': 30, 'top_p': 0.93, 'repetition_penalty': 1.2}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Войната за', 'независимост на САЩ започва през 1775 г., когато колонистите се изправят срещу', 'Великобритания поради данъчно облагане без представителство и търсене на', 'самоуправление. След като спечелиха ключови битки при Конкорд, Лексингтън и', 'Бостън, Декларацията за независимост беше подписана от колониите през юли 1776', 'г., обявявайки ги официално за Съединени американски щати. Войната приключи с', 'подписването на Парижкия договор през 1783 г., който признава суверенитета на', 'САЩ.</s>']\n",
            "['PROMPT 2:[s][INST]Резюмирай текста в рамките на 33 думи. [/INST]']\n",
            "o.max_new_tokens =  1000\n",
            "{'max_default': 1000, 't_default': 0.9, 'top_k_default': 40, 'top_p_default': 0.93, 'repetition_penalty_default': 1.2, 'inst': True, 'answer': None, 'ids': None, 'enabled': True, 'prompt': 'Резюмирай текста в рамките на 33 думи. ', 'max_new_tokens': 1000, 'temperature': 0.9, 'top_k': 20, 'top_p': 0.93, 'repetition_penalty': 1.2}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Криптографията е', 'метод за защита на информация чрез трансформирането й в кодирана форма, която', 'може да бъде декодирана само от определени получатели с правилния ключ. Използва', 'се в цифрови комуникации и системи за криптиране като SSL/TLS сертификати и', 'електронни подписи. Сигурността зависи както от сложността на използваната', 'функция за криптиране, така и от секретността на ключовете.</s>']\n",
            "['PROMPT 3:[s][INST]Резюмирай текста в рамките на 33 думи. [/INST]']\n",
            "o.max_new_tokens =  1000\n",
            "{'max_default': 1000, 't_default': 0.9, 'top_k_default': 40, 'top_p_default': 0.93, 'repetition_penalty_default': 1.2, 'inst': True, 'answer': None, 'ids': None, 'enabled': True, 'prompt': 'Резюмирай текста в рамките на 33 думи. ', 'max_new_tokens': 1000, 'temperature': 0.9, 'top_k': 10, 'top_p': 0.93, 'repetition_penalty': 1.2}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]В тази дискусия', 'разглеждаме как да подобрим нашата производителност и удовлетворение от', 'работата, като се справяме ефективно с прекъсванията по време на работния ден.', 'Ще проучим различни техники за управление на тези смущения, включително', 'поставяне на граници, приоритизиране и използване на инструменти като календарни', 'приложения или асистенти с изкуствен интелект. Освен това ще обсъдим значението', 'на почивките и внимателността в поддържането на фокуса и намаляване на стреса', 'през целия работен ден. В крайна сметка целта е да постигнете баланс между', 'професионалните задължения и личното благополучие, което води до повишено', 'удовлетворение както на работа, така и извън нея.</s>']\n",
            "['PROMPT 4:[s][INST]Резюмирай текста в рамките на 33 думи. [/INST]']\n",
            "o.max_new_tokens =  1000\n",
            "{'max_default': 1000, 't_default': 0.9, 'top_k_default': 40, 'top_p_default': 0.93, 'repetition_penalty_default': 1.2, 'inst': True, 'answer': None, 'ids': None, 'enabled': True, 'prompt': 'Резюмирай текста в рамките на 33 думи. ', 'max_new_tokens': 1000, 'temperature': 0.9, 'top_k': 5, 'top_p': 0.93, 'repetition_penalty': 1.2}\n",
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Внедряването на AI и', 'роботиката ще промени бъдещето на работата, което води до повишена ефективност,', 'производителност и качество на живот. Въпреки това, тези постижения идват със', 'значителни предизвикателства като изместване на работа и необходимост от', 'преквалификация за работниците. Политиците трябва да се справят с този проблем', 'чрез насърчаване на образованието и ученето през целия живот, както и', 'осигуряване на социални мрежи за защита срещу безработица и неравенство в', 'доходите.</s>']\n",
            "{'answer': '<s> [s][INST]Резюмирай текста в рамките на 33 думи. '\n",
            "           '[/INST]Президентът Байдън обяви национална извънредна ситуация по '\n",
            "           'отношение на киберзаплахите, подчертавайки необходимостта от '\n",
            "           'засилени защити срещу враждебни субекти и критична инфраструктура. '\n",
            "           'Изявлението идва след поредица от атаки като SolarWinds, които '\n",
            "           'демонстрират заплахата от чуждестранни играчи за националната '\n",
            "           'сигурност на САЩ. Това решение ще позволи допълнителна федерална '\n",
            "           'подкрепа за подобряване на цялостната устойчивост и намаляване на '\n",
            "           'рисковете за частния сектор.</s>',\n",
            " 'enabled': True,\n",
            " 'ids': None,\n",
            " 'inst': True,\n",
            " 'max_default': 1000,\n",
            " 'max_new_tokens': 1000,\n",
            " 'prompt': 'Резюмирай текста в рамките на 33 думи. ',\n",
            " 'repetition_penalty': 1.2,\n",
            " 'repetition_penalty_default': 1.2,\n",
            " 't_default': 0.9,\n",
            " 'temperature': 0.9,\n",
            " 'top_k': 40,\n",
            " 'top_k_default': 40,\n",
            " 'top_p': 0.93,\n",
            " 'top_p_default': 0.93}\n",
            "('<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Президентът '\n",
            " 'Байдън обяви национална извънредна ситуация по отношение на киберзаплахите, '\n",
            " 'подчертавайки необходимостта от засилени защити срещу враждебни субекти и '\n",
            " 'критична инфраструктура. Изявлението идва след поредица от атаки като '\n",
            " 'SolarWinds, които демонстрират заплахата от чуждестранни играчи за '\n",
            " 'националната сигурност на САЩ. Това решение ще позволи допълнителна '\n",
            " 'федерална подкрепа за подобряване на цялостната устойчивост и намаляване на '\n",
            " 'рисковете за частния сектор.</s>')\n",
            "{'answer': '<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Войната '\n",
            "           'за независимост на САЩ започва през 1775 г., когато колонистите се '\n",
            "           'изправят срещу Великобритания поради данъчно облагане без '\n",
            "           'представителство и търсене на самоуправление. След като спечелиха '\n",
            "           'ключови битки при Конкорд, Лексингтън и Бостън, Декларацията за '\n",
            "           'независимост беше подписана от колониите през юли 1776 г., '\n",
            "           'обявявайки ги официално за Съединени американски щати. Войната '\n",
            "           'приключи с подписването на Парижкия договор през 1783 г., който '\n",
            "           'признава суверенитета на САЩ.</s>',\n",
            " 'enabled': True,\n",
            " 'ids': None,\n",
            " 'inst': True,\n",
            " 'max_default': 1000,\n",
            " 'max_new_tokens': 1000,\n",
            " 'prompt': 'Резюмирай текста в рамките на 33 думи. ',\n",
            " 'repetition_penalty': 1.2,\n",
            " 'repetition_penalty_default': 1.2,\n",
            " 't_default': 0.9,\n",
            " 'temperature': 0.9,\n",
            " 'top_k': 30,\n",
            " 'top_k_default': 40,\n",
            " 'top_p': 0.93,\n",
            " 'top_p_default': 0.93}\n",
            "('<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Войната за '\n",
            " 'независимост на САЩ започва през 1775 г., когато колонистите се изправят '\n",
            " 'срещу Великобритания поради данъчно облагане без представителство и търсене '\n",
            " 'на самоуправление. След като спечелиха ключови битки при Конкорд, Лексингтън '\n",
            " 'и Бостън, Декларацията за независимост беше подписана от колониите през юли '\n",
            " '1776 г., обявявайки ги официално за Съединени американски щати. Войната '\n",
            " 'приключи с подписването на Парижкия договор през 1783 г., който признава '\n",
            " 'суверенитета на САЩ.</s>')\n",
            "{'answer': '<s> [s][INST]Резюмирай текста в рамките на 33 думи. '\n",
            "           '[/INST]Криптографията е метод за защита на информация чрез '\n",
            "           'трансформирането й в кодирана форма, която може да бъде декодирана '\n",
            "           'само от определени получатели с правилния ключ. Използва се в '\n",
            "           'цифрови комуникации и системи за криптиране като SSL/TLS '\n",
            "           'сертификати и електронни подписи. Сигурността зависи както от '\n",
            "           'сложността на използваната функция за криптиране, така и от '\n",
            "           'секретността на ключовете.</s>',\n",
            " 'enabled': True,\n",
            " 'ids': None,\n",
            " 'inst': True,\n",
            " 'max_default': 1000,\n",
            " 'max_new_tokens': 1000,\n",
            " 'prompt': 'Резюмирай текста в рамките на 33 думи. ',\n",
            " 'repetition_penalty': 1.2,\n",
            " 'repetition_penalty_default': 1.2,\n",
            " 't_default': 0.9,\n",
            " 'temperature': 0.9,\n",
            " 'top_k': 20,\n",
            " 'top_k_default': 40,\n",
            " 'top_p': 0.93,\n",
            " 'top_p_default': 0.93}\n",
            "('<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Криптографията е '\n",
            " 'метод за защита на информация чрез трансформирането й в кодирана форма, '\n",
            " 'която може да бъде декодирана само от определени получатели с правилния '\n",
            " 'ключ. Използва се в цифрови комуникации и системи за криптиране като SSL/TLS '\n",
            " 'сертификати и електронни подписи. Сигурността зависи както от сложността на '\n",
            " 'използваната функция за криптиране, така и от секретността на ключовете.</s>')\n",
            "{'answer': '<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]В тази '\n",
            "           'дискусия разглеждаме как да подобрим нашата производителност и '\n",
            "           'удовлетворение от работата, като се справяме ефективно с '\n",
            "           'прекъсванията по време на работния ден. Ще проучим различни '\n",
            "           'техники за управление на тези смущения, включително поставяне на '\n",
            "           'граници, приоритизиране и използване на инструменти като '\n",
            "           'календарни приложения или асистенти с изкуствен интелект. Освен '\n",
            "           'това ще обсъдим значението на почивките и внимателността в '\n",
            "           'поддържането на фокуса и намаляване на стреса през целия работен '\n",
            "           'ден. В крайна сметка целта е да постигнете баланс между '\n",
            "           'професионалните задължения и личното благополучие, което води до '\n",
            "           'повишено удовлетворение както на работа, така и извън нея.</s>',\n",
            " 'enabled': True,\n",
            " 'ids': None,\n",
            " 'inst': True,\n",
            " 'max_default': 1000,\n",
            " 'max_new_tokens': 1000,\n",
            " 'prompt': 'Резюмирай текста в рамките на 33 думи. ',\n",
            " 'repetition_penalty': 1.2,\n",
            " 'repetition_penalty_default': 1.2,\n",
            " 't_default': 0.9,\n",
            " 'temperature': 0.9,\n",
            " 'top_k': 10,\n",
            " 'top_k_default': 40,\n",
            " 'top_p': 0.93,\n",
            " 'top_p_default': 0.93}\n",
            "('<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]В тази дискусия '\n",
            " 'разглеждаме как да подобрим нашата производителност и удовлетворение от '\n",
            " 'работата, като се справяме ефективно с прекъсванията по време на работния '\n",
            " 'ден. Ще проучим различни техники за управление на тези смущения, включително '\n",
            " 'поставяне на граници, приоритизиране и използване на инструменти като '\n",
            " 'календарни приложения или асистенти с изкуствен интелект. Освен това ще '\n",
            " 'обсъдим значението на почивките и внимателността в поддържането на фокуса и '\n",
            " 'намаляване на стреса през целия работен ден. В крайна сметка целта е да '\n",
            " 'постигнете баланс между професионалните задължения и личното благополучие, '\n",
            " 'което води до повишено удовлетворение както на работа, така и извън нея.</s>')\n",
            "{'answer': '<s> [s][INST]Резюмирай текста в рамките на 33 думи. '\n",
            "           '[/INST]Внедряването на AI и роботиката ще промени бъдещето на '\n",
            "           'работата, което води до повишена ефективност, производителност и '\n",
            "           'качество на живот. Въпреки това, тези постижения идват със '\n",
            "           'значителни предизвикателства като изместване на работа и '\n",
            "           'необходимост от преквалификация за работниците. Политиците трябва '\n",
            "           'да се справят с този проблем чрез насърчаване на образованието и '\n",
            "           'ученето през целия живот, както и осигуряване на социални мрежи за '\n",
            "           'защита срещу безработица и неравенство в доходите.</s>',\n",
            " 'enabled': True,\n",
            " 'ids': None,\n",
            " 'inst': True,\n",
            " 'max_default': 1000,\n",
            " 'max_new_tokens': 1000,\n",
            " 'prompt': 'Резюмирай текста в рамките на 33 думи. ',\n",
            " 'repetition_penalty': 1.2,\n",
            " 'repetition_penalty_default': 1.2,\n",
            " 't_default': 0.9,\n",
            " 'temperature': 0.9,\n",
            " 'top_k': 5,\n",
            " 'top_k_default': 40,\n",
            " 'top_p': 0.93,\n",
            " 'top_p_default': 0.93}\n",
            "('<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Внедряването на '\n",
            " 'AI и роботиката ще промени бъдещето на работата, което води до повишена '\n",
            " 'ефективност, производителност и качество на живот. Въпреки това, тези '\n",
            " 'постижения идват със значителни предизвикателства като изместване на работа '\n",
            " 'и необходимост от преквалификация за работниците. Политиците трябва да се '\n",
            " 'справят с този проблем чрез насърчаване на образованието и ученето през '\n",
            " 'целия живот, както и осигуряване на социални мрежи за защита срещу '\n",
            " 'безработица и неравенство в доходите.</s>')\n",
            "['Резюмирай текста в рамките на 33 думи.']\n",
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Президентът Байдън', 'обяви национална извънредна ситуация по отношение на киберзаплахите,', 'подчертавайки необходимостта от засилени защити срещу враждебни субекти и', 'критична инфраструктура. Изявлението идва след поредица от атаки като', 'SolarWinds, които демонстрират заплахата от чуждестранни играчи за националната', 'сигурност на САЩ. Това решение ще позволи допълнителна федерална подкрепа за', 'подобряване на цялостната устойчивост и намаляване на рисковете за частния', 'сектор.</s>']\n",
            "['Резюмирай текста в рамките на 33 думи.']\n",
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Войната за', 'независимост на САЩ започва през 1775 г., когато колонистите се изправят срещу', 'Великобритания поради данъчно облагане без представителство и търсене на', 'самоуправление. След като спечелиха ключови битки при Конкорд, Лексингтън и', 'Бостън, Декларацията за независимост беше подписана от колониите през юли 1776', 'г., обявявайки ги официално за Съединени американски щати. Войната приключи с', 'подписването на Парижкия договор през 1783 г., който признава суверенитета на', 'САЩ.</s>']\n",
            "['Резюмирай текста в рамките на 33 думи.']\n",
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Криптографията е', 'метод за защита на информация чрез трансформирането й в кодирана форма, която', 'може да бъде декодирана само от определени получатели с правилния ключ. Използва', 'се в цифрови комуникации и системи за криптиране като SSL/TLS сертификати и', 'електронни подписи. Сигурността зависи както от сложността на използваната', 'функция за криптиране, така и от секретността на ключовете.</s>']\n",
            "['Резюмирай текста в рамките на 33 думи.']\n",
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]В тази дискусия', 'разглеждаме как да подобрим нашата производителност и удовлетворение от', 'работата, като се справяме ефективно с прекъсванията по време на работния ден.', 'Ще проучим различни техники за управление на тези смущения, включително', 'поставяне на граници, приоритизиране и използване на инструменти като календарни', 'приложения или асистенти с изкуствен интелект. Освен това ще обсъдим значението', 'на почивките и внимателността в поддържането на фокуса и намаляване на стреса', 'през целия работен ден. В крайна сметка целта е да постигнете баланс между', 'професионалните задължения и личното благополучие, което води до повишено', 'удовлетворение както на работа, така и извън нея.</s>']\n",
            "['Резюмирай текста в рамките на 33 думи.']\n",
            "['<s> [s][INST]Резюмирай текста в рамките на 33 думи. [/INST]Внедряването на AI и', 'роботиката ще промени бъдещето на работата, което води до повишена ефективност,', 'производителност и качество на живот. Въпреки това, тези постижения идват със', 'значителни предизвикателства като изместване на работа и необходимост от', 'преквалификация за работниците. Политиците трябва да се справят с този проблем', 'чрез насърчаване на образованието и ученето през целия живот, както и', 'осигуряване на социални мрежи за защита срещу безработица и неравенство в', 'доходите.</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi\n",
        "#!/usr/local/cuda/bin/nvcc --version\n",
        "#NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "2-uEaQh7Tv0B",
        "outputId": "9cc9769c-19aa-46df-cffb-5f64dd993785"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-38db1180233d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# !nvidia-smi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/usr/local/cuda/bin/nvcc --version'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ОСНОВЕН ЧАТ ЦИКЪЛ"
      ],
      "metadata": {
        "id": "ckhfVLfH_m3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20.2.2024\n",
        "# MISTRAL - up to 4096 ... context window length, but it seems it won't fit in Tesla T4 - about 2000 or something\n",
        "from textwrap import TextWrapper  # \"word-wrap\" long lines\n",
        "import pprint\n",
        "#import math\n",
        "#import copy\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "m = \"INSAIT-Institute/BgGPT-7B-Instruct-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(m)\n",
        "device = \"cuda\"\n",
        "model.to(device)\n",
        "b_new_chat = True #insert \"[s][INST]...\"\n",
        "max_new_tokens = 500\n",
        "w = TextWrapper(width=80) #break long lines, default: 70 chars  w.wrap(string)\n",
        "b_autoregression = False #True\n",
        "options = Options(); #current state\n",
        "b_print_options = True\n",
        "\n",
        "def valid_range(m):\n",
        "  if m > 4096: return False # m = 2400\n",
        "  if m < 1: return False\n",
        "  return True\n",
        "\n",
        "def cycle():\n",
        "    global max_new_tokens\n",
        "    global b_new_chat\n",
        "    global b_autoregression\n",
        "    prompt = input(\"Enter prompt... \") #\"Изброй седемте най-високоплатени атлети от България. Колко от тях са жени? Кои бягат най-бързо?\"\n",
        "    if prompt == \"q\": return False\n",
        "    if prompt == \"n\": b_new_chat = True; return True\n",
        "    if prompt == \"a\": b_autoregression = True; return True #GPT2, GPT3-like: not very smart\n",
        "    if prompt == \"i\": b_autoregression = False; return True #Instruct\n",
        "    if prompt == \"top_k\": top_k=input(\"top_k=... \"); options.top_k=top_k; return True #Should validate etc. in the class, set ...\n",
        "    if prompt == \"top_p\": top_p=input(\"top_k=... \"); options.top_p=top_p; return True #Should validate etc. in the class, set ...\n",
        "    if prompt == \"t\": t=input(\"temperature=... \"); options.temperature=max(0.001, float(t));  options.temperature; return True #Should validate etc. in the class, set ...\n",
        "    if prompt == 's': prompt = strategy[0:len(strategy)//5]\n",
        "    if prompt == \"m\":\n",
        "      mx = int(input(\"max_tokens=?...\"))\n",
        "      if valid_range(mx): max_new_tokens = mx; options.max_new_tokens = mx; return True\n",
        "      else: print(\"Invalid max_new_tokens\")\n",
        "    if not b_autoregression:\n",
        "      if b_new_chat: prompt = \"[s][INST]\"+prompt+\"[/INST]\"; b_new_chat = False\n",
        "      else: prompt=\"[INST]\"+prompt+\"[/INST]\"\n",
        "    print(prompt)\n",
        "    for i in w.wrap(prompt): print(i) #input window is too small\n",
        "    if b_print_options==True: pp.pprint(options)\n",
        "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "    #generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens, do_sample=True, top_k = options.top_k, top_p = options.top_p, temperature=options.temperature)\n",
        "    generated_ids = model.generate(**model_inputs, max_new_tokens=options.max_new_tokens, do_sample=True, top_k = int(options.top_k), top_p = float(options.top_p), temperature=float(options.temperature))\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    prompt_arr.append(prompt)\n",
        "    answer_arr.append(answer)\n",
        "    prompt_and_answer.append((prompt, answer))\n",
        "    #print(w.wrap(answer))\n",
        "    for i in w.wrap(answer): print(i) #print wrapped lines\n",
        "    f.write(\"\\n\"+get_time_string()+\"\\n???: \"+prompt+\"\\n\\n===: \"+answer+\"\\n------------\\n\")\n",
        "    return True\n",
        "\n",
        "def interact():\n",
        "   i = True\n",
        "   while i == True:\n",
        "      i = cycle()\n",
        "def culturno():\n",
        "    messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин. \"}\n",
        "    ]\n",
        "    #encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, do_sample=True)\n",
        "    answer = tokenizer.batch_decode(generated_ids)[0]\n",
        "    print(answer)\n",
        "#culturno() #proba\n",
        "interact()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahInASr5hb-W",
        "outputId": "ffb46e57-7d1d-4588-dfbc-0a1ffebeb5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__init__.prompt: Преведи на английски: Котката изпи камъка и литна под нанагорнището.\n",
            "init Options... <__main__.Options object at 0x7b5493022080>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\"\n",
        "wr = w.wrap(s)\n",
        "print(s); print(wr)\n",
        "for i in wr: print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzt82L5YmlM5",
        "outputId": "99ff29b8-f628-45bd-af95-4b5e79000be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\n",
            "['Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо,', 'Вазов и Лем. Напиши текст за песен, който започва със селската баня,', 'голям кеф, градската десница, софийската левица с голямата... каца на', 'планетата на маймуните във Вселената на Стартрек, при Хари Потър и', 'философския камък на Властелина на пръстените с лазерния меч на Люк.', 'Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.']\n",
            "Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо,\n",
            "Вазов и Лем. Напиши текст за песен, който започва със селската баня,\n",
            "голям кеф, градската десница, софийската левица с голямата... каца на\n",
            "планетата на маймуните във Вселената на Стартрек, при Хари Потър и\n",
            "философския камък на Властелина на пръстените с лазерния меч на Люк.\n",
            "Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Ти си попфолк звезда от галактиката Андромеда и четеш Азимов, Куельо, Вазов и Лем. Напиши текст за песен, който започва със селската баня, голям кеф, градската десница, софийската левица с голямата... каца на планетата на маймуните във Вселената на Стартрек, при Хари Потър и философския камък на Властелина на пръстените с лазерния меч на Люк. Ползвай метафори, сравнения и алегории, междуметия и синекдохи. Амин. \"\n",
        "]\n",
        "# {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
        "#? why the assistant as well"
      ],
      "metadata": {
        "id": "sdbwldNm0epT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C8-_fnzparaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I2cuHYah0e_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3i1hIrYwo8WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt_and_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1VQ0--zizna",
        "outputId": "b5f9104f-5274-4875-eabd-b251eaa93c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Колко от тях са жени?', '<s> Колко от тях са жени?\\nОбщата средна възраст на всички жени е $30$ години. Каква е средната възраст, в години, на мъжете? Изразете отговора си като десетичен знак до най-близката десета. [/INST]Да приемем, че броят на жените е $w$ и броят на мъжете е $m$.\\nДадено ни е, че $w + m = 32$ и средната възраст на всичките $w + m$ души е $30$ години.\\nСредната възраст може да се изчисли, като се вземе сумата от всички възрасти и се раздели на броя на хората.\\nТака че имаме уравнението $(30)(w + m) = \\\\frac{30}{32}(w + 180)$ (тъй като средната възраст на мъжете е $180$ години).\\nОпростявайки това уравнение, получаваме $')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CLOSE\n",
        "f.close()"
      ],
      "metadata": {
        "id": "eQ4Bve9ljiUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Програмирай на Бейсик за APPLE 2 рисуване на триизмерен цилиндър в перспектива. В режим HGR.\"\n",
        "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
        "generated_ids = model.generate(**model_inputs, max_new_tokens=200, do_sample=True)\n",
        "tokenizer.batch_decode(generated_ids)[0]"
      ],
      "metadata": {
        "id": "EaEQ6a0JNiPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vJKEzme5V1A4"
      }
    }
  ]
}